{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "638d50f3",
   "metadata": {},
   "source": [
    "Comparing artificial background samples from CATHODE, CURTAINS, FETA, and SALAD to each other and to an actually simulated background. \n",
    "\n",
    "Taken from Slack (original text for CATHODE, CURTAINS, FETA):\n",
    "\n",
    "```\n",
    "Each subfolder nsig_injected_x contains 4 .npy files. 1 is the corresponding \"LHC\" (Pythia) data with x injected signal events, and the other 3 are the synthetic samples generated from the LHC data (and \"simulated\" (Herwig) data for FETA). All of these are signal region events, with m_jj between 3.3 and 3.7 TeV.\n",
    "There are also files STS_sig.npy and STS_bkg.npy corresponding to 20,000 signal and 20,000 background dijet events. These events make up the \"Standard Test Set\" and were not used at all during training.\n",
    "The .npy files have dimensions of (num_events, 6). The second index corresponds to the features (mj1, delta mj, tau21_j1, tau21_d2, deltaR, m_jj). All features have been mixmaxscaled to the range (0, 1). The exception is the m_jj feature, which has been scaled to the range (0.33, 0.67). This corresponds to scaling the mass band (3.1, 3.9) TeV (which corresponds to SB1, SR, SB2) to the range (0,1).\n",
    "\n",
    "old data (with flat m_JJ distribution in CATHODE):\n",
    "https://berkeley.box.com/s/rsbqw1fasqc5ykqah40mnd4hkhiyc90r\n",
    "\n",
    "new data (with correct m_JJ distribution):\n",
    "https://berkeley.box.com/s/y7835rnlw6chno3d4tz2mrxhsxo8sjgx\n",
    "```\n",
    "\n",
    "New data, including SALAD:\n",
    "\n",
    "```\n",
    "https://berkeley.box.com/s/hggnrqh83zaojeskdmm3zfi2apk1aeta\n",
    "```\n",
    "\n",
    "\n",
    "Idea of the study: \n",
    "\n",
    "train a multiclass classifier on CATHODE vs CURTAINS vs FETA vs SALAD\n",
    "----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- do it balanced, with the number of samples given by the smallest set of samples (will be CATHODE, 400k total)\n",
    "- do it with all available samples, using class weights\n",
    "\n",
    "- do it with the same NN as the rest of the paper\n",
    "- do it with a bigger NN as the rest of the paper\n",
    "\n",
    "- look at log posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33e198d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mpltern # https://mpltern.readthedocs.io/en/latest/installation.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "plt.style.use(\"science.mplstyle\")\n",
    "matplotlib.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath} \\usepackage{amssymb}'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978c56ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data contains: mj1, delta mj, tau21_j1, tau21_d2, deltaR, m_jj\n",
      "shape:  (400000, 6)\n",
      "min:  [ 4.94362693e-03  1.90454969e-07  2.29636542e-02 -1.57495635e-03\n",
      "  2.00533226e-01  4.50000000e-01]\n",
      "mean:  [0.13518009 0.12043567 0.57921261 0.4824966  0.55880448 0.49511516]\n",
      "max:  [0.84141445 0.73663813 0.9893744  1.00536549 0.85372049 0.55      ]\n",
      "shape:  (1866816, 6)\n",
      "min:  [ 2.77189794e-03  6.36700002e-16  1.53287807e-02 -1.46005545e-02\n",
      "  1.96302757e-01  4.49882358e-01]\n",
      "mean:  [0.13403158 0.12060626 0.57994329 0.48253784 0.55669686 0.49518479]\n",
      "max:  [0.94141889 0.88695806 0.99113005 1.01191103 0.92550874 0.54990494]\n",
      "shape:  (731868, 6)\n",
      "min:  [-0.26088436 -0.33336361 -0.37281863 -0.4324499  -0.36947759  0.4498837 ]\n",
      "mean:  [0.13462199 0.12114861 0.58171537 0.48369635 0.5593914  0.49501906]\n",
      "max:  [1.3124152  1.35115918 1.32156165 1.26961946 1.31514621 0.5499076 ]\n",
      "shape:  (1044782, 6)\n",
      "min:  [ 0.00126073  0.          0.00679913 -0.00465461  0.16702601  0.44988187]\n",
      "mean:  [0.1285373  0.12729118 0.54484807 0.45374795 0.55956109 0.49514224]\n",
      "max:  [0.81743252 0.80235654 0.99286478 1.01752569 0.87719786 0.5499084 ]\n",
      "shape:  (121339, 6)\n",
      "min:  [ 2.19207910e-03  1.11894507e-06 -5.69377642e-04 -2.67070692e-04\n",
      "  2.48984680e-01  4.49882294e-01]\n",
      "mean:  [0.13496635 0.12055243 0.58018826 0.48272053 0.5595958  0.49502351]\n",
      "max:  [0.81570579 0.72605278 0.99362225 0.99692427 0.82035039 0.54990505]\n"
     ]
    }
   ],
   "source": [
    "# loading and understanding the data\n",
    "data_path = '/home/claudius/ML_source/bkg_ccf/data_for_final_analysis/scaled_data_wide/'\n",
    "\n",
    "cathode_data = np.load(f\"{data_path}nsig_injected_0/cathode.npy\")\n",
    "curtains_data = np.load(f\"{data_path}nsig_injected_0/curtains.npy\")\n",
    "feta_data = np.load(f\"{data_path}nsig_injected_0/feta_o6.npy\")\n",
    "salad_data = np.load(f\"{data_path}nsig_injected_0/salad.npy\")\n",
    "salad_weights = np.load(f\"{data_path}nsig_injected_0/salad_weights.npy\")\n",
    "\n",
    "truth_data = np.load(f\"{data_path}nsig_injected_0/data.npy\")\n",
    "\n",
    "\n",
    "print(\"data contains: mj1, delta mj, tau21_j1, tau21_d2, deltaR, m_jj\")\n",
    "for data in [cathode_data, curtains_data, feta_data, salad_data, truth_data]:\n",
    "    print(\"shape: \", data.shape)\n",
    "    print(\"min: \", data.min(axis=0))\n",
    "    print(\"mean: \", data.mean(axis=0))\n",
    "    print(\"max: \", data.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc361059",
   "metadata": {},
   "source": [
    "Looking at these numbers, we see that we can train a **balanced** multiclass classifier of CATHODE vs CURTAINS vs FETA vs SALAD  using 400k events of each method. We evaluate on the 120k truth events in the end. \n",
    "\n",
    "For training, a train / test / val split of 60/20/20 would mean 240k train, 80k test, and 80k val events. \n",
    "\n",
    "Before training the classifier, let's have a look at a few histograms in physical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f030cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/rmastand/FETA/blob/ee4942e668b94df7b504b1503b027bdc28827eb1/helpers/datasets.py#L227\n",
    "\n",
    "# standardization transformation and its inverse:\n",
    "def minmaxscale(data, col_minmax, lower = -3.0, upper = 3.0, forward = True):\n",
    "    if forward:    \n",
    "        minmaxscaled_data = np.zeros(data.shape)\n",
    "        for col in range(data.shape[1]):\n",
    "            X_std = (data[:, col] - col_minmax[col][0]) / (col_minmax[col][1] - col_minmax[col][0])\n",
    "            minmaxscaled_data[:, col] = X_std * (upper - lower) + lower      \n",
    "        return minmaxscaled_data\n",
    "\n",
    "    else:  \n",
    "        reversescaled_data = np.zeros(data.shape)\n",
    "        for col in range(data.shape[1]):\n",
    "            X_std = (data[:, col] - lower) / (upper - lower)\n",
    "            reversescaled_data[:, col] = X_std * (col_minmax[col][1] - col_minmax[col][0]) + col_minmax[col][0]\n",
    "        return reversescaled_data\n",
    "    \n",
    "colors_dict = {\"truth\":\"grey\",\n",
    "               \"salad\": \"#009E73\",\n",
    "               \"feta\": \"#CC79A7\",\n",
    "               \"cathode\":\"#D55E00\",\n",
    "               \"curtains\":\"#E69F00\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d394fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load minmax and transform data to physical space\n",
    "\n",
    "col_minmax = np.load(f\"{data_path}col_minmax.npy\")\n",
    "cathode_physical = minmaxscale(cathode_data, col_minmax, lower=0, upper=1, forward=False)\n",
    "curtains_physical = minmaxscale(curtains_data, col_minmax, lower=0, upper=1, forward=False)\n",
    "feta_physical = minmaxscale(feta_data, col_minmax, lower=0, upper=1, forward=False)\n",
    "salad_physical = minmaxscale(salad_data, col_minmax, lower=0, upper=1, forward=False)\n",
    "truth_physical = minmaxscale(truth_data, col_minmax, lower=0, upper=1, forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b3270b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "luatex: /home/claudius/miniconda3/envs/tf-madnis/lib/libcurl.so.4: no version information available (required by /usr/bin/../lib/../lib/libmiktex-packagemanager.so.10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot them in comparison to see if transformation etc. worked correctly\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "title_dict = {0: f\"$m_{{J1}} [GeV]$\", 1: f\"$\\Delta m_{{JJ}} [GeV]$\", 2: f\"$\\\\tau_{{J1}}^{{21}}$\", \n",
    "              3: f\"$\\\\tau_{{J2}}^{{21}}$\", 4: f\"$\\Delta R_{{JJ}}$\", 5: f\"$m_{{JJ}} [GeV]$\"}\n",
    "kwargs = {'density': True, 'histtype': 'step', 'lw': 2., 'alpha': 0.6}\n",
    "legend_loc = ['upper right', 'upper right', 'upper left', 'upper left', 'upper left', 'lower left']\n",
    "for i in range(6): #loop over 6 features\n",
    "    plt.subplot(2,3,i+1)\n",
    "    _, bins, _ = plt.hist(truth_physical[:, i], bins=25, histtype='stepfilled', alpha=0.5, label=f\"'truth'\", density=True,color=colors_dict['truth'])\n",
    "    _ = plt.hist(cathode_physical[:, i], bins=bins, label='CATHODE', color=colors_dict['cathode'], **kwargs)\n",
    "    _ = plt.hist(curtains_physical[:, i], bins=bins, label='CURTAINs',color=colors_dict['curtains'], **kwargs)\n",
    "    _ = plt.hist(feta_physical[:, i], bins=bins, label='FETA',color=colors_dict['feta'], **kwargs)\n",
    "    _ = plt.hist(salad_physical[:, i], bins=bins, label='SALAD', weights=salad_weights,color=colors_dict['salad'], **kwargs)\n",
    "    plt.xlabel(title_dict[i])\n",
    "    plt.gca().get_yaxis().set_ticks([])\n",
    "    plt.ylabel('a.u.')\n",
    "    \n",
    "plt.subplot(2,3,5)\n",
    "plt.legend(loc='lower center', ncol=5, bbox_to_anchor=(0.5, -0.45))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.3)\n",
    "plt.savefig(f'background_validation.pdf', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88080d",
   "metadata": {},
   "source": [
    "The distributions seem to agree with fig. 4 of FETA (2212.11285)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c66bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/rmastand/synthetic_SM_AD/blob/main/helpers/nn.py on July 7th\n",
    "# and adapted\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "        # with an input probability\n",
    "        #self.dropout1 = nn.Dropout2d(0.1)\n",
    "        #self.dropout2 = nn.Dropout2d(0.1)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_shape, 64) # first size is output of flatten\n",
    "        # Second fully connected layer that outputs our labels\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "        # output changed from 1 to 4 for multiclass test\n",
    "        self.fc3 = nn.Linear(32, 4)\n",
    "\n",
    "        \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        # Apply softmax to x, so NLLLoss can be used\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        #output = torch.sigmoid(x) # for BCE \n",
    "        \n",
    "        return output\n",
    "    \n",
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MyNeuralNet, self).__init__()\n",
    "\n",
    "        # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "        # with an input probability\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_shape, 128) # first size is output of flatten\n",
    "        # Second fully connected layer that outputs our labels\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "\n",
    "        # output changed from 1 to 4 for multiclass test\n",
    "        self.fc4 = nn.Linear(64, 4)\n",
    "\n",
    "        \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "    \n",
    "        # Apply softmax to x, so NLLLoss can be used\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        #output = torch.sigmoid(x) # for BCE \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e263424",
   "metadata": {},
   "source": [
    "# balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70a091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Test / Val split at indices  [240000 320000 400000]\n"
     ]
    }
   ],
   "source": [
    "# data preparation:\n",
    "# select same number of events from datasets\n",
    "# append weight for salad, 1 for the rest\n",
    "# append label (cathode:0, curtains:1, feta: 2, salad: 3)\n",
    "# split train/test/val\n",
    "# make dataset\n",
    "\n",
    "np.random.seed(2104)\n",
    "\n",
    "sample_size = 400_000\n",
    "batch_size = 2_000\n",
    "\n",
    "cathode_indices = np.random.choice(np.arange(len(cathode_data)), size=sample_size, replace=False)\n",
    "cathode_data_cut = cathode_data[cathode_indices]\n",
    "cathode_data_cut = np.concatenate((cathode_data_cut, np.ones((sample_size,1)),0.*np.ones((sample_size,1))), axis=1)\n",
    "\n",
    "curtains_indices = np.random.choice(np.arange(len(curtains_data)), size=sample_size, replace=False)\n",
    "curtains_data_cut = curtains_data[curtains_indices]\n",
    "curtains_data_cut = np.concatenate((curtains_data_cut, np.ones((sample_size,1)), 1.*np.ones((sample_size,1))), axis=1)\n",
    "\n",
    "feta_indices = np.random.choice(np.arange(len(feta_data)), size=sample_size, replace=False)\n",
    "feta_data_cut = feta_data[feta_indices]\n",
    "feta_data_cut = np.concatenate((feta_data_cut, np.ones((sample_size,1)), 2.*np.ones((sample_size,1))), axis=1)\n",
    "\n",
    "salad_indices = np.random.choice(np.arange(len(salad_data)), size=sample_size, replace=False)\n",
    "salad_data_cut = salad_data[salad_indices]\n",
    "salad_data_cut = np.concatenate((salad_data_cut, salad_weights[salad_indices].reshape(sample_size,1), 3.*np.ones((sample_size,1))), axis=1)\n",
    "\n",
    "truth_data_cut = np.concatenate((truth_data, 4.*np.ones((truth_data.shape[0], 1))), axis=1)\n",
    "\n",
    "split_indices = [int(0.6*sample_size), int(0.2*sample_size), int(0.2*sample_size)]\n",
    "assert np.sum(split_indices) == sample_size\n",
    "split_indices = np.cumsum(split_indices)\n",
    "print(\"Train / Test / Val split at indices \", split_indices)\n",
    "\n",
    "train_cathode, test_cathode, val_cathode = np.split(cathode_data_cut, split_indices[:2])\n",
    "train_curtains, test_curtains, val_curtains = np.split(curtains_data_cut, split_indices[:2])\n",
    "train_feta, test_feta, val_feta = np.split(feta_data_cut, split_indices[:2])\n",
    "train_salad, test_salad, val_salad = np.split(salad_data_cut, split_indices[:2])\n",
    "\n",
    "\n",
    "train_data = np.concatenate((train_cathode, train_curtains, train_feta, train_salad))\n",
    "test_data = np.concatenate((test_cathode, test_curtains, test_feta, test_salad))\n",
    "val_data = np.concatenate((val_cathode, val_curtains, val_feta, val_salad))\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_data).to(device))\n",
    "test_dataset = TensorDataset(torch.tensor(test_data).to(device))\n",
    "val_dataset = TensorDataset(torch.tensor(val_data).to(device))\n",
    "truth_dataset = TensorDataset(torch.tensor(truth_data_cut).to(device))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "truth_dataloader = DataLoader(truth_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37726b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sample 240000\n",
      "sum of (weight**2) 259169.82232012894\n",
      "square of (weights.sum) 55248487647.33989\n",
      "((sum w)**2 / (sum w**2), which is Kish's effective sample size 213174.8486484528\n",
      "ratio of size to ((sum w)**2 / (sum w**2) 1.1258363804249\n",
      "sum of |weights| and n/sum|weights|:  235049.96840531565 1.021059486322281\n",
      " * * * \n",
      "size of sample 80000\n",
      "sum of (weight**2) 86234.89506996352\n",
      "square of (weights.sum) 6130749572.45207\n",
      "((sum w)**2 / (sum w**2), which is Kish's effective sample size 71093.60505950765\n",
      "ratio of size to ((sum w)**2 / (sum w**2) 1.1252770194033261\n",
      "sum of |weights| and n/sum|weights|:  78299.10326722823 1.021723067848769\n",
      " * * * \n",
      "size of sample 80000\n",
      "sum of (weight**2) 86402.38343867272\n",
      "square of (weights.sum) 6137247501.907438\n",
      "((sum w)**2 / (sum w**2), which is Kish's effective sample size 71030.99773009821\n",
      "ratio of size to ((sum w)**2 / (sum w**2) 1.1262688482003587\n",
      "sum of |weights| and n/sum|weights|:  78340.58655580413 1.0211820400784697\n",
      " * * * \n"
     ]
    }
   ],
   "source": [
    "# Kish's effective sample size = (sum weights)**2 / sum (weights**2) => too large class weight, salad will dominate\n",
    "# induced sample size: sum weights. \n",
    "\n",
    "for sample in [train_salad, test_salad, val_salad]:\n",
    "    sum_of_square = (sample[:, 6]**2).sum()\n",
    "    square_of_sum = sample[:, 6].sum()**2\n",
    "    print(\"size of sample\", sample.shape[0])\n",
    "    print(\"sum of (weight**2)\", sum_of_square)\n",
    "    print(\"square of (weights.sum)\", square_of_sum)\n",
    "    print(\"((sum w)**2 / (sum w**2), which is Kish's effective sample size\", (square_of_sum/sum_of_square))\n",
    "    print(\"ratio of size to ((sum w)**2 / (sum w**2)\", sample.shape[0] / (square_of_sum/sum_of_square))\n",
    "    print(\"sum of |weights| and n/sum|weights|: \", sample[:, 6].sum(), sample.shape[0]/sample[:, 6].sum())\n",
    "    print(\" * * * \")\n",
    "\n",
    "# will use 1.021 for the subsequent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4783a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(pred, label, individual=False):\n",
    "    overall = (np.argmax(pred, axis=-1) == label).astype(float) #.mean()\n",
    "    if individual:\n",
    "        cathode = (np.argmax(pred[label==0], axis=-1) == 0).astype(float)\n",
    "        curtains = (np.argmax(pred[label==1], axis=-1) == 1).astype(float)\n",
    "        feta = (np.argmax(pred[label==2], axis=-1) == 2).astype(float)\n",
    "        salad = (np.argmax(pred[label==3], axis=-1) == 3).astype(float)\n",
    "        return overall, (cathode, curtains, feta, salad)\n",
    "    else:\n",
    "        return overall\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    train_weights = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        batch_data = batch[0][:, :5]\n",
    "        batch_weight = batch[0][:, 6]\n",
    "        batch_labels = batch[0][:, -1].to(torch.long)\n",
    "            \n",
    "        loss = batch_weight*criterion(model(batch_data), batch_labels)\n",
    "        train_loss.append(loss.tolist())\n",
    "        train_weights.append(batch_weight.tolist())\n",
    "        \n",
    "        #loss = loss.sum()/batch_weight.sum()\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 40 == 0:\n",
    "            print(f\"step {i:4d} / {len(dataloader)}; loss {loss.item():.4f}\")\n",
    "    train_loss = np.array(train_loss).flatten()\n",
    "    train_weights = np.array(train_weights).flatten()\n",
    "    #return train_loss.sum()/train_weights.sum()\n",
    "    return train_loss.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    eval_loss = []\n",
    "    eval_weights = []\n",
    "    acc = []\n",
    "    acc_individual = {'cathode': [], 'curtains': [], 'feta': [], 'salad': []}\n",
    "    for batch in dataloader:            \n",
    "        batch_data = batch[0][:, :5]\n",
    "        batch_weight = batch[0][:, 6]\n",
    "        batch_labels = batch[0][:, -1].to(torch.long)\n",
    "            \n",
    "        loss = batch_weight*criterion(pred:=model(batch_data), batch_labels)\n",
    "        eval_loss.append(loss.tolist())\n",
    "        eval_weights.append(batch_weight.tolist())\n",
    "        local_acc, local_acc_individual = find_accuracy(np.exp(pred.cpu().numpy()), batch_labels.cpu().numpy(), individual=True)\n",
    "        acc.append(local_acc)\n",
    "        acc_individual['cathode'].append(local_acc_individual[0])\n",
    "        acc_individual['curtains'].append(local_acc_individual[1])\n",
    "        acc_individual['feta'].append(local_acc_individual[2])\n",
    "        acc_individual['salad'].append(local_acc_individual[3])\n",
    "    eval_loss = np.array(eval_loss).flatten()\n",
    "    eval_weights = np.array(eval_weights).flatten()\n",
    "    acc = np.array(acc).flatten()\n",
    "    for key in acc_individual:\n",
    "        acc_individual[key] = np.concatenate([*acc_individual[key]]).flatten()\n",
    "    #print(f\"Evaluation loss {eval_loss.sum()/eval_weights.sum()}, accuracy {acc.mean()}, acc CATHODE: {acc_individual['cathode'].mean():.3f}, acc CURTAINS: {acc_individual['curtains'].mean():.3f}, acc FETA: {acc_individual['feta'].mean():.3f}, acc SALAD: {acc_individual['salad'].mean():.3f}\")\n",
    "    #return eval_loss.sum()/eval_weights.sum(), acc\n",
    "    print(f\"Evaluation loss {eval_loss.mean()}, accuracy {acc.mean()}, acc CATHODE: {acc_individual['cathode'].mean():.3f}, acc CURTAINS: {acc_individual['curtains'].mean():.3f}, acc FETA: {acc_individual['feta'].mean():.3f}, acc SALAD: {acc_individual['salad'].mean():.3f}\")\n",
    "    return eval_loss.mean(), acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_prediction(model, dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    weights = []\n",
    "    for batch in dataloader:            \n",
    "        batch_data = batch[0][:, :5]\n",
    "        weights.append(np.array(batch[0][:, 6].tolist()))\n",
    "        batch_labels = batch[0][:, -1]\n",
    "        batch_preds = np.array(model(batch_data).tolist())\n",
    "        \n",
    "        preds.append(np.concatenate((batch_preds, batch_labels.reshape(-1, 1).tolist()), axis=1))\n",
    "    weights = np.concatenate([*weights])\n",
    "    preds = np.concatenate([*preds])\n",
    "    preds[:, :4] = np.exp(preds[:, :4])\n",
    "    return preds, weights\n",
    "\n",
    "def save_weights(model, appendix=None):\n",
    "    \"\"\" saves the model to file \"\"\"\n",
    "    if appendix is not None:\n",
    "        file_name = f'multiclass_weights_{appendix}.pt'\n",
    "    else:\n",
    "        file_name = f'multiclass_weights.pt'\n",
    "    torch.save({'model_state_dict': model.state_dict()}, file_name)\n",
    "    print(\"Model saved\")\n",
    "\n",
    "def load_weights(model, device, appendix=None):\n",
    "    \"\"\" loads the model from file \"\"\"\n",
    "    if appendix is not None:\n",
    "        file_name = f'multiclass_weights_{appendix}.pt'\n",
    "    else:\n",
    "        file_name = f'multiclass_weights.pt'\n",
    "    checkpoint = torch.load(file_name, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model {file_name} loaded\")\n",
    "    \n",
    "def log_posterior(preds, weights=None):\n",
    "    \"\"\" log posterior scores = sum_{x in geant} log p(C|x) where C = CATHODE, CURTAINS, FETA, SALAD \"\"\"\n",
    "    if weights is None:\n",
    "        return np.log(preds).sum(axis=0)\n",
    "    else:\n",
    "        return (weights*np.log(preds)).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa7bb2",
   "metadata": {},
   "source": [
    "## original NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "401e077d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model architecture \n",
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=5, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n",
      "Model has 2596 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# build NN and optimizer, following https://github.com/rmastand/FETA/blob/ee4942e668b94df7b504b1503b027bdc28827eb1/helpers/evaluation.py#L209\n",
    "\n",
    "dense_net = NeuralNet(input_shape=5)\n",
    "dense_net.to(device)\n",
    "\n",
    "print(\"model architecture \")\n",
    "print(dense_net)\n",
    "total_parameters = sum(p.numel() for p in dense_net.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='none', weight=torch.tensor([1., 1., 1., 1.021]).to(device)) #, weight=torch.tensor([1., 1., 1., 9./8.]).to(device)\n",
    "optimizer = torch.optim.Adam(dense_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412254e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss 1.391887272547223, accuracy 0.25008125, acc CATHODE: 1.000, acc CURTAINS: 0.000, acc FETA: 0.000, acc SALAD: 0.000\n",
      "Epoch 1 / 400\n",
      "step    0 / 480; loss 1.3769\n",
      "step   40 / 480; loss 1.3900\n",
      "step   80 / 480; loss 1.3804\n",
      "step  120 / 480; loss 1.3905\n",
      "step  160 / 480; loss 1.3831\n",
      "step  200 / 480; loss 1.3900\n",
      "step  240 / 480; loss 1.3835\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 400\n",
    "name_appendix = 'run09_balanced_original_5_classwgt_1.021'\n",
    "\n",
    "best_val_loss = 1e6\n",
    "\n",
    "accuracy = []\n",
    "_, acc = evaluate(dense_net, test_dataloader, criterion)\n",
    "accuracy.append(acc.mean())\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(f\"Epoch {epoch+1} / {num_epoch}\")\n",
    "    train_loss = train(dense_net, train_dataloader, optimizer, criterion)\n",
    "    eval_loss, acc = evaluate(dense_net, test_dataloader, criterion)\n",
    "    train_losses.append(train_loss.mean())\n",
    "    eval_losses.append(eval_loss)\n",
    "    accuracy.append(acc.mean())\n",
    "    if eval_loss.mean() < best_val_loss:\n",
    "        save_weights(dense_net, appendix=name_appendix)\n",
    "        best_val_loss = eval_loss.mean()\n",
    "    print(\"   - - - - -   \")\n",
    "\n",
    "load_weights(dense_net, device, appendix=name_appendix)\n",
    "_ = evaluate(dense_net, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(num_epoch) + 1\n",
    "plt.plot(epochs, train_losses, label='train')\n",
    "plt.plot(epochs, eval_losses, label='test')\n",
    "plt.legend()\n",
    "#plt.ylim(1.375, 1.38)\n",
    "plt.ylim(1.383, 1.386)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767d5085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model multiclass_weights_run01_balanced_original.pt loaded\n",
      "Evaluation loss 1.377166776743359, accuracy 0.259675, acc CATHODE: 0.286, acc CURTAINS: 0.334, acc FETA: 0.362, acc SALAD: 0.057\n"
     ]
    }
   ],
   "source": [
    "# test eval, not needed for full run\n",
    "name_appendix = 'run01_balanced_original'\n",
    "load_weights(dense_net, device, appendix=name_appendix)\n",
    "_ = evaluate(dense_net, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "891f0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model multiclass_weights_run01_balanced_original.pt loaded\n"
     ]
    }
   ],
   "source": [
    "# test eval, not needed for full run\n",
    "name_appendix = 'run01_balanced_original'\n",
    "load_weights(dense_net, device, appendix=name_appendix)\n",
    "preds_models, weights_models = get_prediction(dense_net, test_dataloader)\n",
    "preds_truth, _ = get_prediction(dense_net, truth_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6c39cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test eval, not needed for full run\n",
    "preds_cathode = preds_models[preds_models[:, -1] == 0.][:, :4]\n",
    "preds_curtains = preds_models[preds_models[:, -1] == 1.][:, :4]\n",
    "preds_feta = preds_models[preds_models[:, -1] == 2.][:, :4]\n",
    "preds_salad = preds_models[preds_models[:, -1] == 3.][:, :4]\n",
    "weights_salad = weights_models[preds_models[:, -1] == 3.]\n",
    "preds_truth = preds_truth[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fd5d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of CATHODE is 0.2860\n",
      "Accuracy of CURTAINS is 0.3339\n",
      "Accuracy of FETA is 0.3622\n",
      "Accuracy of SALAD is 0.0590\n"
     ]
    }
   ],
   "source": [
    "# test eval, not needed for full run\n",
    "output_str = \"Accuracy of {} is {:.4f}\"\n",
    "print(output_str.format(\"CATHODE\", find_accuracy(preds_cathode, 0.).mean()))\n",
    "print(output_str.format(\"CURTAINS\", find_accuracy(preds_curtains, 1.).mean()))\n",
    "print(output_str.format(\"FETA\", find_accuracy(preds_feta, 2.).mean()))\n",
    "#print(output_str.format(\"SALAD\", (weights_salad*find_accuracy(preds_salad, 3.)).sum()/weights_salad.sum()))\n",
    "print(output_str.format(\"SALAD\", (weights_salad*find_accuracy(preds_salad, 3.)).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03721b36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run nr 1\n",
      "Model multiclass_weights_run01_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-111064.31080919 -111059.97141853 -111054.72679676 -110802.48244518]. Argmax is at 3\n",
      "log posterior of CURTAINS samples is [-111244.06524316 -110820.94653394 -111050.82445395 -110871.36410642]. Argmax is at 1\n",
      "log posterior of FETA samples is [-114044.61808296 -113613.40937687 -110467.80556547 -114140.31028932]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-108840.7264864  -108684.81098048 -108610.41132219 -108396.29853698]. Argmax is at 3\n",
      "log posterior of True samples is [-168544.88236554 -168515.47800717 -168321.60503634 -167996.38039852]. Argmax is at 3\n",
      "Run nr 2\n",
      "Model multiclass_weights_run02_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-110344.02035738 -111023.79562665 -111439.09842847 -111278.09078076]. Argmax is at 0\n",
      "log posterior of CURTAINS samples is [-110512.80088472 -110735.28057899 -111526.42429084 -111341.11284277]. Argmax is at 0\n",
      "log posterior of FETA samples is [-113059.42607883 -113398.52330217 -110819.61452147 -114314.02435306]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-108153.341207   -108637.87584184 -109056.31550901 -108783.62903751]. Argmax is at 0\n",
      "log posterior of True samples is [-167506.86601868 -168457.50227648 -168916.25020381 -168663.23856333]. Argmax is at 0\n",
      "Run nr 3\n",
      "Model multiclass_weights_run03_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-110518.82959876 -110939.53128225 -111669.46149293 -110878.07159251]. Argmax is at 0\n",
      "log posterior of CURTAINS samples is [-110693.59245595 -110709.81405206 -111734.5543111  -110903.9198788 ]. Argmax is at 0\n",
      "log posterior of FETA samples is [-114316.06344813 -114243.22098903 -111070.55123321 -115354.61465029]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-108318.6905254  -108560.35492041 -109229.26812724 -108450.7800626 ]. Argmax is at 0\n",
      "log posterior of True samples is [-167765.96576663 -168317.41486327 -169243.03126054 -168105.55671055]. Argmax is at 0\n",
      "Run nr 4\n",
      "Model multiclass_weights_run04_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-111513.95284767 -110427.19209187 -110831.47061724 -111188.32803646]. Argmax is at 1\n",
      "log posterior of CURTAINS samples is [-111658.77357048 -110217.19892839 -110874.98814916 -111217.9440048 ]. Argmax is at 1\n",
      "log posterior of FETA samples is [-114403.10031077 -112563.8169624  -110267.87984264 -114054.14561516]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-109272.37820048 -108051.28247616 -108443.51291937 -108736.72827986]. Argmax is at 1\n",
      "log posterior of True samples is [-169233.71519918 -167519.88923878 -168024.28413962 -168556.49793997]. Argmax is at 1\n",
      "Run nr 5\n",
      "Model multiclass_weights_run05_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-109841.07225717 -111498.51029394 -111779.96282805 -110880.83282941]. Argmax is at 0\n",
      "log posterior of CURTAINS samples is [-110027.71741365 -111269.62634555 -111800.91025889 -110911.95956697]. Argmax is at 0\n",
      "log posterior of FETA samples is [-113056.79253321 -113715.77299121 -111201.86534687 -114096.72711438]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-107659.1439767  -109131.06263211 -109338.10175757 -108433.38718816]. Argmax is at 0\n",
      "log posterior of True samples is [-166742.4809575  -169173.13890922 -169416.4510594  -168095.21503807]. Argmax is at 0\n",
      "Run nr 6\n",
      "Model multiclass_weights_run06_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-110153.08047785 -110906.86257214 -111803.00559207 -111173.33895056]. Argmax is at 0\n",
      "log posterior of CURTAINS samples is [-110314.38211376 -110665.58106714 -111842.61306884 -111240.1752795 ]. Argmax is at 0\n",
      "log posterior of FETA samples is [-115007.72986683 -113525.40420474 -111230.45641048 -115535.71099902]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-107947.33321396 -108531.12112621 -109378.9474799  -108727.10784932]. Argmax is at 0\n",
      "log posterior of True samples is [-167223.24231966 -168241.88375969 -169456.29585233 -168537.63024731]. Argmax is at 0\n",
      "Run nr 7\n",
      "Model multiclass_weights_run07_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-110351.86797075 -111397.48973827 -111803.33654511 -110530.06068042]. Argmax is at 0\n",
      "log posterior of CURTAINS samples is [-110515.75871712 -111152.08567399 -111877.10728185 -110553.60368078]. Argmax is at 0\n",
      "log posterior of FETA samples is [-115745.2766927  -115384.26195356 -111156.92744047 -114804.18129633]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-108159.64390049 -109030.05368577 -109360.05446383 -108078.05755913]. Argmax is at 3\n",
      "log posterior of True samples is [-167543.27157165 -169023.94466399 -169420.25156544 -167576.69190654]. Argmax is at 0\n",
      "Run nr 8\n",
      "Model multiclass_weights_run08_balanced_original_5_classwgt_1.021.pt loaded\n",
      "log posterior of CATHODE samples is [-110608.17976278 -110300.41968307 -111952.62181344 -111241.34682304]. Argmax is at 1\n",
      "log posterior of CURTAINS samples is [-110789.66722013 -110009.8499674  -112082.17487744 -111279.57561205]. Argmax is at 1\n",
      "log posterior of FETA samples is [-113953.09314415 -113193.61672071 -111367.88160749 -115295.51825328]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-108404.29603595 -107956.32375183 -109522.79469225 -108748.45599023]. Argmax is at 1\n",
      "log posterior of True samples is [-167901.7246101  -167339.66202303 -169701.74221498 -168603.77303301]. Argmax is at 1\n",
      "averaged scores: \n",
      "log posterior of CATHODE samples is [-110520.97620696 -110916.69690813 -111496.90511731 -110970.73393137]. Argmax is at 0\n",
      "log posterior of CURTAINS samples is [-110685.97620625 -110666.98657392 -111552.500523   -111007.02059816]. Argmax is at 1\n",
      "log posterior of FETA samples is [-112669.49781686 -112436.95399689 -110899.80554645 -113264.12438595]. Argmax is at 2\n",
      "log posterior of SALAD samples is [-110677.25475586 -110857.85667775 -111540.41188338 -110836.01395508]. Argmax is at 0\n",
      "log posterior of True samples is [-167764.97411135 -168284.51237372 -168995.67119579 -168231.2952069 ]. Argmax is at 0\n",
      "Mean and std of individual runs: \n",
      "Based on 8 runs.\n",
      "log posterior of CATHODE samples is [-1.38186768 -1.38680277 -1.39427138 -1.38745711] +/- [0.00615535 0.00487267 0.00469952 0.00309336]. Argmax is at 0\n",
      "Based on 8 runs.\n",
      "log posterior of CURTAINS samples is [-1.38399493 -1.38371935 -1.39498374 -1.38799946] +/- [0.00607556 0.00495165 0.0049593  0.00318606]. Argmax is at 1\n",
      "Based on 8 runs.\n",
      "log posterior of FETA samples is [-1.42747828 -1.42130942 -1.38684841 -1.43374255] +/- [0.0106323  0.00969063 0.00461153 0.00730972]. Argmax is at 2\n",
      "Based on 8 runs.\n",
      "log posterior of SALAD samples is [-1.35430555 -1.35716076 -1.36396782 -1.35680382] +/- [0.00594507 0.00482392 0.00456667 0.00289713]. Argmax is at 0\n",
      "Based on 8 runs.\n",
      "log posterior of TRUTH samples is [-1.38296647 -1.38721775 -1.39330709 -1.38675012] +/- [0.00598865 0.00495147 0.00460501 0.00296117]. Argmax is at 0\n"
     ]
    }
   ],
   "source": [
    "# run multiple runs at once: compute log posterior\n",
    "\n",
    "output_str = \"log posterior of {} is {}. Argmax is at {}\"\n",
    "output_str_2 = \"log posterior of {} is {} +/- {}. Argmax is at {}\"\n",
    "\n",
    "preds_cathode_list = []\n",
    "preds_curtains_list = []\n",
    "preds_feta_list = []\n",
    "preds_salad_list = []\n",
    "weights_salad_list = []\n",
    "preds_truth_list = []\n",
    "\n",
    "log_posterior_dict = {'CATHODE': [], 'CURTAINS': [], 'FETA': [], 'SALAD': [], 'TRUTH': []}\n",
    "\n",
    "for run_nr in [1,2,3,4,5,6,7,8]: #, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "    print(f\"Run nr {run_nr}\")\n",
    "    name_appendix = f'run{run_nr:02d}_balanced_original_5_classwgt_1.021'\n",
    "    load_weights(dense_net, device, appendix=name_appendix)\n",
    "    preds_models, weights_models = get_prediction(dense_net, test_dataloader)\n",
    "    preds_truth, _ = get_prediction(dense_net, truth_dataloader)\n",
    "    preds_cathode_list.append(preds_models[preds_models[:, -1] == 0.][:, :4])\n",
    "    preds_curtains_list.append(preds_models[preds_models[:, -1] == 1.][:, :4])\n",
    "    preds_feta_list.append(preds_models[preds_models[:, -1] == 2.][:, :4])\n",
    "    preds_salad_list.append(preds_models[preds_models[:, -1] == 3.][:, :4])\n",
    "    weights_salad_list.append(weights_models[preds_models[:, -1] == 3.].reshape(-1,1))\n",
    "    preds_truth_list.append(preds_truth[:, :4])\n",
    "    \n",
    "    print(output_str.format(\"CATHODE samples\", \n",
    "                            post_local:=log_posterior(preds_cathode_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_cathode_list[-1]))))\n",
    "    log_posterior_dict['CATHODE'].append(post_local/len(preds_cathode_list[-1]))\n",
    "    print(output_str.format(\"CURTAINS samples\", \n",
    "                            post_local:=log_posterior(preds_curtains_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_curtains_list[-1]))))\n",
    "    log_posterior_dict['CURTAINS'].append(post_local/len(preds_curtains_list[-1]))\n",
    "    print(output_str.format(\"FETA samples\", \n",
    "                            post_local:=log_posterior(preds_feta_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_feta_list[-1]))))\n",
    "    log_posterior_dict['FETA'].append(post_local/len(preds_feta_list[-1]))\n",
    "    print(output_str.format(\"SALAD samples\", \n",
    "                            post_local:=log_posterior(preds_salad_list[-1], weights=weights_salad_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_salad_list[-1]))))\n",
    "    #log_posterior_dict['SALAD'].append(post_local/(weights_salad_list[-1].sum()))\n",
    "    log_posterior_dict['SALAD'].append(post_local/len(preds_salad_list[-1]))\n",
    "    print(output_str.format(\"True samples\", \n",
    "                            post_local:=log_posterior(preds_truth_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_truth_list[-1]))))\n",
    "    log_posterior_dict['TRUTH'].append(post_local/len(preds_truth_list[-1]))\n",
    "    \n",
    "preds_cathode_list = np.array(preds_cathode_list)\n",
    "preds_curtains_list = np.array(preds_curtains_list)\n",
    "preds_feta_list = np.array(preds_feta_list)\n",
    "preds_salad_list = np.array(preds_salad_list)\n",
    "weights_salad_list = np.array(weights_salad_list)\n",
    "preds_truth_list = np.array(preds_truth_list)\n",
    "\n",
    "for key in log_posterior_dict:\n",
    "    log_posterior_dict[key] = np.array(log_posterior_dict[key])\n",
    "\n",
    "print(\"averaged scores: \")\n",
    "print(output_str.format(\"CATHODE samples\", \n",
    "                        log_posterior(preds_cathode_list.mean(0)),\n",
    "                        np.argmax(log_posterior(preds_cathode_list.mean(0)))))\n",
    "print(output_str.format(\"CURTAINS samples\", \n",
    "                        log_posterior(preds_curtains_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_curtains_list.mean(0)))))\n",
    "print(output_str.format(\"FETA samples\", \n",
    "                        log_posterior(preds_feta_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_feta_list.mean(0)))))\n",
    "print(output_str.format(\"SALAD samples\", \n",
    "                        log_posterior(preds_salad_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_salad_list.mean(0)))))\n",
    "print(output_str.format(\"True samples\", \n",
    "                        log_posterior(preds_truth_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_truth_list.mean(0)))))\n",
    "print(\"Mean and std of individual runs: \")\n",
    "\n",
    "to_plot_central = []\n",
    "to_plot_err = []\n",
    "\n",
    "for method in ['CATHODE', 'CURTAINS', 'FETA', 'SALAD', 'TRUTH']:\n",
    "    print(f\"Based on {len(log_posterior_dict[method])} runs.\")\n",
    "    print(output_str_2.format(f\"{method} samples\", \n",
    "                              cen:=log_posterior_dict[method].mean(0),\n",
    "                              err:=log_posterior_dict[method].std(0),\n",
    "                              np.argmax(log_posterior_dict[method].mean(0))))\n",
    "    to_plot_central.append(cen)\n",
    "    to_plot_err.append(err)\n",
    "to_plot_central = np.array(to_plot_central).flatten()\n",
    "to_plot_err = np.array(to_plot_err).flatten()\n",
    "\n",
    "with open(f'log_posterior_{name_appendix}.npy', 'wb') as f:\n",
    "    np.save(f, to_plot_central)\n",
    "    np.save(f, to_plot_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb9626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "luatex: /home/claudius/miniconda3/envs/tf-madnis/lib/libcurl.so.4: no version information available (required by /usr/bin/../lib/../lib/libmiktex-packagemanager.so.10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAFPCAYAAACvTqLdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAv9ElEQVR4nO3de5QcV30n8O/PNo9gyx7LwEGSIeNRDIFjFGs0ELwLZGyPwgYkLTGyTEzi+GStEZw9m7PnxCDL2Y1mThIb2SSbZB+JRnksbOINtlCykpNs0BiGBMLDo3FQSLIkaDTLRlIea3lkwgYw8Ns/7u35lXq6b1d1V1fdqv5+zpmjnup63Pl2qW9V3ap7RVVBRESD7ZKyC0BEROVjZUBERLis7AJQXETkfQAWASz7SaMAZgEc8v8+6KcfAHCq6b0nAYwAWFDV2cT64Nd5CMBuAGsBbFTVvX6exwDsVtVl//to0/ZG/PYWVHWviAz535f97+8DsABgCMCIqj6UZybUWafPoMNnfFBVF1uss90yH4bbn9YCgKrO9OWPGjSqyp8B+IH7TzoBYBLAaJt5jsP9R278Pgr3HxUAHksu598bavOeJl5PJF6fajP9BID3NZWleZ0Tvnyjye0D2Nm0rve1+tv40799J81nkOYz7mYZX6bJsjOqww8vE9WAiOwUkfeJyIiITPoj5+T7owB2AZhX1RlVXWixjlEAazVxhObneyxrWQCsHBWqP0NopnbmsBPubOGOFKvfA3dkCNiZyyKAvSIy4n/nUWIGeew76PAZZPyMsy7zKIC9addL7bEyqDj/n3UB7j/kqJ+8tmm2Cf8feTmwqjEA880T232Zt1peRCYAbIVdSkpjrf+CGUp8mbTkK6oPi8iBxLRGhfWYiJyCu6REKeS176T4DFJ/xlmX8eXiZ54DVgYVp6oL/ktyK4BZ/x935ejeH+lt9Ed9jZ/RFqtahKsQLpLhP++8rziOA9iX4U/Y4iuRRbjLDUHqrkVP+B+IyJD/m7cA2ALXlkAp5LXvpPgMUn3Gfp6sywz5eahHrAwqzp/iD8Fd61/2p9cr/JHTcf+fdabdqb7/Ij/f4st/yP97HBdXFmvbHC0uwx9lNl9yaFH2naq6x297D9JfRrgd9oWzq7EdX55WZaIW8tp3EPgMMn7Go10ssws8AMgF7yaqvmW4L+kD/kiq1X/Wxn+ww6EVqepW/wWxcjdR4zKRqs749xpfGAt+vY0jxQkRWVTVWRHZ68sy7+cZgjuSX+uXb9x1skdEGkenQwBGRGTSLzcC4A5flhG4a9LnG0ezIpK8FDUhIst+voOdIyNvGTntO2jxGfh9I/QZj8J9xiNwZyfLKZYZ8dta619DeTdRLkSVTyAPguQXMoDFDG0BNOC47wwGVgZERMQ2AyIiYmVARERgZUBERGBlQERE4K2lK1p10KaJzrZadJrVtgM2uI60kh2tjcHdbnfA36I5BP+Iv19mpHF7XKfOuJreb9yiuc8/8EMlaep47Um4z+12f7tu28+sm478qFrSfMYp17Nyi6+/FXivqm7NraBld47U7x/02EFbYtpFnWahQwdsWN2h1glYx27HG6/97zub1h3sjKvF+6PJ9fGntP2o+XPZ2ekz67Qftdr3+BPXT6d9I81nnHIbzd9Jj+X5d1T6MlFRHbS16jRLO3TAFijzqJ9vObHMYYS7cGjbGZc/WlhQHjV2LafO2prX2ej3p+X2Gp9Zp/2om07eKD957Buhz1hEJkTkeGI7EyJy3K/7fYl+uEZg/X8ltz/h52vVxUwmlb1M1PSfLdnJ1nJitgnt3Ld9mg7a1qrqYREZEpERbdH3ehuNJyUB69ZhDG36UvF9vCy3KMtyi24iJkRkVV9ClE2O+9HKvI3PRVc/GdvNZ9btvkc96sO+sYqvEA5q4glvEdnjXx6G644DqrrQeMI/sfioX37Rz9fxICWkspVBowYWkb1w186Wk+8nO9lKTJ5vUXM3grxI03+85k6z0n74s4lyNsq3CNe3TrOWFYFfdgirK5BZv4OM+HlG0xyx0sVy3I8aZuE+q4lW73XxmXW771GP+rBvtNPt/9tc/79XtjLwjTIzSHSylaxd/bTj2rk/nlkROd/iqGvIb2enqu7xrxfhLh/1eiRwIHkWIE1jALTQtjMudf30DGF118OUQl77UZL/XNvOn/Yzy3vfo2z6sW9k1LLH4H4d+FW5zWAZKTvZ6rQidS3yO/11uwkRmfBHcI1OsxofyhCs0yz4U/ed8B2wJY/4YB2tDfnyjcB9qQPArQAmG9uCuxSwt2nZCf/+pC9j8m6i0cT7OwE8AXbj261l5LAfNX3mIy3ea/uZtdqPOu17VIhl5PQdE/iumAAw2nTN/8mm74EhP32xcZbY2Kf8ehrrGEIPat83kbCTLcoB9yNqpy77Ru0rAyIi6qzKl4mIiCgnrAyIiIiVARERsTIgIiKwMiAiIrAyICIisDIgIiJUuDsKALjnnnv02muvxdLSEoaHhwGg7evQe3nPV+S2Gq/n5uYwPj4eXRZlZFb1LPLMrFMWxz79RWy/6VXR/o15zjco+0Wa+aanp39NVe9BUp79YRf9s3//flVVbfwbel3kfGWUadu2baVsN6/58txW1bPIM7NOWVzxI79VStnLmG9Q9os08wGY0jqNZ9AwPj7e8nXa+ZqXabeO0Hx5ryPr33Hs2LGe19GpfKHpvf4deayjLllk2c86zccsmEXa+Uo/uu/lp7kGbPbxj388+H4aVVlH46inzDLEsg5mYTpl0Xxm0I8yFLGONMsPyn6RZnm0ODOodN9Ed999tw4PD2N8fDxc4w0AEUGVP8s8MQvTKYs1dz2Cr3zozgJLVB7uF87c3BxuvvnmD6rq3cnplW5AHh4extTUVNnFICKqDH/gvNQ8vRZtBkRE1BtWBjXB01/DLAyzMMwijJVBTczMNI+9PriYhWEWhlmEsTKoiT179pRdhGgwC8MsDLMIY2VARESsDIioNw8cOVl2ESgHrAxq4ujRo2UXIRrMwhSRxYO/+4W+byMP3C/CWBnUxJYtW8ouQjSYhWEWhlmEsTKoiQ0bNpRdhGgwC8MsDLMIY2VARJXHdovesTIgGjAPHDmJNXc9gjV3PQIAK6+r/IValXaLmFW6byIyu3fvLrsI0WAWplUW99+2CffftgnAYHVUx/0irNJnBktLS5iamsLc3FzZRSkdn640zMIwC8MsHP99Odw8vdKVQaPX0kHvvhrgnRJJzMIwC8MsHPZaWnMLCwtlFyEazMIwC8MswlgZEBERG5DrYt26dWUXIRrMwtQ5iweOnLzoLqLG3VH73n7DSgN5Up2zyAMrg5o4e/Zs2UWIBrMw/coi6xdxP2S9K4r7RRgvE9UEh/80zML0K4v7b9uEr3zozpUv4MbroiqCbnC/CGNlUBPT09NlFyEazMIwC8MswlgZEBERKwMiIiq4MhCRnSJyPPD+hP85kJg26pfbWUwpq2l+fr7sIkSDWRhmYZhFWKGVgaoebveeiIwCGFXVWQCjIjLi39rnl1ubmEZENVLlTvLqIprLRKq6oKoPicgQgEVVXRSRSQBPisiIqs6o6mLJxYzW2NhY2UWIBrMwVcmi115H9739ho7zVCWLssT4nMEYgFP+9Ub/73kROQhgr6ouN2ZsdFQHuP422EcRUTZpvkSrIOZbWmMxNzeX7NRzuPn9XCsDf11/bdPkRX/pJxVVnRWR2xNtBKdUdVlETgCYBPBQY95GR3VE1B1+iQ6O5AHz9PT0UvP7uVYGoTaBTnyj8SlVnQGwDFepPAmrXIb8dGph//79ZRchGszCMAvDLMKKvptoAsBY8s6gxN1FBwEs+nmGfBvBYQBDfhp8RUEt8AzJMAvDLAyzCCu0zcBfLrq6adpW/+8igEYD8Wzi/Yeap9Fq69evZ98rHrMwzMIwi7Bo7iai3pw7d67sIkSDWZgisqhKIzT3izBWBkTUEzZC1wMrg5oYHR0tuwjRYBaGWRhmEcbKoCZOnDhRdhGiwSwMszDMIoyVQU1MTk6WXYRoMAvDLAyzCBNVLbsMXZuamlLeLuaICKr8WeaJWZiYs2geLa2hX6OlxZxF0URkWlWnktNi7I6CiAZA1mErqb94mYiIiKpdGTQ6qkt0vjSwzpw5U3YRosEsDLMwzMLx35fDzdMrXRk0Oqpjb6W8UyKJWRhmYZiF478vl5qnV7oyILNjx46yixANZmGYhWEWYawMiIiIlQEREbEyqI2DBw+WXYRoMAvDLAyzCGNlUBN8utIwC8MsDLMIY2VQEyJSdhGiwSwMszDMIqzlE8gich2APQCuA3AegAB4BsBBVV0qrHRERFSIVZWBiLwDgKrqfS3eu1VERlT1Y4WUjogGQlUGyKmzVmcGs6p6odXMqvqEiFzV5zJRF7Zt21Z2EaLBLExVsihigJyqZFGWVW0GjYpARK4UkeF271Ncjh07VnYRosEsDLMwzCIs1IB8B4AJwF06alUxUDy2b99edhGiwSwMszDMIqzteAa+EXkUwHFVfVZEbomtreDuu+/W4eFhjI+PD3z/ROyr3TALwywMs3Dm5uZw8803f1BV705OD41n8A4ApwE85CuGA30sX1caHdUREVE67TqqC1UGpwGcUNWP+EbjLX0pGRERla5tm4GqfgTAkP91EsBIEQWi7vD01zALwywMswgLPoGsqn/q/31YVX+1kBJRV2ZmZsouQjSYhWEWhlmErWpA9g+dnWj1pLFvO9isqkeKKV7Y1NSUss3AYeOYYRaGWRhmYURkWlWnktNWtRn4NoJbReTdAJIPmC3D3VkURUVARET5admArKpPAHii4LIQEVFJ2GtpTRw9erTsIkSDWRhmYZhFWOjWUojIowBO+Z9ZAKO8TBSnLVt4528DszDMwjCLsGBloKq7gJWG460A2PoSqQ0bNrBxzGMWhlkYZhEWrAwAwPdJ9LSqHmKPpURE9ZSmzWAjfId1cIPdEBFRzaSpDEZgTx9f08eyZLa0tISpqSnMzc2VXZTS7d69u+wiRINZGGZhmIXjvy+Hm6e37bX0opncg2jXATgc07CXfOiMiCi7Vg+dBc8MRORREfllAFfDPXdQqf6JHjhysuwiFIZ3ShhmYZiFYRZhnfom2gXgMFwPphMAbi+iUHl58He/UHYRCrOwsFB2EaLBLAyzMMwiLG2bwdOq+jBcxUBERDXTsTJQ1UMANorIgwCe7n+RqBvr1q0ruwjRYBaGWRhmEdaxMhCRWwG8DsD5RpfWFJ+zZ8+WXYRoMAvDLEynLAapjbGVNJeJnlbV+wDMiMg9/S4QdYd3VRlmYZiF6ZTFILUxtpKmMnidiNyoqhfgGpIpQtPT08H3B+mop1MWg4RZGGYRxjaDATHoRz1EFJaqC2s/HvL7+1wWitwgnV0QDZo0Dcjv9WcF+wDc0cvGRGSniBwPvD/hfw40LTMhIpO9bLvu5ufn+76NqpxdFJFFVTALwyzCOvZaCmDBj3zWM1U9LCJ7Wr0nIqNw4yU8JCJ7RaTRJ9Kiqi74CmFUVfnkCBFRztJUBqMishNuDOSnVfUD/SiI/5JfEJEhuApgUUTOAzghIrcDGFHV2X5suw7GxsbYV7vHLAyzMMwiLE1lcMI/fVyUMbiR1aCqyyJyEMBjcCOtXaTRaykAjI+PY3x8vLhSEhFVyNzcXLKH5+Hm99NUBt/vj8yXATwZGvbSn0GsbZq8mOWIXlVnReT2xNnIrL90dEBEdqrqSpcYw8PDvI+aiCiF5AHz9PT0UvP7aSqD3wZwWlUv+OEv20p+UWflG41PqeoMXCWwFu7S0EN+lgcB7Op2/XW3f//+sosQDWZhmIVhFmFtKwM/hsEdAK4CcBDAEVXt6aEzEZkAMJY8wheR46q61W9jxM8zpKozIjLk7yJahKsYZnrZfp3xDMkwC8MsDLMIC91auqiqu1T1LQAkj64oVHVWVa9OnkH4igCquujfn1XVPX7asqrO+GmsCALWr19fdhGiwSwMszDMIixUGVwnIjcCKw+dLRZSoh49cOQk1tz1CNbc9QgArLzu9oGpqjxode7cubKLEA1mYZiFYRZhocpgI4B3+tHOHgWwV0RuEZFbCipbV+6/bRO+8qE78ZUP3QkAK6/vv21TV+uryoNWVD1VOdCg9Kr8mYYqg1kAH/aXinYBeDdcBbG3kJJRJqOjo2UXIRpVyaKIA42qZFGEVlnkfSWhygePbRuQVfWppt9PAzjkzxIoMidOnCi7CNFgFoZZmFZZ3H/bppWrBmvuemTlisIgStVRXZLvypoiMznZn66b8j5yKkK/sqgiZmGYRVjbykBEbmv6/UYRebDRqExxOXTo0KppeXyR590GU4RWWQwqZmEGJYtuD9SCDci+8bhxS+k+Vd0H125ABevmA67iFzkR9abbdotQZXDKNxzPi8hVAM776ZW4xbRuqtwwlbeYL1ERVVWoMrjGXyq6A67zuGv89JG+lyqlRkd1ic6XBtaZM2fKLkJhOlWMg5RFJ8zCMAvHf18ON09vWxn44S4vwPUJdAruOYP3+9dRaHRUx95KeddIErMwzMIwC8d/Xy41T+90N9EIgEMAdqrqaVW9T1X/NO/CUe927NhRdhGiwSwMszDMIix0N9FuuPaB+wA8JSL3FlYqIiIqVKgL6/nEg2enRaSI8hARUUYPHDl5UVta43byfW+/IfXdg6HKYExEFG5sgREAmwHkMhZy7PIItmgHDx4suwjRiDmLovetmLMoWp2zyONJ6lB3FIdE5L0AtsINfbmv24JWTSyPqGf54uDTlSbmLIret2LOomj9fEq/agePrQRHOvNjHz8MACIyrKpLRRSKnCxfHCLCwb49ZmGYhelXFrEcPPZqVWUgIh8F8EzzZLjLRNcXUai87Hv7DWUXgXJSl6Mvoli1OjM4oKqr2gZEZHMB5ckVvyTqoy5HX0SxWnVraauKwE9/qtV0isO2bdv6vo2qnGkVkUVVMAvTKYtY9u+yulvJ3IU1xenYsWN930ZVzrSKyKIqmIXplEUs+3dZ/ZCxMihAETX99u3b+76NqmAWhlmYQcmi2zOc1JWBiNzrxzQYjmVMg6p0VFdETf/4448H3x+kU+BOWQwSZmEGJYtOZziZO6pr4QLcOMijcL2Ylm6QOqrr9ct80E+BicjptqO6pPOq+m4Ap8ExDQoXy5c5EdVTlspg1l8eWgZwdV9KQ10bpAeLOp0lDVIWnTALwyzCLqoMROQW3zZwZYt518J1TXGVqn6kkNJFIJZr7Z3MzMyUXYTCdDpLqkoWRexbVcmiCMwirPnM4DiAGVV9tnlGVT0N4CMABmqEiKpcntmzZ0/ZRYhGVbIoYt+qShZFYBZhzZXBoVYVQYOqLmJAei4lIsqqKlcSWmmuDNIMacnGYyKiFqpyJaGV5spgOcUyaeahgh09erTsIkSDWRhmYZhFWHNHdRc1t4vIrwB4pmksAzbJR2jLli1lFyEazMIwCxNzFjH0yttcGQgAiMh1AB4D8CsAHvOD3Bz07Qkc/zJCGzZs4K1zHrMwzMLEnEUMvfKuOjMQkXsA7AFwu7+DCAAeFpHdIvIkeGaQSgw1PRFRWs2VwUMAPqyqr2ue0Q+D+Q4AEwAGZgjMbsVQ0xMRpdXcgHy7qr6n3cz+YbNd/S1SelXpqK4Iu3fvLrsIbT1w5CTW3PXIytlR43W/Oq2LOYuiMQvDLJx2HdVJrNfQ0piamtKpqamyi9ERzwwMsyAKK+L/iIhMq+pUchrHM6iJmO+UKBqzMMzCMIswVgY1sbCwUHYRosEsDLMwzCKMlQEREYUrA3+b6cq/FK9169aVXYRoMAvDLAyzCEt7ZsAHzSJ39uzZsosQDWZhmIVhFmG8TFQTVbirqijMwjALwyzCWBnUxPT0dNlFiAazMMzCVCWLsrrBTlsZVPdhBCKiCimru5pOlYE0/dsTEdkpIsdTzHegaZkJEZnMowxERLRap8rg0aZ/e6KqhzvNIyITAEb8651+udnEe5VTxGnf/Px837dRFczCMAvDLMKClYGqXkj+228iMoKLR1J7XeL3RQCjRZQjb+yllIhi19xradlGVHVWZOWq1FDT+9ckf2l0VAcA4+PjGB8f73Px4jU2NhZtX+1JRZwlVSWLIjALM+hZzM3NJTv1HG5+P9fKwF/WWds0ebFxmafDshMt5ltusb4Vw8PDvF2sYniWRFSO5AHz9PT0UvP7bSsDEblXVT+QZWNp2gQCzvs2gSEAIyIyCuBJ2NnBCICOjc9ERJRdqM0gee0eInJjrxvzX/ZjjYZhP+04AKjqgj8zWAtfAfjKZaRRSaQ5wxhU+/fvL7sI0WAWhlkYZhHWdjwDEfkogOsALMDdWrpZVa8vsGwdVWU8AyKimGQdz+CAql6vqneo6i4A7+5r6agn69evL7sI0WAWhlkYZhHWts1AVZ8Qkd1wYx4/mbX9gIp17ty5sosQDWZhmIVhFmFtzwx8RbAI4D4AT4nIvYWVioiIChW6tXReVZ/yr08n7v2nCI2OVvJ5vL5gFoZZGGYRFqoMxkRE4e71HwGwGcATRRSKsjtx4kTZRYgGszDMwjCLsLaXiVT1EICtAGYAbGWbQdwmJ9mPXwOzMMzCMIuwtreWrppRZFhVl/pbnGx4a6kRkYF+1D6JWRhmYZiFaXVr6arLRCLyYVW9wz9n8ExjMtxloqieMyAiony0ajO4z/+7N9GADBHZXEyRiIioaKvaDFT1tH95a9P0p5rnLVuj19JET3wD68yZM2UXIRrMwjALwywc/3053Dy90L6J8tbotXSQu65u4J0ShlkYZmGYheO/L5eap7Nvoppg45hhFoZZGGZhUjUgJxxQ1ZXnCkTk1sC8RERUYaHLRCMi8ssicqOIXAWAVSoRUU2FKoNTqvoeuEtJhYyBTN07ePBg2UWIBrMwzMIwi7BQm8F74UYWaww7ORrbU8hsMyAiyi5rm8EMgH0ARgF8NLaKgC7GxjHDLAyzMMwiLDSewQX4B9BEZLOIXKmqzxZWMiIiKkxoPINbGq/9A2djhZSIiIgK16pvonfA9VY6JiKn4J4xAIBTAD5WYNkog23btpVdhGgwC8MsDLMIa9mA7G8lHYmxC4okNiATEWXXqgG55WUi315wtYgM++cM7hWR4QLKSF3avn172UWIBrMwzMIwi7DQraW3qeoREflrAFsAbIztTOHuu+/W4eFhjI+PD3z/RLxTwjALwywMs3Dm5uZw8803f1BV705OD91aesE3Ij+lqs+KyHUAoqoMGh3VERFROu06qgs9gXweriH5Ht+o/Pp+FIyIiMoXGgP5KbhurA8BuE5V72s3L5WPp7+GWRhmYZhFWOg5g91wlcF9AJ4SkXsLKxVlNjMzU3YRolFEFg8cOdn3beSB+4VhFmGhBuTNTcNe3prs0joGvLXUsHHMFJHFmrsewVc+dGdft5EH7heGWZisfRONiYgCWAYwAmAzgKgqAyIiykeozeAQXAPyDICt7KiOiKi+QmcGUNWHReSwqp4uqkDUnaNHj5ZdhGgwC8MsDLMICzUg3yoiXwLwKyLy18mO6yg+W7ZsKbsI0WAWhlkYZhEWHPZSVb9LVd+iqtcD2FhUoSi7DRs2lF2EaDALwywMswgLDnvZ9PuTAMA+ioiI6ifUZvCQiDwNdzfR1XAd152Gu6vo+gLKRkREBQlVBntbPVcgIrf2sTyZLC0tYWpqih3VAdi9e3fZRYgGszDMwjALZ25uDgCGm6e3feisCvjQGZWlKg+dEbWSejwDqh7eKWGYhWEWhlmEsTKoiYWFhbKLEA1mYZiFYRZhoecMbmv6/UYReVBEbux7qYiIqFChM4ONIvKoiNzjf9+nqvvA5w2itG7durKLEI1OWVSlx9E8cL8wzCIs+JyBqu4CMC8iV8ENdgO4bq0pMmfPni27CNHolMWDv/uFgkpSPu4XhlmEhSqDa/ylojsAjAG4xk8f6XupKDPeVWWYhWEWhlmEdeq19AKAB+GeRt4rIu/H6ieTKQLT09NlFyEazMIwC8MswoK9lsKdBUwCeNJ3Yc2hL4mIaqjQYS9FZKeIHE8x3wH/75CIjPrlDvS6fSIiai3UZjCvqk+o6mnfLcVTgXlTUdXDneYRkQlYu8QuAGON5URkstcy1NX8/HzZRYgGszDMwjCLsKiGvRSRESTuVlLV5AjWIwA6nlUQEVF2bSsDVT0kIu+FG/ryhH/GoN9GVHVWRC6a6CuJ86o6m5ze6KgOwMB3Vjc2NsbBvj1mYZiFGfQs5ubmGp3UAS06qus47CWAhwE3joGqLoXmF5GdANY2TV5s/hJvs+xEYL6dqrqneeLw8DBvFyMiSiF5wDw9Pb3U/P6qykBEPgrgmebJSDGOQZo2gYDzvr1gCMCIiIyq6oKI7FTVh3zZRlWVHYwQEeWs1ZnBgTbjGGzudWP+y37Mf8E3GoWPq+rWxpe8byQeSsx/QEQal6j29lqGutq/f3/ZRYgGszDMwjCLMI5nQAOn27EIHjhysmVXFvvefgPuv21THkUjKkSr8Qw6PXRGFbF+/Xr2veL1K4v7b9u08qVflcFtuF8YZhHG8Qxq4ty5c2UXIRrMwjALwyzCWBkQERErg7oYHR0tuwjRYBaGWRhmEcbKoCZOnDhRdhGiwSwMszDMIoyVQU1MTrLbpgZmYZiFYRZhrAxq4tChQ2UXIRrMwjALwyzCWBkQEVG1K4NGR3WJzpeIiCjAf18ON0+v9ENn7KjOnDlzpuwiRINZGGZhmIXjO6tbap5e6TMDMrxTwjALwywMswhjZVATO3bsKLsI0WAWhlkYZhHGyoCowh44crLsIlBNsDIgqrBWvagSdYOVQU0cPHiw7CJEg1kYZmGYRVil7yYiw6crTassmsciWHPXIwDqPxYB9wvDLMJYGdSEiAz0YN9JrbKo4lgEeeB+YZhFGC8TERERKwMiImJlUBvbtm0ruwjRYBaGWRhmEcbKoCaOHTtWdhGiwSwMszDMIoyVQU1s37697CJEg1kYZmGYRVilKwP2Wmoef/zxsosQDWZhmIVhFg57LSUiIvZaSpSnfW+/oewiEOWKlUFN8GEaU0QWVXlqmfuFYRZhrAxqYmZmpuwiRINZGGZhmEUYK4Oa2LNnT9lFiAazMMzCMIswVgZERMTKgIiIWBnUxtGjR8suQjSYhWEWhlmEVfo5AzJbtmwpuwjRqHsWWcZmqHsWWTCLMFYGNbFhwwbeOufVPYssYzPUPYssmEUYLxMRERErAyIiqnhlwI7qzO7du8suQjSYhWEWhlk47TqqkypfQ5uamlJ2VEdZ1WkM5Dr9LVQcEZlW1anktEqfGZDhnRKGWRhmYZhFGCuDmlhYWCi7CNFgFoZZGGYRxsqAiIhYGdTFunXryi5CNJiFYRaGWYSxMqiJs2fPll2EaDALwywMswhjZVATvKvKMAvDLAyzCGNlUBPT09NlFyEanbIYpCEruV8YZhHGyoAGTlWGrCQqUqGVgYjsFJHjKeY7kGYaERHlo9DKQFUPd5pHRCYAjHSaRhebn58vuwjRYBaGWRhmERbVZSIRGQGw2GkaERHlK6rKAMCIqjZ/8beaBsA6qmvXWV0eHdhVZR1jY2OllyGWdQxSFp0MShZplh/0LObm5la+L9Gio7pcKwPfJjDZ9DORctkJVZ3tNC1peHh45Y8bHx9f9X4MH0ws64ihDLGsI4Yy5LWOGMoQwzpiKEMs62i3/Pj4eLIyWGp+P9fKQFUPq+pM00/bL/Mm50VkQkR2AhgRkdE201ZJ/vGhINvN17xMqGZNu91e19HN35HHOsr6O/JYR12yyPvvyGMdzCLd+kLTY8wiqdAurP1ZwmMAdjcak0XkuKpuTcwzCWAvgNtVdaHdND/9VwH8Ddwpz5Kf3O516L285ytyW3WZL8YyxT5fjGWKfb4Yy1TGfNeq6j2Jeao9ngEREeUjtgZkIiIqASsDIiIa3MpARH5cRF7R4zouFZE7ReSKDMuknrcovWbRTQ5+OWZhyzELW45Z2HKFZXFZURuKiYhcBuBLAMT//s9V9VNdrGoDgPMA1orItwBcoar/ENiuAPgh/yDdlwD8DwDPqOq3Mpb/xQB+CMA5AJ9T1S93UfbGuvLIIlMOfjvMwrbLLGy7zMK2W2gWg3pmsAnAnwO4VEReBuBpEfmBLtZzrar+TwDvAvATAEZF5PLA/JcB+DyAzwC4CsDv+J+shuHuoroSwM+KyO1drKMhjyyy5gAwiyRmYZiFKTSLgTwzAPCvAFwO4BSAjwAYArCmi/VcEJF3w+X4HwG8GcAVAL7aamZVfQ7A50RkCMAXADwOIFMt712lqo2d4tdF5G0iconbRObbw/LIIlMOALNIYhaGWZjCs1DVgfuBOyN6NYB7ALwKwGsBvCDjOgTACwG80f/+cgBvS7nsNQDeBOCyxroybPd7APwFgEkA6/y0dwF4XhlZ9JIDs2AWzCKeLAbyMpGqfltV/xKutr0PwB5V/XrG1bxRVb+mqp/0v28CsNxuZhHZICLX+O0/DeCfANzrf099pKKqnwfwfQD+FsCN/nriZ9UdRWSWQxaZcgCYRRKzMMzClJJFNzVllX8AvALAdyR+fwmAV2dcxxCAnwbwzwC8IrGe5weWuRWulm7U7i8C8PIe/o41cDX8cQCvKSOLbnJgFsyCWcSZxUA9gSwi3wl3unQpgH8A8DvapkfUlOu7BMBWAPPqau92820AcKuqfsgvA1X9dhfbuwrAN1X1q4lpN6nqp7tYV25ZpM3Bz8ssbF5mYfMyC5u3lCwGrQH5cgC/AODrAP4l3HW4RRERzVArisg9AL4LwN8BWAvgx0TkPlU93WaRTQBeISJrVfV8D+V/HYBtIvINuBr+ZDc7uddzFl3kADCLJGZhmIUpJ4tuTzuq9gNX8f0SgJclpr0TwPqM6xkGcBjAWwCMwh01XIVAow7cPb63Afh1AD/vlw2eJrZZz7UAbgHwRgCPAPgi3HgPhWfRTQ7Mglkwi3izGJjLRP7+4P1wDTfH4e7dfZ2qfqLL9V0Dd5eAqurT7Y4W/HyvUtU/8b9vB/CDACZV9ZtdbPcKAC8F8AIAp1X1a12sI7cs0uaQmJdZgFm0mJdZoNwsBuZuIlX9W1V9D1xN+xIAdwL4vz2s8utwp30/ISLXt/twAbwDwJCIvMCX4xiA3+jygxW4+5JfBHek8LXGNcUscs4ibQ4As0hiFoZZmPKyyHrqUdUfAC8G8Ab/+pUAfguuBs6yjq0AfhK+hd9PuzYwvwDYBuA6//ubcvpbLgPwM3APkxSeRdYcmAWzYBbxZzEQl4lE5PVwH+hWuEaUn+tyPd8H4IcBnAHwm6r6pQ7zvxLANwF8w0+6FMB3q+ofZtzukKou+9dXA1gH4IKqnsn2F+STRdYc/DLMwpZhFrYMs7BlSs1iUC4TPQfgUVX9UQDqP6jMVPUTqrobrsOom0TkneI6sWpnEu66451wH8haAN/ZxaZ/Q0R2+NO+Zbg7G17cxXqAHLLoIgeAWSQxC8MsTKlZ1L4yENevx8tUtVHbPgbg+d1cR0z4MwD/CGCHhq/l/SSAXwTwLNyH8jUA/z3LhkTkeQA+DOB6uLsMLoV7EvLPsxa6D1mkzQFgFknMwjALU2oWtb5M1Gi1F5EfAfCXqjrvG2ZuVNXPZlzXWwE8D8AJVf2bFPO/VFX/3r++Ce7pv+er6mT2vwQQkUsBvBWug6uPqeofZFw+lyyy5uCXYRa2DLOwZZiFLVN6FrV+6Eytpvs8gK2+dr8J7lax1HxteymA7wVwu4h8E8CXAfy0tu/v5N0i8pSqHlPVT4vIl+Eebe+Kqn5LRD4O94j6hS6W7zmLLnMAmEUSszDMwpSeRd3PDNYB+Jaq/r2I/CCAjQBOwtWUmW7VEpHnw1WeL4Crtb9DVR9uM+8auKcIPwvghar6j12W/zoAOwCcBTCnHQbD6LCuXLLIkoOfn1nY/MzC5mcWNn8UWdT6zADAHQDeIiIfUNXfEZHnJ64FZqKq3/A7y6tV9T91mP1fADgNN5jEdwP4k8YpaMbNPgfXW+FGAHeIuzPgA1lPf71cssiYA8AskpiFYRYmiixqfWYArNyu9WMAfl9V/6iL5S/aIUTkNXBPIn4wsMy/A7AZgAL4bwD+ULt4CtKv60Wq+v9E5M0A1gP4lKr+ny7X1XUW3eTg52MWNh+zsPmYhc0XRRa1rQxE5Hvgeuz7cxF5D9x9w69Q1XdkXM+b/MvPqerXxT2mvkVVfy/Fsj8KN+7pn6nqUra/YGUdlwLYAuDb6hq0Mh8x5JFFLzn45ZmFLc8sbHlmYcuXm4Xm8IRbjD8A/g3cEHVTADb6aa/PuI5L4FrkXwBY51LJ103zvxLAjwP4fv/7WgD/GcAvdlH+VzT9/uMAXltGFllzYBbMgllUL4vMoVXtB26ourciMUBFhmVfD2B/hvlH4RpwfhvAuxLTL8+43ZcBeDCxU17iy7KmjCyy5sAsmAWzqF4WtbxMJCLXAnhOVf9ORNbDPdk3qqo7Mq7nB+H6In8RXEv/J7VNS7+43gbH1D86LiJ7/DKf14whi8hrASwCGIM7WlhUN3xdZnlkkSUHPz+zsPmZhc3PLGz+6LKo6xPIrwLwMhG5VFXPquoUgN1ZViDuYZN/UndL2B8BuBnAL/sPsZU3AHiTiNwj7rHzT8HtYCoikrH8L4IbWOOHAQiAG0Sk23uOe8qiixwAZpHELAyzMNFlUbtbS8UNGbdBVZ8QkUtE5DJV/aaq/l3GVW0AcImIXKGqHxeRT8D1Kd5uyLpGi/0mAL8Jl+17gWwDWHtfBvAeAL+mqp8RkVvhOrzKJKcssuYAMIskZmGYhYkvi7pdJhKRH4BrTf8v2sOQceIeKb8WrsOqZT/tUlX9Voplnwc3KhIA/C9VfTLlNl8JN6rRZ1X1c37aLQBepKqPd/E39JxFLzn4eZmFLc8sbHlmYctHkUUdLxOtAfAXAD4gIj8vIm8R90RgauKeCHwGwCkAPyQie0RkIuWyou6x82NwDUQvyLDpK+Bq+XtF5F1+2tfhHo/vRk9Z9JKDX55Z2PLMwpZnFrZ8NFnU6szAX6N7g/r7ekXkbXAjB2UaMk5E/jWAbwH4AwCvhbu+9/eq+ktdlCnVfb7SukHpk6qauefFxPp6yiLPHPz6mIWtj1nY+piFra+ULICaVQbASmPOc6r6bf96TFU/lXEd43AdPF0Jd8/vs7kXdPU23wbXKdYSgP8K4NVwlw+/kHYHabHOnrIoIwe/XWZh22UWtl1mYdvNP4u6VAYiInCDQnwdruvY0zms8ya42v5vVPX3e11fh21tAvByuAal74FvUOrm78g7iyJz8NtjFrY9ZmHbYxa2vdyyWFlnjSqDywH8AFwN+Uq4261Oqup0xvXcCOBLqvqP4rqw3QVgWFXfn3ORQ2XoqkEpsXzPWcSQgy8Hs7ByMAsrB7OwcvSUxcp66lIZAIDYsHIvhxvU4SuqeiTD8pcA2At3/e+vVfWr+ZeyYxkaA2wMwQ2B9xFV/WQX6+k6ixhy8OVgFlYOZmHlYBZWjlyyAGpWGQCAiLwKwHeq6ke7XL4xgPQ1AP5Ksz+fkKtur//5ZbvOIrYcAGaRxCwMszC9ZFGLW0tF5KrGa1X9IoDnROSdgUXarecNqvqMqv6Fqv4xgMtF5IV5ljWrrB9sHlnEmAPALJKYhWEWptuKAKhJZQDgThF5r9jj30sAvpRlBf4a4s+KyH8Qke/1k1+K6j2l3VMWNcoBYBZJzMIwixYqf5lIRK6E66HvE6r6nIhc3s31OxF5nl/+zQBuBPBFAC9W1d/Kt8T9k0cWdcgBYBZJzMIwi/YqXZN5m+Fu53rOX8PbLCKf0+zjiO7xrfKHADwN4BsAumqVL1EeWdQhB4BZJDELwyzaqENlcBmAZwFAVZ8RkWfhHjFP/eH6o4W/BPDH6sYv/St1j4hXTU9Z1CgHgFkkMQvDLNqoQ5vBZwD8gohs8g1DVwLI2qq/GcAZ/+GuBfBm/6FXTa9Z1CUHgFkkMQvDLNqo7JmBuB773grgYwB+CsAbAbwGAFT12xlXlzxaOC8iF+D6By/k0fJe5ZhFpXMAmEUSszDMorPKVgZwPfYtAvj3AH4PbpSgL6rqN7pY12cA/IaI/AyA/w13tLCQV0ELkFcWVc8BYBZJzMIwiw4qeZnI3xL2ElU9qqq3w1Vq/wQg07U7EXmliPxbABvhjhZeD/eI+ku7OLsoRR5Z1CEHgFkkMQvDLNKp6pnBGwDcJCIvh+ux73MAXqjqhozL8OBFnmcXZckjizrkADCLJGZhmEUKlTwzgBsy7tMAXgI3ZNxPAfgqkP4JvLzOLiLQUxY1ygFgFknMwjCLFOrw0Fm3Q8Y19wf+GgDf1h76Ay9bN1nUMQeAWSQxC8Ms2qvqmQGAlU6Zuh0yruezi5j0kEWtcgCYRRKzMMwirPJnBknd1tLdnl3ErJss6pgDwCySmIVhFher9JlBsy4rgl7OLqLVxU5eyxwAZpHELAyzuFitzgzyUKdrgL1gDoZZGGZh6pYFKwMiIqrXZSIiIuoOKwMiIsL/B7+u+WFiaC/bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot average of the log posterior of the runs above\n",
    "\n",
    "with open(f'log_posterior_{name_appendix}.npy', 'rb') as f:\n",
    "    to_plot_central = np.load(f)\n",
    "    to_plot_err = np.load(f)\n",
    "\n",
    "ymin, ymax = np.min(to_plot_central-to_plot_err), np.max(to_plot_central+to_plot_err)\n",
    "\n",
    "plt.figure(figsize=(6,4.5))\n",
    "\n",
    "plt.errorbar(np.arange(20), to_plot_central, to_plot_err, fmt='_')\n",
    "plt.vlines([3.5, 7.5, 11.5, 15.5], ymin, ymax, ls='dashed', color='k')\n",
    "plt.ylim((ymin, ymax))\n",
    "plt.xlim((-0.5, 19.5))\n",
    "plt.ylabel('log posterior = $\\sum_{x} \\log{p_{model}(x)}$')\n",
    "plt.gca().set_xticks(np.arange(20), *[5 * ['$p_{CATHODE}$', '$p_{CURTAINS}$', '$p_{FETA}$', '$p_{SALAD}$']], rotation=70)\n",
    "plt.text(1.5, -1.345, '$x \\in$ CATHODE', ha='center')\n",
    "plt.text(5.5, -1.34, '$x \\in$ CURTAINS', ha='center')\n",
    "plt.text(9.5, -1.345, '$x \\in$ FETA', ha='center')\n",
    "plt.text(13.5, -1.34, '$x \\in$ SALAD', ha='center')\n",
    "plt.text(17.5, -1.345, '$x \\in$ Truth', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f42a92be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot average of the log posterior of the runs above, new layout by Radha\n",
    "\n",
    "l = 3\n",
    "f_1 = 20\n",
    "f_2 = 30\n",
    "alpha = 0.4\n",
    "dpi = 800\n",
    "ms = 12\n",
    "\n",
    "with open(f'log_posterior_{name_appendix}.npy', 'rb') as f:\n",
    "    to_plot_central = np.load(f)\n",
    "    to_plot_err = np.load(f)\n",
    "\n",
    "ymin, ymax = np.min(to_plot_central-to_plot_err), np.max(to_plot_central+to_plot_err)\n",
    "\n",
    "order = [\"cathode\", \"curtains\", \"feta\", \"salad\"]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (16, 5)) \n",
    "\n",
    "to_bold = [0, 5, 10, 15]\n",
    "legends = ['$p_{\\mathrm{CATHODE}}(x)$', '$p_{\\mathrm{CURTAINs}}(x)$','$p_{\\mathrm{FETA}}(x)$', '$p_{\\mathrm{SALAD}}(x)$']\n",
    "for i in range(20):\n",
    "    \n",
    "    if i in to_bold:\n",
    "        plt.errorbar(i, to_plot_central[i], to_plot_err[i], fmt='o', color = colors_dict[order[i%4]],\n",
    "                     elinewidth=5, markersize = ms )\n",
    "    elif i >= 16:\n",
    "        plt.errorbar(i, to_plot_central[i], to_plot_err[i], fmt='x', color = colors_dict[order[i%4]], \n",
    "                     elinewidth=3, markersize = ms, label = legends[i-16] )\n",
    "\n",
    "    else:\n",
    "        plt.errorbar(i, to_plot_central[i], to_plot_err[i], fmt='x', color = colors_dict[order[i%4]], \n",
    "                     elinewidth=3, markersize = ms)\n",
    "    \n",
    "plt.vlines([3.5, 7.5, 11.5, 15.5], ymin, ymax, ls='dashed', color='k')\n",
    "plt.ylim((ymin, ymax))\n",
    "plt.xlim((-0.5, 19.5))\n",
    "plt.xticks([])\n",
    "plt.yticks(fontsize = f_1)\n",
    "\n",
    "plt.ylabel('$\\sum_{x} \\log{p_{model}(x)}$', fontsize = f_2)\n",
    "plt.ylabel('$\\\\langle LP(\\\\text{model } i|\\\\text{samples } j) \\\\rangle$', fontsize = f_2)\n",
    "\n",
    "\n",
    "plt.text(1.5, -1.345, '$x \\in$ CATHODE', ha='center', fontsize = f_1)\n",
    "plt.text(5.5, -1.345, '$x \\in$ CURTAINS', ha='center', fontsize = f_1)\n",
    "plt.text(9.5, -1.345, '$x \\in$ FETA', ha='center', fontsize = f_1)\n",
    "plt.text(13.5, -1.345, '$x \\in$ SALAD', ha='center', fontsize = f_1)\n",
    "plt.text(17.5, -1.345, '$x \\in$ Truth', ha='center', fontsize = f_1)\n",
    "\n",
    "plt.legend(fontsize = f_1, loc = \"center\", bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "\n",
    "fig.savefig(f\"discrim_methods.pdf\", dpi = dpi)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80166c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76f312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce1c445",
   "metadata": {},
   "source": [
    "## modified NN\n",
    "Uses a bigger NN than the rest of the draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ec61760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model architecture \n",
      "MyNeuralNet(\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Model has 25924 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# build NN and optimizer, following https://github.com/rmastand/FETA/blob/ee4942e668b94df7b504b1503b027bdc28827eb1/helpers/evaluation.py#L209\n",
    "\n",
    "\n",
    "dense_net = MyNeuralNet(input_shape=5)\n",
    "dense_net.to(device)\n",
    "\n",
    "print(\"model architecture \")\n",
    "print(dense_net)\n",
    "total_parameters = sum(p.numel() for p in dense_net.parameters() if p.requires_grad)\n",
    "print(f\"Model has {total_parameters:d} trainable parameters\")\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(dense_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74f1c157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss 1.3822634987186806, accuracy 0.25, acc CATHODE: 1.000, acc CURTAINS: 0.000, acc FETA: 0.000, acc SALAD: 0.000\n",
      "Epoch 1 / 400\n",
      "step    0 / 480; loss 1.3778\n",
      "step   40 / 480; loss 1.3757\n",
      "step   80 / 480; loss 1.3803\n",
      "step  120 / 480; loss 1.3800\n",
      "step  160 / 480; loss 1.3738\n",
      "step  200 / 480; loss 1.3747\n",
      "step  240 / 480; loss 1.3733\n",
      "step  280 / 480; loss 1.3830\n",
      "step  320 / 480; loss 1.3877\n",
      "step  360 / 480; loss 1.3772\n",
      "step  400 / 480; loss 1.3768\n",
      "step  440 / 480; loss 1.3798\n",
      "Evaluation loss 1.3788914668362777, accuracy 0.250621875, acc CATHODE: 0.018, acc CURTAINS: 0.981, acc FETA: 0.004, acc SALAD: 0.000\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 2 / 400\n",
      "step    0 / 480; loss 1.3792\n",
      "step   40 / 480; loss 1.3824\n",
      "step   80 / 480; loss 1.3808\n",
      "step  120 / 480; loss 1.3835\n",
      "step  160 / 480; loss 1.3790\n",
      "step  200 / 480; loss 1.3718\n",
      "step  240 / 480; loss 1.3870\n",
      "step  280 / 480; loss 1.3764\n",
      "step  320 / 480; loss 1.3724\n",
      "step  360 / 480; loss 1.3784\n",
      "step  400 / 480; loss 1.3860\n",
      "step  440 / 480; loss 1.3804\n",
      "Evaluation loss 1.3785033636048096, accuracy 0.254603125, acc CATHODE: 0.338, acc CURTAINS: 0.225, acc FETA: 0.456, acc SALAD: 0.000\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 3 / 400\n",
      "step    0 / 480; loss 1.3693\n",
      "step   40 / 480; loss 1.3778\n",
      "step   80 / 480; loss 1.3788\n",
      "step  120 / 480; loss 1.3864\n",
      "step  160 / 480; loss 1.3796\n",
      "step  200 / 480; loss 1.3743\n",
      "step  240 / 480; loss 1.3721\n",
      "step  280 / 480; loss 1.3784\n",
      "step  320 / 480; loss 1.3808\n",
      "step  360 / 480; loss 1.3806\n",
      "step  400 / 480; loss 1.3853\n",
      "step  440 / 480; loss 1.3717\n",
      "Evaluation loss 1.3783294883861477, accuracy 0.254696875, acc CATHODE: 0.476, acc CURTAINS: 0.519, acc FETA: 0.025, acc SALAD: 0.000\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 4 / 400\n",
      "step    0 / 480; loss 1.3779\n",
      "step   40 / 480; loss 1.3768\n",
      "step   80 / 480; loss 1.3785\n",
      "step  120 / 480; loss 1.3818\n",
      "step  160 / 480; loss 1.3693\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3726\n",
      "step  280 / 480; loss 1.3737\n",
      "step  320 / 480; loss 1.3769\n",
      "step  360 / 480; loss 1.3786\n",
      "step  400 / 480; loss 1.3744\n",
      "step  440 / 480; loss 1.3907\n",
      "Evaluation loss 1.3783554571602679, accuracy 0.25380625, acc CATHODE: 0.095, acc CURTAINS: 0.886, acc FETA: 0.035, acc SALAD: 0.000\n",
      "   - - - - -   \n",
      "Epoch 5 / 400\n",
      "step    0 / 480; loss 1.3834\n",
      "step   40 / 480; loss 1.3733\n",
      "step   80 / 480; loss 1.3741\n",
      "step  120 / 480; loss 1.3737\n",
      "step  160 / 480; loss 1.3841\n",
      "step  200 / 480; loss 1.3667\n",
      "step  240 / 480; loss 1.3779\n",
      "step  280 / 480; loss 1.3818\n",
      "step  320 / 480; loss 1.3782\n",
      "step  360 / 480; loss 1.3815\n",
      "step  400 / 480; loss 1.3821\n",
      "step  440 / 480; loss 1.3738\n",
      "Evaluation loss 1.3780928677973991, accuracy 0.2567875, acc CATHODE: 0.101, acc CURTAINS: 0.544, acc FETA: 0.382, acc SALAD: 0.000\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 6 / 400\n",
      "step    0 / 480; loss 1.3684\n",
      "step   40 / 480; loss 1.3824\n",
      "step   80 / 480; loss 1.3710\n",
      "step  120 / 480; loss 1.3841\n",
      "step  160 / 480; loss 1.3863\n",
      "step  200 / 480; loss 1.3806\n",
      "step  240 / 480; loss 1.3827\n",
      "step  280 / 480; loss 1.3819\n",
      "step  320 / 480; loss 1.3845\n",
      "step  360 / 480; loss 1.3775\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3838\n",
      "Evaluation loss 1.3781540283582616, accuracy 0.25951875, acc CATHODE: 0.048, acc CURTAINS: 0.573, acc FETA: 0.370, acc SALAD: 0.047\n",
      "   - - - - -   \n",
      "Epoch 7 / 400\n",
      "step    0 / 480; loss 1.3794\n",
      "step   40 / 480; loss 1.3698\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3841\n",
      "step  160 / 480; loss 1.3826\n",
      "step  200 / 480; loss 1.3739\n",
      "step  240 / 480; loss 1.3806\n",
      "step  280 / 480; loss 1.3762\n",
      "step  320 / 480; loss 1.3882\n",
      "step  360 / 480; loss 1.3708\n",
      "step  400 / 480; loss 1.3744\n",
      "step  440 / 480; loss 1.3802\n",
      "Evaluation loss 1.378116668223094, accuracy 0.25661875, acc CATHODE: 0.034, acc CURTAINS: 0.545, acc FETA: 0.446, acc SALAD: 0.003\n",
      "   - - - - -   \n",
      "Epoch 8 / 400\n",
      "step    0 / 480; loss 1.3828\n",
      "step   40 / 480; loss 1.3820\n",
      "step   80 / 480; loss 1.3709\n",
      "step  120 / 480; loss 1.3879\n",
      "step  160 / 480; loss 1.3863\n",
      "step  200 / 480; loss 1.3775\n",
      "step  240 / 480; loss 1.3716\n",
      "step  280 / 480; loss 1.3653\n",
      "step  320 / 480; loss 1.3686\n",
      "step  360 / 480; loss 1.3717\n",
      "step  400 / 480; loss 1.3756\n",
      "step  440 / 480; loss 1.3763\n",
      "Evaluation loss 1.3780764104715277, accuracy 0.257365625, acc CATHODE: 0.052, acc CURTAINS: 0.687, acc FETA: 0.283, acc SALAD: 0.008\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 9 / 400\n",
      "step    0 / 480; loss 1.3871\n",
      "step   40 / 480; loss 1.3772\n",
      "step   80 / 480; loss 1.3862\n",
      "step  120 / 480; loss 1.3790\n",
      "step  160 / 480; loss 1.3773\n",
      "step  200 / 480; loss 1.3792\n",
      "step  240 / 480; loss 1.3767\n",
      "step  280 / 480; loss 1.3782\n",
      "step  320 / 480; loss 1.3831\n",
      "step  360 / 480; loss 1.3864\n",
      "step  400 / 480; loss 1.3781\n",
      "step  440 / 480; loss 1.3676\n",
      "Evaluation loss 1.378006527106889, accuracy 0.256759375, acc CATHODE: 0.025, acc CURTAINS: 0.418, acc FETA: 0.583, acc SALAD: 0.000\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 10 / 400\n",
      "step    0 / 480; loss 1.3865\n",
      "step   40 / 480; loss 1.3786\n",
      "step   80 / 480; loss 1.3814\n",
      "step  120 / 480; loss 1.3670\n",
      "step  160 / 480; loss 1.3848\n",
      "step  200 / 480; loss 1.3729\n",
      "step  240 / 480; loss 1.3803\n",
      "step  280 / 480; loss 1.3790\n",
      "step  320 / 480; loss 1.3804\n",
      "step  360 / 480; loss 1.3844\n",
      "step  400 / 480; loss 1.3778\n",
      "step  440 / 480; loss 1.3874\n",
      "Evaluation loss 1.377957432541649, accuracy 0.25681875, acc CATHODE: 0.101, acc CURTAINS: 0.564, acc FETA: 0.361, acc SALAD: 0.000\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 11 / 400\n",
      "step    0 / 480; loss 1.3844\n",
      "step   40 / 480; loss 1.3808\n",
      "step   80 / 480; loss 1.3695\n",
      "step  120 / 480; loss 1.3806\n",
      "step  160 / 480; loss 1.3767\n",
      "step  200 / 480; loss 1.3738\n",
      "step  240 / 480; loss 1.3758\n",
      "step  280 / 480; loss 1.3765\n",
      "step  320 / 480; loss 1.3804\n",
      "step  360 / 480; loss 1.3819\n",
      "step  400 / 480; loss 1.3675\n",
      "step  440 / 480; loss 1.3780\n",
      "Evaluation loss 1.3779479077031058, accuracy 0.25614375, acc CATHODE: 0.176, acc CURTAINS: 0.466, acc FETA: 0.382, acc SALAD: 0.001\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 12 / 400\n",
      "step    0 / 480; loss 1.3805\n",
      "step   40 / 480; loss 1.3680\n",
      "step   80 / 480; loss 1.3728\n",
      "step  120 / 480; loss 1.3760\n",
      "step  160 / 480; loss 1.3705\n",
      "step  200 / 480; loss 1.3718\n",
      "step  240 / 480; loss 1.3788\n",
      "step  280 / 480; loss 1.3965\n",
      "step  320 / 480; loss 1.3740\n",
      "step  360 / 480; loss 1.3902\n",
      "step  400 / 480; loss 1.3662\n",
      "step  440 / 480; loss 1.3849\n",
      "Evaluation loss 1.3780446336583232, accuracy 0.2572125, acc CATHODE: 0.074, acc CURTAINS: 0.638, acc FETA: 0.316, acc SALAD: 0.000\n",
      "   - - - - -   \n",
      "Epoch 13 / 400\n",
      "step    0 / 480; loss 1.3701\n",
      "step   40 / 480; loss 1.3875\n",
      "step   80 / 480; loss 1.3691\n",
      "step  120 / 480; loss 1.3773\n",
      "step  160 / 480; loss 1.3870\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3852\n",
      "step  280 / 480; loss 1.3766\n",
      "step  320 / 480; loss 1.3758\n",
      "step  360 / 480; loss 1.3708\n",
      "step  400 / 480; loss 1.3843\n",
      "step  440 / 480; loss 1.3778\n",
      "Evaluation loss 1.3778798076540195, accuracy 0.25658125, acc CATHODE: 0.100, acc CURTAINS: 0.346, acc FETA: 0.571, acc SALAD: 0.009\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 14 / 400\n",
      "step    0 / 480; loss 1.3788\n",
      "step   40 / 480; loss 1.3801\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3765\n",
      "step  160 / 480; loss 1.3659\n",
      "step  200 / 480; loss 1.3790\n",
      "step  240 / 480; loss 1.3691\n",
      "step  280 / 480; loss 1.3846\n",
      "step  320 / 480; loss 1.3732\n",
      "step  360 / 480; loss 1.3771\n",
      "step  400 / 480; loss 1.3829\n",
      "step  440 / 480; loss 1.3737\n",
      "Evaluation loss 1.3778165754689982, accuracy 0.257046875, acc CATHODE: 0.350, acc CURTAINS: 0.425, acc FETA: 0.251, acc SALAD: 0.002\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 15 / 400\n",
      "step    0 / 480; loss 1.3799\n",
      "step   40 / 480; loss 1.3815\n",
      "step   80 / 480; loss 1.3873\n",
      "step  120 / 480; loss 1.3847\n",
      "step  160 / 480; loss 1.3736\n",
      "step  200 / 480; loss 1.3826\n",
      "step  240 / 480; loss 1.3775\n",
      "step  280 / 480; loss 1.3786\n",
      "step  320 / 480; loss 1.3824\n",
      "step  360 / 480; loss 1.3766\n",
      "step  400 / 480; loss 1.3784\n",
      "step  440 / 480; loss 1.3753\n",
      "Evaluation loss 1.3777986128528392, accuracy 0.258546875, acc CATHODE: 0.375, acc CURTAINS: 0.466, acc FETA: 0.149, acc SALAD: 0.045\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 16 / 400\n",
      "step    0 / 480; loss 1.3789\n",
      "step   40 / 480; loss 1.3787\n",
      "step   80 / 480; loss 1.3768\n",
      "step  120 / 480; loss 1.3810\n",
      "step  160 / 480; loss 1.3714\n",
      "step  200 / 480; loss 1.3696\n",
      "step  240 / 480; loss 1.3678\n",
      "step  280 / 480; loss 1.3797\n",
      "step  320 / 480; loss 1.3751\n",
      "step  360 / 480; loss 1.3767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  400 / 480; loss 1.3756\n",
      "step  440 / 480; loss 1.3868\n",
      "Evaluation loss 1.3778339740545702, accuracy 0.257278125, acc CATHODE: 0.253, acc CURTAINS: 0.633, acc FETA: 0.143, acc SALAD: 0.000\n",
      "   - - - - -   \n",
      "Epoch 17 / 400\n",
      "step    0 / 480; loss 1.3792\n",
      "step   40 / 480; loss 1.3887\n",
      "step   80 / 480; loss 1.3787\n",
      "step  120 / 480; loss 1.3748\n",
      "step  160 / 480; loss 1.3668\n",
      "step  200 / 480; loss 1.3828\n",
      "step  240 / 480; loss 1.3779\n",
      "step  280 / 480; loss 1.3678\n",
      "step  320 / 480; loss 1.3767\n",
      "step  360 / 480; loss 1.3786\n",
      "step  400 / 480; loss 1.3818\n",
      "step  440 / 480; loss 1.3737\n",
      "Evaluation loss 1.3778368385889792, accuracy 0.2568375, acc CATHODE: 0.175, acc CURTAINS: 0.466, acc FETA: 0.384, acc SALAD: 0.003\n",
      "   - - - - -   \n",
      "Epoch 18 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3750\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3710\n",
      "step  160 / 480; loss 1.3718\n",
      "step  200 / 480; loss 1.3795\n",
      "step  240 / 480; loss 1.3752\n",
      "step  280 / 480; loss 1.3758\n",
      "step  320 / 480; loss 1.3772\n",
      "step  360 / 480; loss 1.3749\n",
      "step  400 / 480; loss 1.3671\n",
      "step  440 / 480; loss 1.3791\n",
      "Evaluation loss 1.3776755416166595, accuracy 0.257359375, acc CATHODE: 0.194, acc CURTAINS: 0.417, acc FETA: 0.413, acc SALAD: 0.005\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 19 / 400\n",
      "step    0 / 480; loss 1.3675\n",
      "step   40 / 480; loss 1.3802\n",
      "step   80 / 480; loss 1.3818\n",
      "step  120 / 480; loss 1.3786\n",
      "step  160 / 480; loss 1.3677\n",
      "step  200 / 480; loss 1.3816\n",
      "step  240 / 480; loss 1.3829\n",
      "step  280 / 480; loss 1.3733\n",
      "step  320 / 480; loss 1.3756\n",
      "step  360 / 480; loss 1.3834\n",
      "step  400 / 480; loss 1.3807\n",
      "step  440 / 480; loss 1.3871\n",
      "Evaluation loss 1.3777227535960797, accuracy 0.25739375, acc CATHODE: 0.098, acc CURTAINS: 0.645, acc FETA: 0.267, acc SALAD: 0.019\n",
      "   - - - - -   \n",
      "Epoch 20 / 400\n",
      "step    0 / 480; loss 1.3826\n",
      "step   40 / 480; loss 1.3797\n",
      "step   80 / 480; loss 1.3661\n",
      "step  120 / 480; loss 1.3791\n",
      "step  160 / 480; loss 1.3826\n",
      "step  200 / 480; loss 1.3818\n",
      "step  240 / 480; loss 1.3716\n",
      "step  280 / 480; loss 1.3790\n",
      "step  320 / 480; loss 1.3695\n",
      "step  360 / 480; loss 1.3841\n",
      "step  400 / 480; loss 1.3725\n",
      "step  440 / 480; loss 1.3776\n",
      "Evaluation loss 1.377605935090805, accuracy 0.2574, acc CATHODE: 0.129, acc CURTAINS: 0.553, acc FETA: 0.324, acc SALAD: 0.024\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 21 / 400\n",
      "step    0 / 480; loss 1.3809\n",
      "step   40 / 480; loss 1.3778\n",
      "step   80 / 480; loss 1.3681\n",
      "step  120 / 480; loss 1.3666\n",
      "step  160 / 480; loss 1.3754\n",
      "step  200 / 480; loss 1.3879\n",
      "step  240 / 480; loss 1.3675\n",
      "step  280 / 480; loss 1.3796\n",
      "step  320 / 480; loss 1.3769\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3800\n",
      "step  440 / 480; loss 1.3821\n",
      "Evaluation loss 1.3774911283280473, accuracy 0.2572125, acc CATHODE: 0.138, acc CURTAINS: 0.365, acc FETA: 0.505, acc SALAD: 0.021\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 22 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3789\n",
      "step   80 / 480; loss 1.3810\n",
      "step  120 / 480; loss 1.3824\n",
      "step  160 / 480; loss 1.3765\n",
      "step  200 / 480; loss 1.3650\n",
      "step  240 / 480; loss 1.3754\n",
      "step  280 / 480; loss 1.3769\n",
      "step  320 / 480; loss 1.3705\n",
      "step  360 / 480; loss 1.3741\n",
      "step  400 / 480; loss 1.3844\n",
      "step  440 / 480; loss 1.3783\n",
      "Evaluation loss 1.3775228792227385, accuracy 0.257465625, acc CATHODE: 0.302, acc CURTAINS: 0.408, acc FETA: 0.314, acc SALAD: 0.006\n",
      "   - - - - -   \n",
      "Epoch 23 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3708\n",
      "step   80 / 480; loss 1.3817\n",
      "step  120 / 480; loss 1.3776\n",
      "step  160 / 480; loss 1.3788\n",
      "step  200 / 480; loss 1.3773\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3740\n",
      "step  320 / 480; loss 1.3819\n",
      "step  360 / 480; loss 1.3801\n",
      "step  400 / 480; loss 1.3661\n",
      "step  440 / 480; loss 1.3724\n",
      "Evaluation loss 1.37748912066459, accuracy 0.2575875, acc CATHODE: 0.218, acc CURTAINS: 0.388, acc FETA: 0.419, acc SALAD: 0.006\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 24 / 400\n",
      "step    0 / 480; loss 1.3792\n",
      "step   40 / 480; loss 1.3800\n",
      "step   80 / 480; loss 1.3834\n",
      "step  120 / 480; loss 1.3849\n",
      "step  160 / 480; loss 1.3847\n",
      "step  200 / 480; loss 1.3732\n",
      "step  240 / 480; loss 1.3760\n",
      "step  280 / 480; loss 1.3778\n",
      "step  320 / 480; loss 1.3821\n",
      "step  360 / 480; loss 1.3762\n",
      "step  400 / 480; loss 1.3718\n",
      "step  440 / 480; loss 1.3815\n",
      "Evaluation loss 1.3775396904860708, accuracy 0.25705, acc CATHODE: 0.141, acc CURTAINS: 0.431, acc FETA: 0.446, acc SALAD: 0.011\n",
      "   - - - - -   \n",
      "Epoch 25 / 400\n",
      "step    0 / 480; loss 1.3775\n",
      "step   40 / 480; loss 1.3714\n",
      "step   80 / 480; loss 1.3751\n",
      "step  120 / 480; loss 1.3831\n",
      "step  160 / 480; loss 1.3814\n",
      "step  200 / 480; loss 1.3821\n",
      "step  240 / 480; loss 1.3772\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3789\n",
      "step  360 / 480; loss 1.3814\n",
      "step  400 / 480; loss 1.3833\n",
      "step  440 / 480; loss 1.3797\n",
      "Evaluation loss 1.3774762795618403, accuracy 0.257459375, acc CATHODE: 0.204, acc CURTAINS: 0.502, acc FETA: 0.284, acc SALAD: 0.040\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 26 / 400\n",
      "step    0 / 480; loss 1.3792\n",
      "step   40 / 480; loss 1.3812\n",
      "step   80 / 480; loss 1.3771\n",
      "step  120 / 480; loss 1.3822\n",
      "step  160 / 480; loss 1.3717\n",
      "step  200 / 480; loss 1.3742\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3781\n",
      "step  320 / 480; loss 1.3808\n",
      "step  360 / 480; loss 1.3803\n",
      "step  400 / 480; loss 1.3678\n",
      "step  440 / 480; loss 1.3680\n",
      "Evaluation loss 1.377356138092831, accuracy 0.257903125, acc CATHODE: 0.119, acc CURTAINS: 0.338, acc FETA: 0.534, acc SALAD: 0.042\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 27 / 400\n",
      "step    0 / 480; loss 1.3857\n",
      "step   40 / 480; loss 1.3846\n",
      "step   80 / 480; loss 1.3745\n",
      "step  120 / 480; loss 1.3681\n",
      "step  160 / 480; loss 1.3783\n",
      "step  200 / 480; loss 1.3753\n",
      "step  240 / 480; loss 1.3752\n",
      "step  280 / 480; loss 1.3744\n",
      "step  320 / 480; loss 1.3800\n",
      "step  360 / 480; loss 1.3708\n",
      "step  400 / 480; loss 1.3734\n",
      "step  440 / 480; loss 1.3772\n",
      "Evaluation loss 1.37743506090548, accuracy 0.25771875, acc CATHODE: 0.068, acc CURTAINS: 0.465, acc FETA: 0.483, acc SALAD: 0.015\n",
      "   - - - - -   \n",
      "Epoch 28 / 400\n",
      "step    0 / 480; loss 1.3735\n",
      "step   40 / 480; loss 1.3770\n",
      "step   80 / 480; loss 1.3704\n",
      "step  120 / 480; loss 1.3830\n",
      "step  160 / 480; loss 1.3815\n",
      "step  200 / 480; loss 1.3787\n",
      "step  240 / 480; loss 1.3679\n",
      "step  280 / 480; loss 1.3805\n",
      "step  320 / 480; loss 1.3858\n",
      "step  360 / 480; loss 1.3760\n",
      "step  400 / 480; loss 1.3854\n",
      "step  440 / 480; loss 1.3737\n",
      "Evaluation loss 1.377281375268495, accuracy 0.258165625, acc CATHODE: 0.331, acc CURTAINS: 0.394, acc FETA: 0.264, acc SALAD: 0.044\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 29 / 400\n",
      "step    0 / 480; loss 1.3745\n",
      "step   40 / 480; loss 1.3815\n",
      "step   80 / 480; loss 1.3696\n",
      "step  120 / 480; loss 1.3848\n",
      "step  160 / 480; loss 1.3826\n",
      "step  200 / 480; loss 1.3751\n",
      "step  240 / 480; loss 1.3792\n",
      "step  280 / 480; loss 1.3770\n",
      "step  320 / 480; loss 1.3777\n",
      "step  360 / 480; loss 1.3787\n",
      "step  400 / 480; loss 1.3808\n",
      "step  440 / 480; loss 1.3835\n",
      "Evaluation loss 1.3773496602431174, accuracy 0.258409375, acc CATHODE: 0.316, acc CURTAINS: 0.399, acc FETA: 0.295, acc SALAD: 0.023\n",
      "   - - - - -   \n",
      "Epoch 30 / 400\n",
      "step    0 / 480; loss 1.3796\n",
      "step   40 / 480; loss 1.3823\n",
      "step   80 / 480; loss 1.3811\n",
      "step  120 / 480; loss 1.3775\n",
      "step  160 / 480; loss 1.3749\n",
      "step  200 / 480; loss 1.3800\n",
      "step  240 / 480; loss 1.3815\n",
      "step  280 / 480; loss 1.3805\n",
      "step  320 / 480; loss 1.3856\n",
      "step  360 / 480; loss 1.3784\n",
      "step  400 / 480; loss 1.3776\n",
      "step  440 / 480; loss 1.3758\n",
      "Evaluation loss 1.3773478499105316, accuracy 0.258659375, acc CATHODE: 0.282, acc CURTAINS: 0.398, acc FETA: 0.320, acc SALAD: 0.034\n",
      "   - - - - -   \n",
      "Epoch 31 / 400\n",
      "step    0 / 480; loss 1.3829\n",
      "step   40 / 480; loss 1.3821\n",
      "step   80 / 480; loss 1.3749\n",
      "step  120 / 480; loss 1.3751\n",
      "step  160 / 480; loss 1.3817\n",
      "step  200 / 480; loss 1.3829\n",
      "step  240 / 480; loss 1.3757\n",
      "step  280 / 480; loss 1.3781\n",
      "step  320 / 480; loss 1.3806\n",
      "step  360 / 480; loss 1.3908\n",
      "step  400 / 480; loss 1.3719\n",
      "step  440 / 480; loss 1.3832\n",
      "Evaluation loss 1.3774391703690463, accuracy 0.257971875, acc CATHODE: 0.311, acc CURTAINS: 0.370, acc FETA: 0.339, acc SALAD: 0.012\n",
      "   - - - - -   \n",
      "Epoch 32 / 400\n",
      "step    0 / 480; loss 1.3753\n",
      "step   40 / 480; loss 1.3789\n",
      "step   80 / 480; loss 1.3781\n",
      "step  120 / 480; loss 1.3797\n",
      "step  160 / 480; loss 1.3726\n",
      "step  200 / 480; loss 1.3789\n",
      "step  240 / 480; loss 1.3762\n",
      "step  280 / 480; loss 1.3871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  320 / 480; loss 1.3790\n",
      "step  360 / 480; loss 1.3822\n",
      "step  400 / 480; loss 1.3772\n",
      "step  440 / 480; loss 1.3855\n",
      "Evaluation loss 1.3773227128894039, accuracy 0.25848125, acc CATHODE: 0.214, acc CURTAINS: 0.414, acc FETA: 0.385, acc SALAD: 0.021\n",
      "   - - - - -   \n",
      "Epoch 33 / 400\n",
      "step    0 / 480; loss 1.3718\n",
      "step   40 / 480; loss 1.3699\n",
      "step   80 / 480; loss 1.3712\n",
      "step  120 / 480; loss 1.3725\n",
      "step  160 / 480; loss 1.3766\n",
      "step  200 / 480; loss 1.3832\n",
      "step  240 / 480; loss 1.3843\n",
      "step  280 / 480; loss 1.3725\n",
      "step  320 / 480; loss 1.3852\n",
      "step  360 / 480; loss 1.3738\n",
      "step  400 / 480; loss 1.3746\n",
      "step  440 / 480; loss 1.3739\n",
      "Evaluation loss 1.3772452011176108, accuracy 0.258921875, acc CATHODE: 0.188, acc CURTAINS: 0.390, acc FETA: 0.444, acc SALAD: 0.014\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 34 / 400\n",
      "step    0 / 480; loss 1.3755\n",
      "step   40 / 480; loss 1.3775\n",
      "step   80 / 480; loss 1.3677\n",
      "step  120 / 480; loss 1.3813\n",
      "step  160 / 480; loss 1.3774\n",
      "step  200 / 480; loss 1.3797\n",
      "step  240 / 480; loss 1.3770\n",
      "step  280 / 480; loss 1.3813\n",
      "step  320 / 480; loss 1.3764\n",
      "step  360 / 480; loss 1.3790\n",
      "step  400 / 480; loss 1.3809\n",
      "step  440 / 480; loss 1.3756\n",
      "Evaluation loss 1.3772642719161152, accuracy 0.258775, acc CATHODE: 0.280, acc CURTAINS: 0.344, acc FETA: 0.381, acc SALAD: 0.031\n",
      "   - - - - -   \n",
      "Epoch 35 / 400\n",
      "step    0 / 480; loss 1.3742\n",
      "step   40 / 480; loss 1.3742\n",
      "step   80 / 480; loss 1.3823\n",
      "step  120 / 480; loss 1.3814\n",
      "step  160 / 480; loss 1.3775\n",
      "step  200 / 480; loss 1.3719\n",
      "step  240 / 480; loss 1.3752\n",
      "step  280 / 480; loss 1.3717\n",
      "step  320 / 480; loss 1.3796\n",
      "step  360 / 480; loss 1.3660\n",
      "step  400 / 480; loss 1.3815\n",
      "step  440 / 480; loss 1.3748\n",
      "Evaluation loss 1.3772868631781447, accuracy 0.25880625, acc CATHODE: 0.205, acc CURTAINS: 0.372, acc FETA: 0.436, acc SALAD: 0.022\n",
      "   - - - - -   \n",
      "Epoch 36 / 400\n",
      "step    0 / 480; loss 1.3794\n",
      "step   40 / 480; loss 1.3832\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3776\n",
      "step  160 / 480; loss 1.3643\n",
      "step  200 / 480; loss 1.3764\n",
      "step  240 / 480; loss 1.3855\n",
      "step  280 / 480; loss 1.3771\n",
      "step  320 / 480; loss 1.3748\n",
      "step  360 / 480; loss 1.3839\n",
      "step  400 / 480; loss 1.3708\n",
      "step  440 / 480; loss 1.3728\n",
      "Evaluation loss 1.3771933876908897, accuracy 0.26043125, acc CATHODE: 0.212, acc CURTAINS: 0.369, acc FETA: 0.401, acc SALAD: 0.060\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 37 / 400\n",
      "step    0 / 480; loss 1.3860\n",
      "step   40 / 480; loss 1.3677\n",
      "step   80 / 480; loss 1.3739\n",
      "step  120 / 480; loss 1.3782\n",
      "step  160 / 480; loss 1.3796\n",
      "step  200 / 480; loss 1.3807\n",
      "step  240 / 480; loss 1.3722\n",
      "step  280 / 480; loss 1.3718\n",
      "step  320 / 480; loss 1.3709\n",
      "step  360 / 480; loss 1.3766\n",
      "step  400 / 480; loss 1.3794\n",
      "step  440 / 480; loss 1.3826\n",
      "Evaluation loss 1.3773020968987353, accuracy 0.2590125, acc CATHODE: 0.251, acc CURTAINS: 0.405, acc FETA: 0.301, acc SALAD: 0.079\n",
      "   - - - - -   \n",
      "Epoch 38 / 400\n",
      "step    0 / 480; loss 1.3745\n",
      "step   40 / 480; loss 1.3847\n",
      "step   80 / 480; loss 1.3799\n",
      "step  120 / 480; loss 1.3761\n",
      "step  160 / 480; loss 1.3697\n",
      "step  200 / 480; loss 1.3767\n",
      "step  240 / 480; loss 1.3835\n",
      "step  280 / 480; loss 1.3850\n",
      "step  320 / 480; loss 1.3842\n",
      "step  360 / 480; loss 1.3734\n",
      "step  400 / 480; loss 1.3782\n",
      "step  440 / 480; loss 1.3763\n",
      "Evaluation loss 1.3771849371887444, accuracy 0.259671875, acc CATHODE: 0.148, acc CURTAINS: 0.499, acc FETA: 0.338, acc SALAD: 0.054\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 39 / 400\n",
      "step    0 / 480; loss 1.3840\n",
      "step   40 / 480; loss 1.3718\n",
      "step   80 / 480; loss 1.3815\n",
      "step  120 / 480; loss 1.3825\n",
      "step  160 / 480; loss 1.3706\n",
      "step  200 / 480; loss 1.3866\n",
      "step  240 / 480; loss 1.3731\n",
      "step  280 / 480; loss 1.3766\n",
      "step  320 / 480; loss 1.3759\n",
      "step  360 / 480; loss 1.3736\n",
      "step  400 / 480; loss 1.3814\n",
      "step  440 / 480; loss 1.3794\n",
      "Evaluation loss 1.3771206320663116, accuracy 0.25935625, acc CATHODE: 0.081, acc CURTAINS: 0.333, acc FETA: 0.589, acc SALAD: 0.034\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 40 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3759\n",
      "step   80 / 480; loss 1.3682\n",
      "step  120 / 480; loss 1.3829\n",
      "step  160 / 480; loss 1.3758\n",
      "step  200 / 480; loss 1.3732\n",
      "step  240 / 480; loss 1.3606\n",
      "step  280 / 480; loss 1.3703\n",
      "step  320 / 480; loss 1.3754\n",
      "step  360 / 480; loss 1.3795\n",
      "step  400 / 480; loss 1.3807\n",
      "step  440 / 480; loss 1.3745\n",
      "Evaluation loss 1.377096920168858, accuracy 0.26058125, acc CATHODE: 0.293, acc CURTAINS: 0.382, acc FETA: 0.306, acc SALAD: 0.062\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 41 / 400\n",
      "step    0 / 480; loss 1.3767\n",
      "step   40 / 480; loss 1.3913\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3670\n",
      "step  160 / 480; loss 1.3818\n",
      "step  200 / 480; loss 1.3770\n",
      "step  240 / 480; loss 1.3755\n",
      "step  280 / 480; loss 1.3743\n",
      "step  320 / 480; loss 1.3787\n",
      "step  360 / 480; loss 1.3648\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3689\n",
      "Evaluation loss 1.377254816882256, accuracy 0.25935625, acc CATHODE: 0.175, acc CURTAINS: 0.565, acc FETA: 0.259, acc SALAD: 0.038\n",
      "   - - - - -   \n",
      "Epoch 42 / 400\n",
      "step    0 / 480; loss 1.3776\n",
      "step   40 / 480; loss 1.3674\n",
      "step   80 / 480; loss 1.3702\n",
      "step  120 / 480; loss 1.3837\n",
      "step  160 / 480; loss 1.3701\n",
      "step  200 / 480; loss 1.3697\n",
      "step  240 / 480; loss 1.3740\n",
      "step  280 / 480; loss 1.3740\n",
      "step  320 / 480; loss 1.3745\n",
      "step  360 / 480; loss 1.3829\n",
      "step  400 / 480; loss 1.3722\n",
      "step  440 / 480; loss 1.3720\n",
      "Evaluation loss 1.3770828602827856, accuracy 0.259175, acc CATHODE: 0.151, acc CURTAINS: 0.463, acc FETA: 0.359, acc SALAD: 0.063\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 43 / 400\n",
      "step    0 / 480; loss 1.3777\n",
      "step   40 / 480; loss 1.3751\n",
      "step   80 / 480; loss 1.3832\n",
      "step  120 / 480; loss 1.3828\n",
      "step  160 / 480; loss 1.3778\n",
      "step  200 / 480; loss 1.3808\n",
      "step  240 / 480; loss 1.3722\n",
      "step  280 / 480; loss 1.3744\n",
      "step  320 / 480; loss 1.3796\n",
      "step  360 / 480; loss 1.3735\n",
      "step  400 / 480; loss 1.3731\n",
      "step  440 / 480; loss 1.3779\n",
      "Evaluation loss 1.3770700812213412, accuracy 0.259875, acc CATHODE: 0.314, acc CURTAINS: 0.295, acc FETA: 0.397, acc SALAD: 0.034\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 44 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3765\n",
      "step   80 / 480; loss 1.3784\n",
      "step  120 / 480; loss 1.3901\n",
      "step  160 / 480; loss 1.3798\n",
      "step  200 / 480; loss 1.3730\n",
      "step  240 / 480; loss 1.3785\n",
      "step  280 / 480; loss 1.3757\n",
      "step  320 / 480; loss 1.3790\n",
      "step  360 / 480; loss 1.3842\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3870\n",
      "Evaluation loss 1.3770469383133124, accuracy 0.26035625, acc CATHODE: 0.206, acc CURTAINS: 0.339, acc FETA: 0.432, acc SALAD: 0.065\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 45 / 400\n",
      "step    0 / 480; loss 1.3836\n",
      "step   40 / 480; loss 1.3814\n",
      "step   80 / 480; loss 1.3852\n",
      "step  120 / 480; loss 1.3731\n",
      "step  160 / 480; loss 1.3767\n",
      "step  200 / 480; loss 1.3834\n",
      "step  240 / 480; loss 1.3742\n",
      "step  280 / 480; loss 1.3620\n",
      "step  320 / 480; loss 1.3765\n",
      "step  360 / 480; loss 1.3792\n",
      "step  400 / 480; loss 1.3823\n",
      "step  440 / 480; loss 1.3729\n",
      "Evaluation loss 1.3770612707233614, accuracy 0.259753125, acc CATHODE: 0.258, acc CURTAINS: 0.391, acc FETA: 0.342, acc SALAD: 0.048\n",
      "   - - - - -   \n",
      "Epoch 46 / 400\n",
      "step    0 / 480; loss 1.3778\n",
      "step   40 / 480; loss 1.3816\n",
      "step   80 / 480; loss 1.3735\n",
      "step  120 / 480; loss 1.3767\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3702\n",
      "step  240 / 480; loss 1.3688\n",
      "step  280 / 480; loss 1.3774\n",
      "step  320 / 480; loss 1.3735\n",
      "step  360 / 480; loss 1.3909\n",
      "step  400 / 480; loss 1.3859\n",
      "step  440 / 480; loss 1.3805\n",
      "Evaluation loss 1.3770075096267471, accuracy 0.25949375, acc CATHODE: 0.251, acc CURTAINS: 0.424, acc FETA: 0.292, acc SALAD: 0.072\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 47 / 400\n",
      "step    0 / 480; loss 1.3759\n",
      "step   40 / 480; loss 1.3776\n",
      "step   80 / 480; loss 1.3794\n",
      "step  120 / 480; loss 1.3764\n",
      "step  160 / 480; loss 1.3756\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3678\n",
      "step  280 / 480; loss 1.3740\n",
      "step  320 / 480; loss 1.3793\n",
      "step  360 / 480; loss 1.3676\n",
      "step  400 / 480; loss 1.3788\n",
      "step  440 / 480; loss 1.3741\n",
      "Evaluation loss 1.3771306352347972, accuracy 0.260384375, acc CATHODE: 0.169, acc CURTAINS: 0.530, acc FETA: 0.292, acc SALAD: 0.051\n",
      "   - - - - -   \n",
      "Epoch 48 / 400\n",
      "step    0 / 480; loss 1.3750\n",
      "step   40 / 480; loss 1.3715\n",
      "step   80 / 480; loss 1.3759\n",
      "step  120 / 480; loss 1.3729\n",
      "step  160 / 480; loss 1.3747\n",
      "step  200 / 480; loss 1.3801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  240 / 480; loss 1.3749\n",
      "step  280 / 480; loss 1.3743\n",
      "step  320 / 480; loss 1.3716\n",
      "step  360 / 480; loss 1.3702\n",
      "step  400 / 480; loss 1.3776\n",
      "step  440 / 480; loss 1.3722\n",
      "Evaluation loss 1.3769896096598992, accuracy 0.260621875, acc CATHODE: 0.204, acc CURTAINS: 0.438, acc FETA: 0.353, acc SALAD: 0.047\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 49 / 400\n",
      "step    0 / 480; loss 1.3793\n",
      "step   40 / 480; loss 1.3766\n",
      "step   80 / 480; loss 1.3717\n",
      "step  120 / 480; loss 1.3800\n",
      "step  160 / 480; loss 1.3735\n",
      "step  200 / 480; loss 1.3717\n",
      "step  240 / 480; loss 1.3848\n",
      "step  280 / 480; loss 1.3772\n",
      "step  320 / 480; loss 1.3775\n",
      "step  360 / 480; loss 1.3707\n",
      "step  400 / 480; loss 1.3817\n",
      "step  440 / 480; loss 1.3756\n",
      "Evaluation loss 1.3770005950054884, accuracy 0.261834375, acc CATHODE: 0.230, acc CURTAINS: 0.406, acc FETA: 0.359, acc SALAD: 0.053\n",
      "   - - - - -   \n",
      "Epoch 50 / 400\n",
      "step    0 / 480; loss 1.3824\n",
      "step   40 / 480; loss 1.3752\n",
      "step   80 / 480; loss 1.3703\n",
      "step  120 / 480; loss 1.3738\n",
      "step  160 / 480; loss 1.3698\n",
      "step  200 / 480; loss 1.3749\n",
      "step  240 / 480; loss 1.3766\n",
      "step  280 / 480; loss 1.3693\n",
      "step  320 / 480; loss 1.3783\n",
      "step  360 / 480; loss 1.3756\n",
      "step  400 / 480; loss 1.3733\n",
      "step  440 / 480; loss 1.3707\n",
      "Evaluation loss 1.377207675282755, accuracy 0.259478125, acc CATHODE: 0.230, acc CURTAINS: 0.575, acc FETA: 0.197, acc SALAD: 0.036\n",
      "   - - - - -   \n",
      "Epoch 51 / 400\n",
      "step    0 / 480; loss 1.3818\n",
      "step   40 / 480; loss 1.3803\n",
      "step   80 / 480; loss 1.3783\n",
      "step  120 / 480; loss 1.3776\n",
      "step  160 / 480; loss 1.3878\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3714\n",
      "step  280 / 480; loss 1.3713\n",
      "step  320 / 480; loss 1.3667\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3868\n",
      "step  440 / 480; loss 1.3807\n",
      "Evaluation loss 1.3770836740028503, accuracy 0.258953125, acc CATHODE: 0.266, acc CURTAINS: 0.430, acc FETA: 0.307, acc SALAD: 0.032\n",
      "   - - - - -   \n",
      "Epoch 52 / 400\n",
      "step    0 / 480; loss 1.3772\n",
      "step   40 / 480; loss 1.3800\n",
      "step   80 / 480; loss 1.3712\n",
      "step  120 / 480; loss 1.3749\n",
      "step  160 / 480; loss 1.3819\n",
      "step  200 / 480; loss 1.3732\n",
      "step  240 / 480; loss 1.3696\n",
      "step  280 / 480; loss 1.3701\n",
      "step  320 / 480; loss 1.3812\n",
      "step  360 / 480; loss 1.3785\n",
      "step  400 / 480; loss 1.3798\n",
      "step  440 / 480; loss 1.3717\n",
      "Evaluation loss 1.3769740882220032, accuracy 0.260065625, acc CATHODE: 0.208, acc CURTAINS: 0.405, acc FETA: 0.389, acc SALAD: 0.039\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 53 / 400\n",
      "step    0 / 480; loss 1.3799\n",
      "step   40 / 480; loss 1.3751\n",
      "step   80 / 480; loss 1.3855\n",
      "step  120 / 480; loss 1.3736\n",
      "step  160 / 480; loss 1.3734\n",
      "step  200 / 480; loss 1.3735\n",
      "step  240 / 480; loss 1.3748\n",
      "step  280 / 480; loss 1.3842\n",
      "step  320 / 480; loss 1.3746\n",
      "step  360 / 480; loss 1.3775\n",
      "step  400 / 480; loss 1.3740\n",
      "step  440 / 480; loss 1.3649\n",
      "Evaluation loss 1.3770007518061156, accuracy 0.261459375, acc CATHODE: 0.194, acc CURTAINS: 0.303, acc FETA: 0.517, acc SALAD: 0.032\n",
      "   - - - - -   \n",
      "Epoch 54 / 400\n",
      "step    0 / 480; loss 1.3828\n",
      "step   40 / 480; loss 1.3769\n",
      "step   80 / 480; loss 1.3888\n",
      "step  120 / 480; loss 1.3759\n",
      "step  160 / 480; loss 1.3680\n",
      "step  200 / 480; loss 1.3775\n",
      "step  240 / 480; loss 1.3814\n",
      "step  280 / 480; loss 1.3722\n",
      "step  320 / 480; loss 1.3715\n",
      "step  360 / 480; loss 1.3831\n",
      "step  400 / 480; loss 1.3833\n",
      "step  440 / 480; loss 1.3810\n",
      "Evaluation loss 1.3769820695219523, accuracy 0.26000625, acc CATHODE: 0.250, acc CURTAINS: 0.348, acc FETA: 0.394, acc SALAD: 0.049\n",
      "   - - - - -   \n",
      "Epoch 55 / 400\n",
      "step    0 / 480; loss 1.3793\n",
      "step   40 / 480; loss 1.3855\n",
      "step   80 / 480; loss 1.3866\n",
      "step  120 / 480; loss 1.3701\n",
      "step  160 / 480; loss 1.3777\n",
      "step  200 / 480; loss 1.3748\n",
      "step  240 / 480; loss 1.3776\n",
      "step  280 / 480; loss 1.3724\n",
      "step  320 / 480; loss 1.3721\n",
      "step  360 / 480; loss 1.3823\n",
      "step  400 / 480; loss 1.3659\n",
      "step  440 / 480; loss 1.3695\n",
      "Evaluation loss 1.3769509286670278, accuracy 0.260996875, acc CATHODE: 0.185, acc CURTAINS: 0.406, acc FETA: 0.386, acc SALAD: 0.067\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 56 / 400\n",
      "step    0 / 480; loss 1.3719\n",
      "step   40 / 480; loss 1.3837\n",
      "step   80 / 480; loss 1.3695\n",
      "step  120 / 480; loss 1.3677\n",
      "step  160 / 480; loss 1.3824\n",
      "step  200 / 480; loss 1.3772\n",
      "step  240 / 480; loss 1.3729\n",
      "step  280 / 480; loss 1.3780\n",
      "step  320 / 480; loss 1.3802\n",
      "step  360 / 480; loss 1.3707\n",
      "step  400 / 480; loss 1.3808\n",
      "step  440 / 480; loss 1.3771\n",
      "Evaluation loss 1.376927953244416, accuracy 0.2607625, acc CATHODE: 0.210, acc CURTAINS: 0.268, acc FETA: 0.500, acc SALAD: 0.065\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 57 / 400\n",
      "step    0 / 480; loss 1.3764\n",
      "step   40 / 480; loss 1.3789\n",
      "step   80 / 480; loss 1.3707\n",
      "step  120 / 480; loss 1.3774\n",
      "step  160 / 480; loss 1.3827\n",
      "step  200 / 480; loss 1.3757\n",
      "step  240 / 480; loss 1.3768\n",
      "step  280 / 480; loss 1.3871\n",
      "step  320 / 480; loss 1.3822\n",
      "step  360 / 480; loss 1.3768\n",
      "step  400 / 480; loss 1.3776\n",
      "step  440 / 480; loss 1.3754\n",
      "Evaluation loss 1.377000862727044, accuracy 0.2615, acc CATHODE: 0.277, acc CURTAINS: 0.378, acc FETA: 0.349, acc SALAD: 0.042\n",
      "   - - - - -   \n",
      "Epoch 58 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3664\n",
      "step   80 / 480; loss 1.3835\n",
      "step  120 / 480; loss 1.3662\n",
      "step  160 / 480; loss 1.3698\n",
      "step  200 / 480; loss 1.3749\n",
      "step  240 / 480; loss 1.3805\n",
      "step  280 / 480; loss 1.3770\n",
      "step  320 / 480; loss 1.3725\n",
      "step  360 / 480; loss 1.3755\n",
      "step  400 / 480; loss 1.3796\n",
      "step  440 / 480; loss 1.3790\n",
      "Evaluation loss 1.376931971646763, accuracy 0.262471875, acc CATHODE: 0.307, acc CURTAINS: 0.301, acc FETA: 0.393, acc SALAD: 0.049\n",
      "   - - - - -   \n",
      "Epoch 59 / 400\n",
      "step    0 / 480; loss 1.3724\n",
      "step   40 / 480; loss 1.3767\n",
      "step   80 / 480; loss 1.3863\n",
      "step  120 / 480; loss 1.3715\n",
      "step  160 / 480; loss 1.3708\n",
      "step  200 / 480; loss 1.3840\n",
      "step  240 / 480; loss 1.3788\n",
      "step  280 / 480; loss 1.3793\n",
      "step  320 / 480; loss 1.3772\n",
      "step  360 / 480; loss 1.3742\n",
      "step  400 / 480; loss 1.3688\n",
      "step  440 / 480; loss 1.3741\n",
      "Evaluation loss 1.376986438279518, accuracy 0.260175, acc CATHODE: 0.084, acc CURTAINS: 0.364, acc FETA: 0.560, acc SALAD: 0.033\n",
      "   - - - - -   \n",
      "Epoch 60 / 400\n",
      "step    0 / 480; loss 1.3676\n",
      "step   40 / 480; loss 1.3678\n",
      "step   80 / 480; loss 1.3669\n",
      "step  120 / 480; loss 1.3794\n",
      "step  160 / 480; loss 1.3818\n",
      "step  200 / 480; loss 1.3911\n",
      "step  240 / 480; loss 1.3833\n",
      "step  280 / 480; loss 1.3850\n",
      "step  320 / 480; loss 1.3725\n",
      "step  360 / 480; loss 1.3790\n",
      "step  400 / 480; loss 1.3843\n",
      "step  440 / 480; loss 1.3804\n",
      "Evaluation loss 1.3769645749195376, accuracy 0.26005, acc CATHODE: 0.172, acc CURTAINS: 0.322, acc FETA: 0.504, acc SALAD: 0.042\n",
      "   - - - - -   \n",
      "Epoch 61 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3784\n",
      "step   80 / 480; loss 1.3802\n",
      "step  120 / 480; loss 1.3760\n",
      "step  160 / 480; loss 1.3756\n",
      "step  200 / 480; loss 1.3712\n",
      "step  240 / 480; loss 1.3795\n",
      "step  280 / 480; loss 1.3872\n",
      "step  320 / 480; loss 1.3819\n",
      "step  360 / 480; loss 1.3735\n",
      "step  400 / 480; loss 1.3780\n",
      "step  440 / 480; loss 1.3698\n",
      "Evaluation loss 1.3770514394953235, accuracy 0.25995625, acc CATHODE: 0.216, acc CURTAINS: 0.270, acc FETA: 0.494, acc SALAD: 0.060\n",
      "   - - - - -   \n",
      "Epoch 62 / 400\n",
      "step    0 / 480; loss 1.3839\n",
      "step   40 / 480; loss 1.3769\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3731\n",
      "step  160 / 480; loss 1.3737\n",
      "step  200 / 480; loss 1.3760\n",
      "step  240 / 480; loss 1.3773\n",
      "step  280 / 480; loss 1.3725\n",
      "step  320 / 480; loss 1.3634\n",
      "step  360 / 480; loss 1.3760\n",
      "step  400 / 480; loss 1.3730\n",
      "step  440 / 480; loss 1.3740\n",
      "Evaluation loss 1.3768961091999998, accuracy 0.26074375, acc CATHODE: 0.233, acc CURTAINS: 0.386, acc FETA: 0.393, acc SALAD: 0.030\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 63 / 400\n",
      "step    0 / 480; loss 1.3746\n",
      "step   40 / 480; loss 1.3781\n",
      "step   80 / 480; loss 1.3769\n",
      "step  120 / 480; loss 1.3724\n",
      "step  160 / 480; loss 1.3757\n",
      "step  200 / 480; loss 1.3823\n",
      "step  240 / 480; loss 1.3790\n",
      "step  280 / 480; loss 1.3769\n",
      "step  320 / 480; loss 1.3838\n",
      "step  360 / 480; loss 1.3791\n",
      "step  400 / 480; loss 1.3869\n",
      "step  440 / 480; loss 1.3768\n",
      "Evaluation loss 1.376889051645772, accuracy 0.261771875, acc CATHODE: 0.161, acc CURTAINS: 0.370, acc FETA: 0.430, acc SALAD: 0.087\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 64 / 400\n",
      "step    0 / 480; loss 1.3765\n",
      "step   40 / 480; loss 1.3742\n",
      "step   80 / 480; loss 1.3711\n",
      "step  120 / 480; loss 1.3765\n",
      "step  160 / 480; loss 1.3677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200 / 480; loss 1.3724\n",
      "step  240 / 480; loss 1.3631\n",
      "step  280 / 480; loss 1.3806\n",
      "step  320 / 480; loss 1.3810\n",
      "step  360 / 480; loss 1.3760\n",
      "step  400 / 480; loss 1.3754\n",
      "step  440 / 480; loss 1.3753\n",
      "Evaluation loss 1.3768617626838944, accuracy 0.2620875, acc CATHODE: 0.158, acc CURTAINS: 0.412, acc FETA: 0.429, acc SALAD: 0.049\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 65 / 400\n",
      "step    0 / 480; loss 1.3783\n",
      "step   40 / 480; loss 1.3769\n",
      "step   80 / 480; loss 1.3834\n",
      "step  120 / 480; loss 1.3767\n",
      "step  160 / 480; loss 1.3897\n",
      "step  200 / 480; loss 1.3833\n",
      "step  240 / 480; loss 1.3781\n",
      "step  280 / 480; loss 1.3668\n",
      "step  320 / 480; loss 1.3762\n",
      "step  360 / 480; loss 1.3706\n",
      "step  400 / 480; loss 1.3769\n",
      "step  440 / 480; loss 1.3813\n",
      "Evaluation loss 1.3769755933810894, accuracy 0.259634375, acc CATHODE: 0.244, acc CURTAINS: 0.481, acc FETA: 0.289, acc SALAD: 0.024\n",
      "   - - - - -   \n",
      "Epoch 66 / 400\n",
      "step    0 / 480; loss 1.3693\n",
      "step   40 / 480; loss 1.3734\n",
      "step   80 / 480; loss 1.3784\n",
      "step  120 / 480; loss 1.3662\n",
      "step  160 / 480; loss 1.3773\n",
      "step  200 / 480; loss 1.3697\n",
      "step  240 / 480; loss 1.3710\n",
      "step  280 / 480; loss 1.3757\n",
      "step  320 / 480; loss 1.3738\n",
      "step  360 / 480; loss 1.3805\n",
      "step  400 / 480; loss 1.3833\n",
      "step  440 / 480; loss 1.3785\n",
      "Evaluation loss 1.3769135558251029, accuracy 0.2614625, acc CATHODE: 0.165, acc CURTAINS: 0.267, acc FETA: 0.527, acc SALAD: 0.086\n",
      "   - - - - -   \n",
      "Epoch 67 / 400\n",
      "step    0 / 480; loss 1.3789\n",
      "step   40 / 480; loss 1.3859\n",
      "step   80 / 480; loss 1.3720\n",
      "step  120 / 480; loss 1.3819\n",
      "step  160 / 480; loss 1.3767\n",
      "step  200 / 480; loss 1.3679\n",
      "step  240 / 480; loss 1.3751\n",
      "step  280 / 480; loss 1.3704\n",
      "step  320 / 480; loss 1.3709\n",
      "step  360 / 480; loss 1.3763\n",
      "step  400 / 480; loss 1.3675\n",
      "step  440 / 480; loss 1.3764\n",
      "Evaluation loss 1.376898133986666, accuracy 0.263446875, acc CATHODE: 0.157, acc CURTAINS: 0.356, acc FETA: 0.431, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 68 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3799\n",
      "step   80 / 480; loss 1.3813\n",
      "step  120 / 480; loss 1.3801\n",
      "step  160 / 480; loss 1.3670\n",
      "step  200 / 480; loss 1.3763\n",
      "step  240 / 480; loss 1.3712\n",
      "step  280 / 480; loss 1.3838\n",
      "step  320 / 480; loss 1.3788\n",
      "step  360 / 480; loss 1.3810\n",
      "step  400 / 480; loss 1.3800\n",
      "step  440 / 480; loss 1.3683\n",
      "Evaluation loss 1.3768247764287886, accuracy 0.261171875, acc CATHODE: 0.263, acc CURTAINS: 0.301, acc FETA: 0.440, acc SALAD: 0.040\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 69 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3760\n",
      "step   80 / 480; loss 1.3724\n",
      "step  120 / 480; loss 1.3771\n",
      "step  160 / 480; loss 1.3682\n",
      "step  200 / 480; loss 1.3715\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3762\n",
      "step  320 / 480; loss 1.3728\n",
      "step  360 / 480; loss 1.3778\n",
      "step  400 / 480; loss 1.3741\n",
      "step  440 / 480; loss 1.3867\n",
      "Evaluation loss 1.3768511432699464, accuracy 0.26181875, acc CATHODE: 0.296, acc CURTAINS: 0.310, acc FETA: 0.366, acc SALAD: 0.075\n",
      "   - - - - -   \n",
      "Epoch 70 / 400\n",
      "step    0 / 480; loss 1.3768\n",
      "step   40 / 480; loss 1.3748\n",
      "step   80 / 480; loss 1.3684\n",
      "step  120 / 480; loss 1.3786\n",
      "step  160 / 480; loss 1.3742\n",
      "step  200 / 480; loss 1.3769\n",
      "step  240 / 480; loss 1.3791\n",
      "step  280 / 480; loss 1.3745\n",
      "step  320 / 480; loss 1.3810\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3678\n",
      "step  440 / 480; loss 1.3765\n",
      "Evaluation loss 1.3768925306575799, accuracy 0.262103125, acc CATHODE: 0.253, acc CURTAINS: 0.408, acc FETA: 0.298, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 71 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3771\n",
      "step   80 / 480; loss 1.3814\n",
      "step  120 / 480; loss 1.3713\n",
      "step  160 / 480; loss 1.3748\n",
      "step  200 / 480; loss 1.3778\n",
      "step  240 / 480; loss 1.3783\n",
      "step  280 / 480; loss 1.3786\n",
      "step  320 / 480; loss 1.3788\n",
      "step  360 / 480; loss 1.3669\n",
      "step  400 / 480; loss 1.3680\n",
      "step  440 / 480; loss 1.3775\n",
      "Evaluation loss 1.3768623961644213, accuracy 0.26153125, acc CATHODE: 0.227, acc CURTAINS: 0.426, acc FETA: 0.313, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 72 / 400\n",
      "step    0 / 480; loss 1.3764\n",
      "step   40 / 480; loss 1.3770\n",
      "step   80 / 480; loss 1.3758\n",
      "step  120 / 480; loss 1.3751\n",
      "step  160 / 480; loss 1.3711\n",
      "step  200 / 480; loss 1.3725\n",
      "step  240 / 480; loss 1.3748\n",
      "step  280 / 480; loss 1.3637\n",
      "step  320 / 480; loss 1.3722\n",
      "step  360 / 480; loss 1.3738\n",
      "step  400 / 480; loss 1.3810\n",
      "step  440 / 480; loss 1.3790\n",
      "Evaluation loss 1.3767848684905057, accuracy 0.263078125, acc CATHODE: 0.248, acc CURTAINS: 0.318, acc FETA: 0.403, acc SALAD: 0.083\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 73 / 400\n",
      "step    0 / 480; loss 1.3768\n",
      "step   40 / 480; loss 1.3682\n",
      "step   80 / 480; loss 1.3861\n",
      "step  120 / 480; loss 1.3743\n",
      "step  160 / 480; loss 1.3730\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3768\n",
      "step  280 / 480; loss 1.3754\n",
      "step  320 / 480; loss 1.3755\n",
      "step  360 / 480; loss 1.3703\n",
      "step  400 / 480; loss 1.3589\n",
      "step  440 / 480; loss 1.3722\n",
      "Evaluation loss 1.3767749666415448, accuracy 0.262346875, acc CATHODE: 0.160, acc CURTAINS: 0.463, acc FETA: 0.375, acc SALAD: 0.052\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 74 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3825\n",
      "step   80 / 480; loss 1.3676\n",
      "step  120 / 480; loss 1.3786\n",
      "step  160 / 480; loss 1.3749\n",
      "step  200 / 480; loss 1.3815\n",
      "step  240 / 480; loss 1.3766\n",
      "step  280 / 480; loss 1.3767\n",
      "step  320 / 480; loss 1.3764\n",
      "step  360 / 480; loss 1.3732\n",
      "step  400 / 480; loss 1.3796\n",
      "step  440 / 480; loss 1.3712\n",
      "Evaluation loss 1.3768266305188843, accuracy 0.261115625, acc CATHODE: 0.195, acc CURTAINS: 0.354, acc FETA: 0.442, acc SALAD: 0.053\n",
      "   - - - - -   \n",
      "Epoch 75 / 400\n",
      "step    0 / 480; loss 1.3795\n",
      "step   40 / 480; loss 1.3797\n",
      "step   80 / 480; loss 1.3746\n",
      "step  120 / 480; loss 1.3620\n",
      "step  160 / 480; loss 1.3808\n",
      "step  200 / 480; loss 1.3753\n",
      "step  240 / 480; loss 1.3713\n",
      "step  280 / 480; loss 1.3851\n",
      "step  320 / 480; loss 1.3805\n",
      "step  360 / 480; loss 1.3851\n",
      "step  400 / 480; loss 1.3908\n",
      "step  440 / 480; loss 1.3791\n",
      "Evaluation loss 1.3767960030006743, accuracy 0.26221875, acc CATHODE: 0.274, acc CURTAINS: 0.334, acc FETA: 0.373, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 76 / 400\n",
      "step    0 / 480; loss 1.3716\n",
      "step   40 / 480; loss 1.3743\n",
      "step   80 / 480; loss 1.3747\n",
      "step  120 / 480; loss 1.3742\n",
      "step  160 / 480; loss 1.3777\n",
      "step  200 / 480; loss 1.3768\n",
      "step  240 / 480; loss 1.3942\n",
      "step  280 / 480; loss 1.3765\n",
      "step  320 / 480; loss 1.3744\n",
      "step  360 / 480; loss 1.3741\n",
      "step  400 / 480; loss 1.3774\n",
      "step  440 / 480; loss 1.3730\n",
      "Evaluation loss 1.3769236816485961, accuracy 0.260984375, acc CATHODE: 0.122, acc CURTAINS: 0.333, acc FETA: 0.536, acc SALAD: 0.052\n",
      "   - - - - -   \n",
      "Epoch 77 / 400\n",
      "step    0 / 480; loss 1.3755\n",
      "step   40 / 480; loss 1.3752\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3889\n",
      "step  160 / 480; loss 1.3846\n",
      "step  200 / 480; loss 1.3662\n",
      "step  240 / 480; loss 1.3862\n",
      "step  280 / 480; loss 1.3743\n",
      "step  320 / 480; loss 1.3729\n",
      "step  360 / 480; loss 1.3837\n",
      "step  400 / 480; loss 1.3752\n",
      "step  440 / 480; loss 1.3723\n",
      "Evaluation loss 1.3768381413999937, accuracy 0.2629875, acc CATHODE: 0.194, acc CURTAINS: 0.419, acc FETA: 0.381, acc SALAD: 0.059\n",
      "   - - - - -   \n",
      "Epoch 78 / 400\n",
      "step    0 / 480; loss 1.3745\n",
      "step   40 / 480; loss 1.3750\n",
      "step   80 / 480; loss 1.3730\n",
      "step  120 / 480; loss 1.3798\n",
      "step  160 / 480; loss 1.3784\n",
      "step  200 / 480; loss 1.3833\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3715\n",
      "step  320 / 480; loss 1.3762\n",
      "step  360 / 480; loss 1.3785\n",
      "step  400 / 480; loss 1.3757\n",
      "step  440 / 480; loss 1.3829\n",
      "Evaluation loss 1.3768772492044887, accuracy 0.262175, acc CATHODE: 0.199, acc CURTAINS: 0.297, acc FETA: 0.499, acc SALAD: 0.054\n",
      "   - - - - -   \n",
      "Epoch 79 / 400\n",
      "step    0 / 480; loss 1.3840\n",
      "step   40 / 480; loss 1.3769\n",
      "step   80 / 480; loss 1.3749\n",
      "step  120 / 480; loss 1.3770\n",
      "step  160 / 480; loss 1.3864\n",
      "step  200 / 480; loss 1.3767\n",
      "step  240 / 480; loss 1.3706\n",
      "step  280 / 480; loss 1.3814\n",
      "step  320 / 480; loss 1.3701\n",
      "step  360 / 480; loss 1.3784\n",
      "step  400 / 480; loss 1.3819\n",
      "step  440 / 480; loss 1.3751\n",
      "Evaluation loss 1.376911408659603, accuracy 0.26213125, acc CATHODE: 0.134, acc CURTAINS: 0.426, acc FETA: 0.426, acc SALAD: 0.062\n",
      "   - - - - -   \n",
      "Epoch 80 / 400\n",
      "step    0 / 480; loss 1.3835\n",
      "step   40 / 480; loss 1.3776\n",
      "step   80 / 480; loss 1.3784\n",
      "step  120 / 480; loss 1.3753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  160 / 480; loss 1.3762\n",
      "step  200 / 480; loss 1.3824\n",
      "step  240 / 480; loss 1.3720\n",
      "step  280 / 480; loss 1.3794\n",
      "step  320 / 480; loss 1.3778\n",
      "step  360 / 480; loss 1.3639\n",
      "step  400 / 480; loss 1.3809\n",
      "step  440 / 480; loss 1.3698\n",
      "Evaluation loss 1.376874020969473, accuracy 0.2621625, acc CATHODE: 0.255, acc CURTAINS: 0.316, acc FETA: 0.442, acc SALAD: 0.036\n",
      "   - - - - -   \n",
      "Epoch 81 / 400\n",
      "step    0 / 480; loss 1.3709\n",
      "step   40 / 480; loss 1.3759\n",
      "step   80 / 480; loss 1.3798\n",
      "step  120 / 480; loss 1.3779\n",
      "step  160 / 480; loss 1.3758\n",
      "step  200 / 480; loss 1.3803\n",
      "step  240 / 480; loss 1.3782\n",
      "step  280 / 480; loss 1.3910\n",
      "step  320 / 480; loss 1.3784\n",
      "step  360 / 480; loss 1.3812\n",
      "step  400 / 480; loss 1.3769\n",
      "step  440 / 480; loss 1.3661\n",
      "Evaluation loss 1.3770022417054701, accuracy 0.26231875, acc CATHODE: 0.274, acc CURTAINS: 0.198, acc FETA: 0.485, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 82 / 400\n",
      "step    0 / 480; loss 1.3845\n",
      "step   40 / 480; loss 1.3782\n",
      "step   80 / 480; loss 1.3724\n",
      "step  120 / 480; loss 1.3789\n",
      "step  160 / 480; loss 1.3734\n",
      "step  200 / 480; loss 1.3766\n",
      "step  240 / 480; loss 1.3737\n",
      "step  280 / 480; loss 1.3760\n",
      "step  320 / 480; loss 1.3740\n",
      "step  360 / 480; loss 1.3818\n",
      "step  400 / 480; loss 1.3779\n",
      "step  440 / 480; loss 1.3708\n",
      "Evaluation loss 1.376952133425484, accuracy 0.26321875, acc CATHODE: 0.240, acc CURTAINS: 0.308, acc FETA: 0.427, acc SALAD: 0.078\n",
      "   - - - - -   \n",
      "Epoch 83 / 400\n",
      "step    0 / 480; loss 1.3775\n",
      "step   40 / 480; loss 1.3690\n",
      "step   80 / 480; loss 1.3813\n",
      "step  120 / 480; loss 1.3761\n",
      "step  160 / 480; loss 1.3722\n",
      "step  200 / 480; loss 1.3742\n",
      "step  240 / 480; loss 1.3784\n",
      "step  280 / 480; loss 1.3776\n",
      "step  320 / 480; loss 1.3707\n",
      "step  360 / 480; loss 1.3711\n",
      "step  400 / 480; loss 1.3786\n",
      "step  440 / 480; loss 1.3756\n",
      "Evaluation loss 1.376863412911901, accuracy 0.262365625, acc CATHODE: 0.194, acc CURTAINS: 0.418, acc FETA: 0.359, acc SALAD: 0.079\n",
      "   - - - - -   \n",
      "Epoch 84 / 400\n",
      "step    0 / 480; loss 1.3758\n",
      "step   40 / 480; loss 1.3813\n",
      "step   80 / 480; loss 1.3735\n",
      "step  120 / 480; loss 1.3738\n",
      "step  160 / 480; loss 1.3778\n",
      "step  200 / 480; loss 1.3828\n",
      "step  240 / 480; loss 1.3757\n",
      "step  280 / 480; loss 1.3779\n",
      "step  320 / 480; loss 1.3725\n",
      "step  360 / 480; loss 1.3829\n",
      "step  400 / 480; loss 1.3830\n",
      "step  440 / 480; loss 1.3707\n",
      "Evaluation loss 1.3768297465892996, accuracy 0.262428125, acc CATHODE: 0.151, acc CURTAINS: 0.358, acc FETA: 0.452, acc SALAD: 0.088\n",
      "   - - - - -   \n",
      "Epoch 85 / 400\n",
      "step    0 / 480; loss 1.3662\n",
      "step   40 / 480; loss 1.3740\n",
      "step   80 / 480; loss 1.3750\n",
      "step  120 / 480; loss 1.3675\n",
      "step  160 / 480; loss 1.3624\n",
      "step  200 / 480; loss 1.3769\n",
      "step  240 / 480; loss 1.3842\n",
      "step  280 / 480; loss 1.3805\n",
      "step  320 / 480; loss 1.3746\n",
      "step  360 / 480; loss 1.3845\n",
      "step  400 / 480; loss 1.3783\n",
      "step  440 / 480; loss 1.3747\n",
      "Evaluation loss 1.3769363046486212, accuracy 0.263109375, acc CATHODE: 0.201, acc CURTAINS: 0.386, acc FETA: 0.386, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 86 / 400\n",
      "step    0 / 480; loss 1.3772\n",
      "step   40 / 480; loss 1.3815\n",
      "step   80 / 480; loss 1.3652\n",
      "step  120 / 480; loss 1.3732\n",
      "step  160 / 480; loss 1.3769\n",
      "step  200 / 480; loss 1.3765\n",
      "step  240 / 480; loss 1.3718\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3703\n",
      "step  360 / 480; loss 1.3661\n",
      "step  400 / 480; loss 1.3819\n",
      "step  440 / 480; loss 1.3742\n",
      "Evaluation loss 1.376921434747418, accuracy 0.26349375, acc CATHODE: 0.236, acc CURTAINS: 0.326, acc FETA: 0.396, acc SALAD: 0.095\n",
      "   - - - - -   \n",
      "Epoch 87 / 400\n",
      "step    0 / 480; loss 1.3715\n",
      "step   40 / 480; loss 1.3810\n",
      "step   80 / 480; loss 1.3808\n",
      "step  120 / 480; loss 1.3774\n",
      "step  160 / 480; loss 1.3727\n",
      "step  200 / 480; loss 1.3924\n",
      "step  240 / 480; loss 1.3813\n",
      "step  280 / 480; loss 1.3775\n",
      "step  320 / 480; loss 1.3789\n",
      "step  360 / 480; loss 1.3769\n",
      "step  400 / 480; loss 1.3665\n",
      "step  440 / 480; loss 1.3771\n",
      "Evaluation loss 1.3768407379149317, accuracy 0.262559375, acc CATHODE: 0.212, acc CURTAINS: 0.342, acc FETA: 0.428, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 88 / 400\n",
      "step    0 / 480; loss 1.3739\n",
      "step   40 / 480; loss 1.3727\n",
      "step   80 / 480; loss 1.3861\n",
      "step  120 / 480; loss 1.3780\n",
      "step  160 / 480; loss 1.3717\n",
      "step  200 / 480; loss 1.3736\n",
      "step  240 / 480; loss 1.3887\n",
      "step  280 / 480; loss 1.3714\n",
      "step  320 / 480; loss 1.3946\n",
      "step  360 / 480; loss 1.3810\n",
      "step  400 / 480; loss 1.3800\n",
      "step  440 / 480; loss 1.3784\n",
      "Evaluation loss 1.3768382804356165, accuracy 0.2630875, acc CATHODE: 0.305, acc CURTAINS: 0.305, acc FETA: 0.385, acc SALAD: 0.057\n",
      "   - - - - -   \n",
      "Epoch 89 / 400\n",
      "step    0 / 480; loss 1.3736\n",
      "step   40 / 480; loss 1.3685\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3782\n",
      "step  160 / 480; loss 1.3785\n",
      "step  200 / 480; loss 1.3689\n",
      "step  240 / 480; loss 1.3759\n",
      "step  280 / 480; loss 1.3833\n",
      "step  320 / 480; loss 1.3770\n",
      "step  360 / 480; loss 1.3707\n",
      "step  400 / 480; loss 1.3670\n",
      "step  440 / 480; loss 1.3786\n",
      "Evaluation loss 1.3768171570510706, accuracy 0.262734375, acc CATHODE: 0.242, acc CURTAINS: 0.364, acc FETA: 0.329, acc SALAD: 0.116\n",
      "   - - - - -   \n",
      "Epoch 90 / 400\n",
      "step    0 / 480; loss 1.3761\n",
      "step   40 / 480; loss 1.3754\n",
      "step   80 / 480; loss 1.3763\n",
      "step  120 / 480; loss 1.3724\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3792\n",
      "step  240 / 480; loss 1.3778\n",
      "step  280 / 480; loss 1.3701\n",
      "step  320 / 480; loss 1.3754\n",
      "step  360 / 480; loss 1.3706\n",
      "step  400 / 480; loss 1.3807\n",
      "step  440 / 480; loss 1.3773\n",
      "Evaluation loss 1.376849961513678, accuracy 0.262728125, acc CATHODE: 0.201, acc CURTAINS: 0.256, acc FETA: 0.493, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 91 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3750\n",
      "step   80 / 480; loss 1.3733\n",
      "step  120 / 480; loss 1.3690\n",
      "step  160 / 480; loss 1.3814\n",
      "step  200 / 480; loss 1.3768\n",
      "step  240 / 480; loss 1.3758\n",
      "step  280 / 480; loss 1.3787\n",
      "step  320 / 480; loss 1.3818\n",
      "step  360 / 480; loss 1.3744\n",
      "step  400 / 480; loss 1.3798\n",
      "step  440 / 480; loss 1.3738\n",
      "Evaluation loss 1.3769282782887593, accuracy 0.263675, acc CATHODE: 0.135, acc CURTAINS: 0.452, acc FETA: 0.385, acc SALAD: 0.083\n",
      "   - - - - -   \n",
      "Epoch 92 / 400\n",
      "step    0 / 480; loss 1.3805\n",
      "step   40 / 480; loss 1.3798\n",
      "step   80 / 480; loss 1.3705\n",
      "step  120 / 480; loss 1.3717\n",
      "step  160 / 480; loss 1.3686\n",
      "step  200 / 480; loss 1.3762\n",
      "step  240 / 480; loss 1.3859\n",
      "step  280 / 480; loss 1.3740\n",
      "step  320 / 480; loss 1.3693\n",
      "step  360 / 480; loss 1.3762\n",
      "step  400 / 480; loss 1.3788\n",
      "step  440 / 480; loss 1.3872\n",
      "Evaluation loss 1.3768205702968708, accuracy 0.2638125, acc CATHODE: 0.217, acc CURTAINS: 0.362, acc FETA: 0.365, acc SALAD: 0.112\n",
      "   - - - - -   \n",
      "Epoch 93 / 400\n",
      "step    0 / 480; loss 1.3685\n",
      "step   40 / 480; loss 1.3683\n",
      "step   80 / 480; loss 1.3796\n",
      "step  120 / 480; loss 1.3831\n",
      "step  160 / 480; loss 1.3830\n",
      "step  200 / 480; loss 1.3764\n",
      "step  240 / 480; loss 1.3672\n",
      "step  280 / 480; loss 1.3710\n",
      "step  320 / 480; loss 1.3730\n",
      "step  360 / 480; loss 1.3760\n",
      "step  400 / 480; loss 1.3810\n",
      "step  440 / 480; loss 1.3610\n",
      "Evaluation loss 1.3767410630230474, accuracy 0.262275, acc CATHODE: 0.280, acc CURTAINS: 0.317, acc FETA: 0.383, acc SALAD: 0.069\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 94 / 400\n",
      "step    0 / 480; loss 1.3780\n",
      "step   40 / 480; loss 1.3713\n",
      "step   80 / 480; loss 1.3695\n",
      "step  120 / 480; loss 1.3669\n",
      "step  160 / 480; loss 1.3742\n",
      "step  200 / 480; loss 1.3736\n",
      "step  240 / 480; loss 1.3794\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3811\n",
      "step  360 / 480; loss 1.3720\n",
      "step  400 / 480; loss 1.3773\n",
      "step  440 / 480; loss 1.3651\n",
      "Evaluation loss 1.376866534041737, accuracy 0.262959375, acc CATHODE: 0.176, acc CURTAINS: 0.354, acc FETA: 0.473, acc SALAD: 0.049\n",
      "   - - - - -   \n",
      "Epoch 95 / 400\n",
      "step    0 / 480; loss 1.3704\n",
      "step   40 / 480; loss 1.3790\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3752\n",
      "step  160 / 480; loss 1.3686\n",
      "step  200 / 480; loss 1.3760\n",
      "step  240 / 480; loss 1.3728\n",
      "step  280 / 480; loss 1.3851\n",
      "step  320 / 480; loss 1.3820\n",
      "step  360 / 480; loss 1.3736\n",
      "step  400 / 480; loss 1.3777\n",
      "step  440 / 480; loss 1.3801\n",
      "Evaluation loss 1.3768829838049226, accuracy 0.263784375, acc CATHODE: 0.214, acc CURTAINS: 0.311, acc FETA: 0.420, acc SALAD: 0.111\n",
      "   - - - - -   \n",
      "Epoch 96 / 400\n",
      "step    0 / 480; loss 1.3796\n",
      "step   40 / 480; loss 1.3697\n",
      "step   80 / 480; loss 1.3713\n",
      "step  120 / 480; loss 1.3792\n",
      "step  160 / 480; loss 1.3820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200 / 480; loss 1.3795\n",
      "step  240 / 480; loss 1.3869\n",
      "step  280 / 480; loss 1.3767\n",
      "step  320 / 480; loss 1.3762\n",
      "step  360 / 480; loss 1.3774\n",
      "step  400 / 480; loss 1.3830\n",
      "step  440 / 480; loss 1.3803\n",
      "Evaluation loss 1.376820178050894, accuracy 0.26278125, acc CATHODE: 0.162, acc CURTAINS: 0.369, acc FETA: 0.399, acc SALAD: 0.121\n",
      "   - - - - -   \n",
      "Epoch 97 / 400\n",
      "step    0 / 480; loss 1.3712\n",
      "step   40 / 480; loss 1.3793\n",
      "step   80 / 480; loss 1.3815\n",
      "step  120 / 480; loss 1.3709\n",
      "step  160 / 480; loss 1.3692\n",
      "step  200 / 480; loss 1.3867\n",
      "step  240 / 480; loss 1.3740\n",
      "step  280 / 480; loss 1.3738\n",
      "step  320 / 480; loss 1.3707\n",
      "step  360 / 480; loss 1.3740\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3745\n",
      "Evaluation loss 1.376879197433599, accuracy 0.264209375, acc CATHODE: 0.166, acc CURTAINS: 0.409, acc FETA: 0.333, acc SALAD: 0.148\n",
      "   - - - - -   \n",
      "Epoch 98 / 400\n",
      "step    0 / 480; loss 1.3745\n",
      "step   40 / 480; loss 1.3698\n",
      "step   80 / 480; loss 1.3661\n",
      "step  120 / 480; loss 1.3674\n",
      "step  160 / 480; loss 1.3912\n",
      "step  200 / 480; loss 1.3727\n",
      "step  240 / 480; loss 1.3679\n",
      "step  280 / 480; loss 1.3747\n",
      "step  320 / 480; loss 1.3741\n",
      "step  360 / 480; loss 1.3800\n",
      "step  400 / 480; loss 1.3671\n",
      "step  440 / 480; loss 1.3808\n",
      "Evaluation loss 1.3767782707091247, accuracy 0.26246875, acc CATHODE: 0.188, acc CURTAINS: 0.395, acc FETA: 0.394, acc SALAD: 0.073\n",
      "   - - - - -   \n",
      "Epoch 99 / 400\n",
      "step    0 / 480; loss 1.3886\n",
      "step   40 / 480; loss 1.3661\n",
      "step   80 / 480; loss 1.3725\n",
      "step  120 / 480; loss 1.3889\n",
      "step  160 / 480; loss 1.3811\n",
      "step  200 / 480; loss 1.3736\n",
      "step  240 / 480; loss 1.3777\n",
      "step  280 / 480; loss 1.3763\n",
      "step  320 / 480; loss 1.3800\n",
      "step  360 / 480; loss 1.3682\n",
      "step  400 / 480; loss 1.3713\n",
      "step  440 / 480; loss 1.3768\n",
      "Evaluation loss 1.3768357005156273, accuracy 0.26313125, acc CATHODE: 0.147, acc CURTAINS: 0.413, acc FETA: 0.399, acc SALAD: 0.094\n",
      "   - - - - -   \n",
      "Epoch 100 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3812\n",
      "step   80 / 480; loss 1.3825\n",
      "step  120 / 480; loss 1.3676\n",
      "step  160 / 480; loss 1.3725\n",
      "step  200 / 480; loss 1.3644\n",
      "step  240 / 480; loss 1.3809\n",
      "step  280 / 480; loss 1.3736\n",
      "step  320 / 480; loss 1.3820\n",
      "step  360 / 480; loss 1.3801\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3749\n",
      "Evaluation loss 1.3768873975134277, accuracy 0.262959375, acc CATHODE: 0.258, acc CURTAINS: 0.460, acc FETA: 0.258, acc SALAD: 0.075\n",
      "   - - - - -   \n",
      "Epoch 101 / 400\n",
      "step    0 / 480; loss 1.3758\n",
      "step   40 / 480; loss 1.3751\n",
      "step   80 / 480; loss 1.3745\n",
      "step  120 / 480; loss 1.3828\n",
      "step  160 / 480; loss 1.3826\n",
      "step  200 / 480; loss 1.3730\n",
      "step  240 / 480; loss 1.3736\n",
      "step  280 / 480; loss 1.3752\n",
      "step  320 / 480; loss 1.3772\n",
      "step  360 / 480; loss 1.3788\n",
      "step  400 / 480; loss 1.3749\n",
      "step  440 / 480; loss 1.3750\n",
      "Evaluation loss 1.3768301037913908, accuracy 0.262940625, acc CATHODE: 0.173, acc CURTAINS: 0.422, acc FETA: 0.361, acc SALAD: 0.097\n",
      "   - - - - -   \n",
      "Epoch 102 / 400\n",
      "step    0 / 480; loss 1.3680\n",
      "step   40 / 480; loss 1.3730\n",
      "step   80 / 480; loss 1.3797\n",
      "step  120 / 480; loss 1.3861\n",
      "step  160 / 480; loss 1.3826\n",
      "step  200 / 480; loss 1.3845\n",
      "step  240 / 480; loss 1.3790\n",
      "step  280 / 480; loss 1.3708\n",
      "step  320 / 480; loss 1.3850\n",
      "step  360 / 480; loss 1.3738\n",
      "step  400 / 480; loss 1.3841\n",
      "step  440 / 480; loss 1.3710\n",
      "Evaluation loss 1.376871023887145, accuracy 0.264584375, acc CATHODE: 0.178, acc CURTAINS: 0.322, acc FETA: 0.470, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 103 / 400\n",
      "step    0 / 480; loss 1.3789\n",
      "step   40 / 480; loss 1.3710\n",
      "step   80 / 480; loss 1.3711\n",
      "step  120 / 480; loss 1.3858\n",
      "step  160 / 480; loss 1.3812\n",
      "step  200 / 480; loss 1.3743\n",
      "step  240 / 480; loss 1.3751\n",
      "step  280 / 480; loss 1.3742\n",
      "step  320 / 480; loss 1.3705\n",
      "step  360 / 480; loss 1.3824\n",
      "step  400 / 480; loss 1.3750\n",
      "step  440 / 480; loss 1.3801\n",
      "Evaluation loss 1.376984720724945, accuracy 0.26215625, acc CATHODE: 0.197, acc CURTAINS: 0.345, acc FETA: 0.450, acc SALAD: 0.056\n",
      "   - - - - -   \n",
      "Epoch 104 / 400\n",
      "step    0 / 480; loss 1.3772\n",
      "step   40 / 480; loss 1.3676\n",
      "step   80 / 480; loss 1.3870\n",
      "step  120 / 480; loss 1.3856\n",
      "step  160 / 480; loss 1.3739\n",
      "step  200 / 480; loss 1.3809\n",
      "step  240 / 480; loss 1.3675\n",
      "step  280 / 480; loss 1.3844\n",
      "step  320 / 480; loss 1.3704\n",
      "step  360 / 480; loss 1.3744\n",
      "step  400 / 480; loss 1.3741\n",
      "step  440 / 480; loss 1.3816\n",
      "Evaluation loss 1.3767796042147455, accuracy 0.2629625, acc CATHODE: 0.265, acc CURTAINS: 0.333, acc FETA: 0.343, acc SALAD: 0.111\n",
      "   - - - - -   \n",
      "Epoch 105 / 400\n",
      "step    0 / 480; loss 1.3661\n",
      "step   40 / 480; loss 1.3673\n",
      "step   80 / 480; loss 1.3814\n",
      "step  120 / 480; loss 1.3774\n",
      "step  160 / 480; loss 1.3693\n",
      "step  200 / 480; loss 1.3727\n",
      "step  240 / 480; loss 1.3747\n",
      "step  280 / 480; loss 1.3683\n",
      "step  320 / 480; loss 1.3695\n",
      "step  360 / 480; loss 1.3763\n",
      "step  400 / 480; loss 1.3813\n",
      "step  440 / 480; loss 1.3813\n",
      "Evaluation loss 1.376934024707174, accuracy 0.263171875, acc CATHODE: 0.137, acc CURTAINS: 0.397, acc FETA: 0.458, acc SALAD: 0.060\n",
      "   - - - - -   \n",
      "Epoch 106 / 400\n",
      "step    0 / 480; loss 1.3809\n",
      "step   40 / 480; loss 1.3738\n",
      "step   80 / 480; loss 1.3829\n",
      "step  120 / 480; loss 1.3768\n",
      "step  160 / 480; loss 1.3813\n",
      "step  200 / 480; loss 1.3765\n",
      "step  240 / 480; loss 1.3725\n",
      "step  280 / 480; loss 1.3816\n",
      "step  320 / 480; loss 1.3759\n",
      "step  360 / 480; loss 1.3844\n",
      "step  400 / 480; loss 1.3705\n",
      "step  440 / 480; loss 1.3751\n",
      "Evaluation loss 1.37687134516025, accuracy 0.263065625, acc CATHODE: 0.238, acc CURTAINS: 0.381, acc FETA: 0.377, acc SALAD: 0.056\n",
      "   - - - - -   \n",
      "Epoch 107 / 400\n",
      "step    0 / 480; loss 1.3861\n",
      "step   40 / 480; loss 1.3737\n",
      "step   80 / 480; loss 1.3835\n",
      "step  120 / 480; loss 1.3782\n",
      "step  160 / 480; loss 1.3803\n",
      "step  200 / 480; loss 1.3825\n",
      "step  240 / 480; loss 1.3868\n",
      "step  280 / 480; loss 1.3726\n",
      "step  320 / 480; loss 1.3743\n",
      "step  360 / 480; loss 1.3884\n",
      "step  400 / 480; loss 1.3831\n",
      "step  440 / 480; loss 1.3792\n",
      "Evaluation loss 1.3768287774595531, accuracy 0.264059375, acc CATHODE: 0.154, acc CURTAINS: 0.374, acc FETA: 0.417, acc SALAD: 0.111\n",
      "   - - - - -   \n",
      "Epoch 108 / 400\n",
      "step    0 / 480; loss 1.3644\n",
      "step   40 / 480; loss 1.3708\n",
      "step   80 / 480; loss 1.3674\n",
      "step  120 / 480; loss 1.3803\n",
      "step  160 / 480; loss 1.3733\n",
      "step  200 / 480; loss 1.3671\n",
      "step  240 / 480; loss 1.3663\n",
      "step  280 / 480; loss 1.3749\n",
      "step  320 / 480; loss 1.3799\n",
      "step  360 / 480; loss 1.3795\n",
      "step  400 / 480; loss 1.3766\n",
      "step  440 / 480; loss 1.3636\n",
      "Evaluation loss 1.3767676676742422, accuracy 0.263265625, acc CATHODE: 0.223, acc CURTAINS: 0.333, acc FETA: 0.424, acc SALAD: 0.074\n",
      "   - - - - -   \n",
      "Epoch 109 / 400\n",
      "step    0 / 480; loss 1.3773\n",
      "step   40 / 480; loss 1.3805\n",
      "step   80 / 480; loss 1.3822\n",
      "step  120 / 480; loss 1.3658\n",
      "step  160 / 480; loss 1.3802\n",
      "step  200 / 480; loss 1.3775\n",
      "step  240 / 480; loss 1.3860\n",
      "step  280 / 480; loss 1.3838\n",
      "step  320 / 480; loss 1.3779\n",
      "step  360 / 480; loss 1.3736\n",
      "step  400 / 480; loss 1.3787\n",
      "step  440 / 480; loss 1.3804\n",
      "Evaluation loss 1.376941751010072, accuracy 0.263475, acc CATHODE: 0.177, acc CURTAINS: 0.366, acc FETA: 0.389, acc SALAD: 0.122\n",
      "   - - - - -   \n",
      "Epoch 110 / 400\n",
      "step    0 / 480; loss 1.3892\n",
      "step   40 / 480; loss 1.3743\n",
      "step   80 / 480; loss 1.3759\n",
      "step  120 / 480; loss 1.3764\n",
      "step  160 / 480; loss 1.3716\n",
      "step  200 / 480; loss 1.3742\n",
      "step  240 / 480; loss 1.3798\n",
      "step  280 / 480; loss 1.3730\n",
      "step  320 / 480; loss 1.3806\n",
      "step  360 / 480; loss 1.3826\n",
      "step  400 / 480; loss 1.3728\n",
      "step  440 / 480; loss 1.3689\n",
      "Evaluation loss 1.3766724147159581, accuracy 0.26406875, acc CATHODE: 0.184, acc CURTAINS: 0.449, acc FETA: 0.319, acc SALAD: 0.105\n",
      "Model saved\n",
      "   - - - - -   \n",
      "Epoch 111 / 400\n",
      "step    0 / 480; loss 1.3744\n",
      "step   40 / 480; loss 1.3771\n",
      "step   80 / 480; loss 1.3775\n",
      "step  120 / 480; loss 1.3857\n",
      "step  160 / 480; loss 1.3709\n",
      "step  200 / 480; loss 1.3775\n",
      "step  240 / 480; loss 1.3775\n",
      "step  280 / 480; loss 1.3774\n",
      "step  320 / 480; loss 1.3709\n",
      "step  360 / 480; loss 1.3679\n",
      "step  400 / 480; loss 1.3837\n",
      "step  440 / 480; loss 1.3852\n",
      "Evaluation loss 1.376816427432085, accuracy 0.262234375, acc CATHODE: 0.289, acc CURTAINS: 0.348, acc FETA: 0.356, acc SALAD: 0.056\n",
      "   - - - - -   \n",
      "Epoch 112 / 400\n",
      "step    0 / 480; loss 1.3675\n",
      "step   40 / 480; loss 1.3827\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3849\n",
      "step  160 / 480; loss 1.3844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200 / 480; loss 1.3716\n",
      "step  240 / 480; loss 1.3690\n",
      "step  280 / 480; loss 1.3850\n",
      "step  320 / 480; loss 1.3772\n",
      "step  360 / 480; loss 1.3808\n",
      "step  400 / 480; loss 1.3761\n",
      "step  440 / 480; loss 1.3782\n",
      "Evaluation loss 1.376731260319813, accuracy 0.263759375, acc CATHODE: 0.238, acc CURTAINS: 0.319, acc FETA: 0.415, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 113 / 400\n",
      "step    0 / 480; loss 1.3707\n",
      "step   40 / 480; loss 1.3793\n",
      "step   80 / 480; loss 1.3684\n",
      "step  120 / 480; loss 1.3789\n",
      "step  160 / 480; loss 1.3719\n",
      "step  200 / 480; loss 1.3709\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3775\n",
      "step  320 / 480; loss 1.3692\n",
      "step  360 / 480; loss 1.3810\n",
      "step  400 / 480; loss 1.3754\n",
      "step  440 / 480; loss 1.3614\n",
      "Evaluation loss 1.3768278310661743, accuracy 0.263975, acc CATHODE: 0.183, acc CURTAINS: 0.328, acc FETA: 0.399, acc SALAD: 0.146\n",
      "   - - - - -   \n",
      "Epoch 114 / 400\n",
      "step    0 / 480; loss 1.3780\n",
      "step   40 / 480; loss 1.3788\n",
      "step   80 / 480; loss 1.3819\n",
      "step  120 / 480; loss 1.3785\n",
      "step  160 / 480; loss 1.3737\n",
      "step  200 / 480; loss 1.3793\n",
      "step  240 / 480; loss 1.3744\n",
      "step  280 / 480; loss 1.3806\n",
      "step  320 / 480; loss 1.3738\n",
      "step  360 / 480; loss 1.3738\n",
      "step  400 / 480; loss 1.3781\n",
      "step  440 / 480; loss 1.3880\n",
      "Evaluation loss 1.376822995252199, accuracy 0.263553125, acc CATHODE: 0.191, acc CURTAINS: 0.388, acc FETA: 0.380, acc SALAD: 0.094\n",
      "   - - - - -   \n",
      "Epoch 115 / 400\n",
      "step    0 / 480; loss 1.3808\n",
      "step   40 / 480; loss 1.3842\n",
      "step   80 / 480; loss 1.3818\n",
      "step  120 / 480; loss 1.3756\n",
      "step  160 / 480; loss 1.3657\n",
      "step  200 / 480; loss 1.3734\n",
      "step  240 / 480; loss 1.3767\n",
      "step  280 / 480; loss 1.3734\n",
      "step  320 / 480; loss 1.3732\n",
      "step  360 / 480; loss 1.3788\n",
      "step  400 / 480; loss 1.3805\n",
      "step  440 / 480; loss 1.3822\n",
      "Evaluation loss 1.3769233536605479, accuracy 0.262028125, acc CATHODE: 0.364, acc CURTAINS: 0.287, acc FETA: 0.326, acc SALAD: 0.071\n",
      "   - - - - -   \n",
      "Epoch 116 / 400\n",
      "step    0 / 480; loss 1.3818\n",
      "step   40 / 480; loss 1.3754\n",
      "step   80 / 480; loss 1.3798\n",
      "step  120 / 480; loss 1.3839\n",
      "step  160 / 480; loss 1.3803\n",
      "step  200 / 480; loss 1.3706\n",
      "step  240 / 480; loss 1.3817\n",
      "step  280 / 480; loss 1.3766\n",
      "step  320 / 480; loss 1.3795\n",
      "step  360 / 480; loss 1.3702\n",
      "step  400 / 480; loss 1.3806\n",
      "step  440 / 480; loss 1.3774\n",
      "Evaluation loss 1.3768409878555703, accuracy 0.263815625, acc CATHODE: 0.170, acc CURTAINS: 0.404, acc FETA: 0.421, acc SALAD: 0.061\n",
      "   - - - - -   \n",
      "Epoch 117 / 400\n",
      "step    0 / 480; loss 1.3737\n",
      "step   40 / 480; loss 1.3818\n",
      "step   80 / 480; loss 1.3789\n",
      "step  120 / 480; loss 1.3668\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3769\n",
      "step  240 / 480; loss 1.3838\n",
      "step  280 / 480; loss 1.3754\n",
      "step  320 / 480; loss 1.3803\n",
      "step  360 / 480; loss 1.3729\n",
      "step  400 / 480; loss 1.3792\n",
      "step  440 / 480; loss 1.3816\n",
      "Evaluation loss 1.3768012247529047, accuracy 0.263640625, acc CATHODE: 0.186, acc CURTAINS: 0.392, acc FETA: 0.397, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 118 / 400\n",
      "step    0 / 480; loss 1.3759\n",
      "step   40 / 480; loss 1.3679\n",
      "step   80 / 480; loss 1.3747\n",
      "step  120 / 480; loss 1.3644\n",
      "step  160 / 480; loss 1.3830\n",
      "step  200 / 480; loss 1.3845\n",
      "step  240 / 480; loss 1.3790\n",
      "step  280 / 480; loss 1.3750\n",
      "step  320 / 480; loss 1.3755\n",
      "step  360 / 480; loss 1.3729\n",
      "step  400 / 480; loss 1.3694\n",
      "step  440 / 480; loss 1.3675\n",
      "Evaluation loss 1.3769976383514093, accuracy 0.263015625, acc CATHODE: 0.193, acc CURTAINS: 0.443, acc FETA: 0.293, acc SALAD: 0.124\n",
      "   - - - - -   \n",
      "Epoch 119 / 400\n",
      "step    0 / 480; loss 1.3758\n",
      "step   40 / 480; loss 1.3721\n",
      "step   80 / 480; loss 1.3856\n",
      "step  120 / 480; loss 1.3668\n",
      "step  160 / 480; loss 1.3762\n",
      "step  200 / 480; loss 1.3758\n",
      "step  240 / 480; loss 1.3654\n",
      "step  280 / 480; loss 1.3770\n",
      "step  320 / 480; loss 1.3707\n",
      "step  360 / 480; loss 1.3715\n",
      "step  400 / 480; loss 1.3847\n",
      "step  440 / 480; loss 1.3704\n",
      "Evaluation loss 1.3769107959584905, accuracy 0.261390625, acc CATHODE: 0.192, acc CURTAINS: 0.348, acc FETA: 0.387, acc SALAD: 0.119\n",
      "   - - - - -   \n",
      "Epoch 120 / 400\n",
      "step    0 / 480; loss 1.3826\n",
      "step   40 / 480; loss 1.3709\n",
      "step   80 / 480; loss 1.3860\n",
      "step  120 / 480; loss 1.3779\n",
      "step  160 / 480; loss 1.3815\n",
      "step  200 / 480; loss 1.3841\n",
      "step  240 / 480; loss 1.3721\n",
      "step  280 / 480; loss 1.3743\n",
      "step  320 / 480; loss 1.3894\n",
      "step  360 / 480; loss 1.3711\n",
      "step  400 / 480; loss 1.3725\n",
      "step  440 / 480; loss 1.3826\n",
      "Evaluation loss 1.3768900527539778, accuracy 0.26215, acc CATHODE: 0.228, acc CURTAINS: 0.402, acc FETA: 0.292, acc SALAD: 0.127\n",
      "   - - - - -   \n",
      "Epoch 121 / 400\n",
      "step    0 / 480; loss 1.3777\n",
      "step   40 / 480; loss 1.3723\n",
      "step   80 / 480; loss 1.3769\n",
      "step  120 / 480; loss 1.3784\n",
      "step  160 / 480; loss 1.3660\n",
      "step  200 / 480; loss 1.3768\n",
      "step  240 / 480; loss 1.3749\n",
      "step  280 / 480; loss 1.3739\n",
      "step  320 / 480; loss 1.3869\n",
      "step  360 / 480; loss 1.3775\n",
      "step  400 / 480; loss 1.3656\n",
      "step  440 / 480; loss 1.3757\n",
      "Evaluation loss 1.3768397030063757, accuracy 0.262565625, acc CATHODE: 0.298, acc CURTAINS: 0.329, acc FETA: 0.358, acc SALAD: 0.065\n",
      "   - - - - -   \n",
      "Epoch 122 / 400\n",
      "step    0 / 480; loss 1.3728\n",
      "step   40 / 480; loss 1.3757\n",
      "step   80 / 480; loss 1.3713\n",
      "step  120 / 480; loss 1.3761\n",
      "step  160 / 480; loss 1.3773\n",
      "step  200 / 480; loss 1.3753\n",
      "step  240 / 480; loss 1.3708\n",
      "step  280 / 480; loss 1.3758\n",
      "step  320 / 480; loss 1.3736\n",
      "step  360 / 480; loss 1.3694\n",
      "step  400 / 480; loss 1.3787\n",
      "step  440 / 480; loss 1.3704\n",
      "Evaluation loss 1.3767962017272213, accuracy 0.264653125, acc CATHODE: 0.186, acc CURTAINS: 0.273, acc FETA: 0.515, acc SALAD: 0.084\n",
      "   - - - - -   \n",
      "Epoch 123 / 400\n",
      "step    0 / 480; loss 1.3822\n",
      "step   40 / 480; loss 1.3849\n",
      "step   80 / 480; loss 1.3719\n",
      "step  120 / 480; loss 1.3711\n",
      "step  160 / 480; loss 1.3736\n",
      "step  200 / 480; loss 1.3734\n",
      "step  240 / 480; loss 1.3900\n",
      "step  280 / 480; loss 1.3783\n",
      "step  320 / 480; loss 1.3773\n",
      "step  360 / 480; loss 1.3785\n",
      "step  400 / 480; loss 1.3717\n",
      "step  440 / 480; loss 1.3714\n",
      "Evaluation loss 1.3769357784924698, accuracy 0.262371875, acc CATHODE: 0.309, acc CURTAINS: 0.332, acc FETA: 0.331, acc SALAD: 0.077\n",
      "   - - - - -   \n",
      "Epoch 124 / 400\n",
      "step    0 / 480; loss 1.3709\n",
      "step   40 / 480; loss 1.3788\n",
      "step   80 / 480; loss 1.3737\n",
      "step  120 / 480; loss 1.3737\n",
      "step  160 / 480; loss 1.3823\n",
      "step  200 / 480; loss 1.3784\n",
      "step  240 / 480; loss 1.3713\n",
      "step  280 / 480; loss 1.3745\n",
      "step  320 / 480; loss 1.3788\n",
      "step  360 / 480; loss 1.3776\n",
      "step  400 / 480; loss 1.3758\n",
      "step  440 / 480; loss 1.3774\n",
      "Evaluation loss 1.3767728339063015, accuracy 0.26535, acc CATHODE: 0.176, acc CURTAINS: 0.324, acc FETA: 0.437, acc SALAD: 0.124\n",
      "   - - - - -   \n",
      "Epoch 125 / 400\n",
      "step    0 / 480; loss 1.3765\n",
      "step   40 / 480; loss 1.3732\n",
      "step   80 / 480; loss 1.3792\n",
      "step  120 / 480; loss 1.3709\n",
      "step  160 / 480; loss 1.3664\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3658\n",
      "step  280 / 480; loss 1.3729\n",
      "step  320 / 480; loss 1.3758\n",
      "step  360 / 480; loss 1.3677\n",
      "step  400 / 480; loss 1.3702\n",
      "step  440 / 480; loss 1.3863\n",
      "Evaluation loss 1.3768013980929137, accuracy 0.263375, acc CATHODE: 0.240, acc CURTAINS: 0.350, acc FETA: 0.383, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 126 / 400\n",
      "step    0 / 480; loss 1.3785\n",
      "step   40 / 480; loss 1.3726\n",
      "step   80 / 480; loss 1.3759\n",
      "step  120 / 480; loss 1.3782\n",
      "step  160 / 480; loss 1.3791\n",
      "step  200 / 480; loss 1.3757\n",
      "step  240 / 480; loss 1.3721\n",
      "step  280 / 480; loss 1.3756\n",
      "step  320 / 480; loss 1.3838\n",
      "step  360 / 480; loss 1.3746\n",
      "step  400 / 480; loss 1.3735\n",
      "step  440 / 480; loss 1.3712\n",
      "Evaluation loss 1.3770900684756053, accuracy 0.263134375, acc CATHODE: 0.221, acc CURTAINS: 0.348, acc FETA: 0.316, acc SALAD: 0.168\n",
      "   - - - - -   \n",
      "Epoch 127 / 400\n",
      "step    0 / 480; loss 1.3753\n",
      "step   40 / 480; loss 1.3811\n",
      "step   80 / 480; loss 1.3800\n",
      "step  120 / 480; loss 1.3709\n",
      "step  160 / 480; loss 1.3734\n",
      "step  200 / 480; loss 1.3666\n",
      "step  240 / 480; loss 1.3763\n",
      "step  280 / 480; loss 1.3807\n",
      "step  320 / 480; loss 1.3822\n",
      "step  360 / 480; loss 1.3723\n",
      "step  400 / 480; loss 1.3675\n",
      "step  440 / 480; loss 1.3690\n",
      "Evaluation loss 1.376826047068989, accuracy 0.26288125, acc CATHODE: 0.214, acc CURTAINS: 0.369, acc FETA: 0.385, acc SALAD: 0.083\n",
      "   - - - - -   \n",
      "Epoch 128 / 400\n",
      "step    0 / 480; loss 1.3824\n",
      "step   40 / 480; loss 1.3906\n",
      "step   80 / 480; loss 1.3690\n",
      "step  120 / 480; loss 1.3762\n",
      "step  160 / 480; loss 1.3742\n",
      "step  200 / 480; loss 1.3659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  240 / 480; loss 1.3781\n",
      "step  280 / 480; loss 1.3766\n",
      "step  320 / 480; loss 1.3766\n",
      "step  360 / 480; loss 1.3703\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3762\n",
      "Evaluation loss 1.3768971562087922, accuracy 0.263578125, acc CATHODE: 0.150, acc CURTAINS: 0.367, acc FETA: 0.474, acc SALAD: 0.064\n",
      "   - - - - -   \n",
      "Epoch 129 / 400\n",
      "step    0 / 480; loss 1.3736\n",
      "step   40 / 480; loss 1.3785\n",
      "step   80 / 480; loss 1.3764\n",
      "step  120 / 480; loss 1.3762\n",
      "step  160 / 480; loss 1.3683\n",
      "step  200 / 480; loss 1.3705\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3756\n",
      "step  320 / 480; loss 1.3738\n",
      "step  360 / 480; loss 1.3788\n",
      "step  400 / 480; loss 1.3778\n",
      "step  440 / 480; loss 1.3743\n",
      "Evaluation loss 1.376929657120884, accuracy 0.26295625, acc CATHODE: 0.274, acc CURTAINS: 0.264, acc FETA: 0.431, acc SALAD: 0.084\n",
      "   - - - - -   \n",
      "Epoch 130 / 400\n",
      "step    0 / 480; loss 1.3783\n",
      "step   40 / 480; loss 1.3757\n",
      "step   80 / 480; loss 1.3736\n",
      "step  120 / 480; loss 1.3763\n",
      "step  160 / 480; loss 1.3868\n",
      "step  200 / 480; loss 1.3782\n",
      "step  240 / 480; loss 1.3823\n",
      "step  280 / 480; loss 1.3773\n",
      "step  320 / 480; loss 1.3794\n",
      "step  360 / 480; loss 1.3735\n",
      "step  400 / 480; loss 1.3794\n",
      "step  440 / 480; loss 1.3698\n",
      "Evaluation loss 1.3768595332194589, accuracy 0.26264375, acc CATHODE: 0.216, acc CURTAINS: 0.327, acc FETA: 0.406, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 131 / 400\n",
      "step    0 / 480; loss 1.3668\n",
      "step   40 / 480; loss 1.3705\n",
      "step   80 / 480; loss 1.3662\n",
      "step  120 / 480; loss 1.3825\n",
      "step  160 / 480; loss 1.3752\n",
      "step  200 / 480; loss 1.3665\n",
      "step  240 / 480; loss 1.3740\n",
      "step  280 / 480; loss 1.3857\n",
      "step  320 / 480; loss 1.3703\n",
      "step  360 / 480; loss 1.3840\n",
      "step  400 / 480; loss 1.3667\n",
      "step  440 / 480; loss 1.3853\n",
      "Evaluation loss 1.376971837528566, accuracy 0.265509375, acc CATHODE: 0.157, acc CURTAINS: 0.334, acc FETA: 0.426, acc SALAD: 0.145\n",
      "   - - - - -   \n",
      "Epoch 132 / 400\n",
      "step    0 / 480; loss 1.3679\n",
      "step   40 / 480; loss 1.3703\n",
      "step   80 / 480; loss 1.3747\n",
      "step  120 / 480; loss 1.3733\n",
      "step  160 / 480; loss 1.3799\n",
      "step  200 / 480; loss 1.3748\n",
      "step  240 / 480; loss 1.3760\n",
      "step  280 / 480; loss 1.3746\n",
      "step  320 / 480; loss 1.3790\n",
      "step  360 / 480; loss 1.3686\n",
      "step  400 / 480; loss 1.3785\n",
      "step  440 / 480; loss 1.3788\n",
      "Evaluation loss 1.3768484283858047, accuracy 0.263303125, acc CATHODE: 0.176, acc CURTAINS: 0.374, acc FETA: 0.428, acc SALAD: 0.075\n",
      "   - - - - -   \n",
      "Epoch 133 / 400\n",
      "step    0 / 480; loss 1.3664\n",
      "step   40 / 480; loss 1.3790\n",
      "step   80 / 480; loss 1.3869\n",
      "step  120 / 480; loss 1.3779\n",
      "step  160 / 480; loss 1.3701\n",
      "step  200 / 480; loss 1.3776\n",
      "step  240 / 480; loss 1.3753\n",
      "step  280 / 480; loss 1.3705\n",
      "step  320 / 480; loss 1.3709\n",
      "step  360 / 480; loss 1.3770\n",
      "step  400 / 480; loss 1.3765\n",
      "step  440 / 480; loss 1.3733\n",
      "Evaluation loss 1.3769434802073095, accuracy 0.265559375, acc CATHODE: 0.225, acc CURTAINS: 0.372, acc FETA: 0.296, acc SALAD: 0.170\n",
      "   - - - - -   \n",
      "Epoch 134 / 400\n",
      "step    0 / 480; loss 1.3843\n",
      "step   40 / 480; loss 1.3697\n",
      "step   80 / 480; loss 1.3694\n",
      "step  120 / 480; loss 1.3733\n",
      "step  160 / 480; loss 1.3721\n",
      "step  200 / 480; loss 1.3715\n",
      "step  240 / 480; loss 1.3683\n",
      "step  280 / 480; loss 1.3765\n",
      "step  320 / 480; loss 1.3719\n",
      "step  360 / 480; loss 1.3723\n",
      "step  400 / 480; loss 1.3689\n",
      "step  440 / 480; loss 1.3757\n",
      "Evaluation loss 1.3768939736917805, accuracy 0.264521875, acc CATHODE: 0.195, acc CURTAINS: 0.419, acc FETA: 0.340, acc SALAD: 0.104\n",
      "   - - - - -   \n",
      "Epoch 135 / 400\n",
      "step    0 / 480; loss 1.3728\n",
      "step   40 / 480; loss 1.3775\n",
      "step   80 / 480; loss 1.3798\n",
      "step  120 / 480; loss 1.3652\n",
      "step  160 / 480; loss 1.3722\n",
      "step  200 / 480; loss 1.3720\n",
      "step  240 / 480; loss 1.3801\n",
      "step  280 / 480; loss 1.3846\n",
      "step  320 / 480; loss 1.3718\n",
      "step  360 / 480; loss 1.3861\n",
      "step  400 / 480; loss 1.3730\n",
      "step  440 / 480; loss 1.3882\n",
      "Evaluation loss 1.3767496268544612, accuracy 0.2641625, acc CATHODE: 0.181, acc CURTAINS: 0.395, acc FETA: 0.383, acc SALAD: 0.097\n",
      "   - - - - -   \n",
      "Epoch 136 / 400\n",
      "step    0 / 480; loss 1.3727\n",
      "step   40 / 480; loss 1.3735\n",
      "step   80 / 480; loss 1.3928\n",
      "step  120 / 480; loss 1.3822\n",
      "step  160 / 480; loss 1.3750\n",
      "step  200 / 480; loss 1.3801\n",
      "step  240 / 480; loss 1.3699\n",
      "step  280 / 480; loss 1.3786\n",
      "step  320 / 480; loss 1.3737\n",
      "step  360 / 480; loss 1.3807\n",
      "step  400 / 480; loss 1.3734\n",
      "step  440 / 480; loss 1.3760\n",
      "Evaluation loss 1.3769151030068496, accuracy 0.2629625, acc CATHODE: 0.223, acc CURTAINS: 0.286, acc FETA: 0.487, acc SALAD: 0.056\n",
      "   - - - - -   \n",
      "Epoch 137 / 400\n",
      "step    0 / 480; loss 1.3758\n",
      "step   40 / 480; loss 1.3737\n",
      "step   80 / 480; loss 1.3676\n",
      "step  120 / 480; loss 1.3791\n",
      "step  160 / 480; loss 1.3681\n",
      "step  200 / 480; loss 1.3713\n",
      "step  240 / 480; loss 1.3739\n",
      "step  280 / 480; loss 1.3790\n",
      "step  320 / 480; loss 1.3731\n",
      "step  360 / 480; loss 1.3721\n",
      "step  400 / 480; loss 1.3735\n",
      "step  440 / 480; loss 1.3796\n",
      "Evaluation loss 1.3769261761759855, accuracy 0.263628125, acc CATHODE: 0.209, acc CURTAINS: 0.340, acc FETA: 0.375, acc SALAD: 0.130\n",
      "   - - - - -   \n",
      "Epoch 138 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3789\n",
      "step   80 / 480; loss 1.3845\n",
      "step  120 / 480; loss 1.3763\n",
      "step  160 / 480; loss 1.3805\n",
      "step  200 / 480; loss 1.3804\n",
      "step  240 / 480; loss 1.3772\n",
      "step  280 / 480; loss 1.3739\n",
      "step  320 / 480; loss 1.3767\n",
      "step  360 / 480; loss 1.3831\n",
      "step  400 / 480; loss 1.3776\n",
      "step  440 / 480; loss 1.3751\n",
      "Evaluation loss 1.3768806770042625, accuracy 0.26425625, acc CATHODE: 0.229, acc CURTAINS: 0.317, acc FETA: 0.346, acc SALAD: 0.165\n",
      "   - - - - -   \n",
      "Epoch 139 / 400\n",
      "step    0 / 480; loss 1.3801\n",
      "step   40 / 480; loss 1.3728\n",
      "step   80 / 480; loss 1.3832\n",
      "step  120 / 480; loss 1.3758\n",
      "step  160 / 480; loss 1.3813\n",
      "step  200 / 480; loss 1.3786\n",
      "step  240 / 480; loss 1.3797\n",
      "step  280 / 480; loss 1.3714\n",
      "step  320 / 480; loss 1.3731\n",
      "step  360 / 480; loss 1.3641\n",
      "step  400 / 480; loss 1.3718\n",
      "step  440 / 480; loss 1.3826\n",
      "Evaluation loss 1.3768608436091376, accuracy 0.262853125, acc CATHODE: 0.178, acc CURTAINS: 0.382, acc FETA: 0.451, acc SALAD: 0.040\n",
      "   - - - - -   \n",
      "Epoch 140 / 400\n",
      "step    0 / 480; loss 1.3755\n",
      "step   40 / 480; loss 1.3728\n",
      "step   80 / 480; loss 1.3705\n",
      "step  120 / 480; loss 1.3801\n",
      "step  160 / 480; loss 1.3742\n",
      "step  200 / 480; loss 1.3792\n",
      "step  240 / 480; loss 1.3792\n",
      "step  280 / 480; loss 1.3704\n",
      "step  320 / 480; loss 1.3816\n",
      "step  360 / 480; loss 1.3782\n",
      "step  400 / 480; loss 1.3727\n",
      "step  440 / 480; loss 1.3695\n",
      "Evaluation loss 1.3767913144016317, accuracy 0.263253125, acc CATHODE: 0.192, acc CURTAINS: 0.362, acc FETA: 0.416, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 141 / 400\n",
      "step    0 / 480; loss 1.3764\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3699\n",
      "step  120 / 480; loss 1.3709\n",
      "step  160 / 480; loss 1.3709\n",
      "step  200 / 480; loss 1.3774\n",
      "step  240 / 480; loss 1.3757\n",
      "step  280 / 480; loss 1.3872\n",
      "step  320 / 480; loss 1.3722\n",
      "step  360 / 480; loss 1.3825\n",
      "step  400 / 480; loss 1.3754\n",
      "step  440 / 480; loss 1.3825\n",
      "Evaluation loss 1.376900062139214, accuracy 0.263628125, acc CATHODE: 0.196, acc CURTAINS: 0.391, acc FETA: 0.400, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 142 / 400\n",
      "step    0 / 480; loss 1.3789\n",
      "step   40 / 480; loss 1.3738\n",
      "step   80 / 480; loss 1.3833\n",
      "step  120 / 480; loss 1.3766\n",
      "step  160 / 480; loss 1.3692\n",
      "step  200 / 480; loss 1.3802\n",
      "step  240 / 480; loss 1.3718\n",
      "step  280 / 480; loss 1.3712\n",
      "step  320 / 480; loss 1.3880\n",
      "step  360 / 480; loss 1.3736\n",
      "step  400 / 480; loss 1.3713\n",
      "step  440 / 480; loss 1.3899\n",
      "Evaluation loss 1.3769104677063573, accuracy 0.26371875, acc CATHODE: 0.231, acc CURTAINS: 0.396, acc FETA: 0.332, acc SALAD: 0.096\n",
      "   - - - - -   \n",
      "Epoch 143 / 400\n",
      "step    0 / 480; loss 1.3738\n",
      "step   40 / 480; loss 1.3677\n",
      "step   80 / 480; loss 1.3827\n",
      "step  120 / 480; loss 1.3792\n",
      "step  160 / 480; loss 1.3820\n",
      "step  200 / 480; loss 1.3755\n",
      "step  240 / 480; loss 1.3820\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3827\n",
      "step  360 / 480; loss 1.3595\n",
      "step  400 / 480; loss 1.3694\n",
      "step  440 / 480; loss 1.3716\n",
      "Evaluation loss 1.3767656910063524, accuracy 0.263315625, acc CATHODE: 0.258, acc CURTAINS: 0.346, acc FETA: 0.393, acc SALAD: 0.056\n",
      "   - - - - -   \n",
      "Epoch 144 / 400\n",
      "step    0 / 480; loss 1.3719\n",
      "step   40 / 480; loss 1.3729\n",
      "step   80 / 480; loss 1.3751\n",
      "step  120 / 480; loss 1.3703\n",
      "step  160 / 480; loss 1.3736\n",
      "step  200 / 480; loss 1.3812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  240 / 480; loss 1.3777\n",
      "step  280 / 480; loss 1.3744\n",
      "step  320 / 480; loss 1.3765\n",
      "step  360 / 480; loss 1.3733\n",
      "step  400 / 480; loss 1.3778\n",
      "step  440 / 480; loss 1.3793\n",
      "Evaluation loss 1.3768994206785656, accuracy 0.264109375, acc CATHODE: 0.206, acc CURTAINS: 0.346, acc FETA: 0.376, acc SALAD: 0.128\n",
      "   - - - - -   \n",
      "Epoch 145 / 400\n",
      "step    0 / 480; loss 1.3745\n",
      "step   40 / 480; loss 1.3731\n",
      "step   80 / 480; loss 1.3632\n",
      "step  120 / 480; loss 1.3727\n",
      "step  160 / 480; loss 1.3764\n",
      "step  200 / 480; loss 1.3728\n",
      "step  240 / 480; loss 1.3726\n",
      "step  280 / 480; loss 1.3696\n",
      "step  320 / 480; loss 1.3674\n",
      "step  360 / 480; loss 1.3858\n",
      "step  400 / 480; loss 1.3792\n",
      "step  440 / 480; loss 1.3799\n",
      "Evaluation loss 1.3770910720255731, accuracy 0.262628125, acc CATHODE: 0.246, acc CURTAINS: 0.341, acc FETA: 0.342, acc SALAD: 0.122\n",
      "   - - - - -   \n",
      "Epoch 146 / 400\n",
      "step    0 / 480; loss 1.3698\n",
      "step   40 / 480; loss 1.3642\n",
      "step   80 / 480; loss 1.3629\n",
      "step  120 / 480; loss 1.3777\n",
      "step  160 / 480; loss 1.3693\n",
      "step  200 / 480; loss 1.3633\n",
      "step  240 / 480; loss 1.3846\n",
      "step  280 / 480; loss 1.3803\n",
      "step  320 / 480; loss 1.3857\n",
      "step  360 / 480; loss 1.3716\n",
      "step  400 / 480; loss 1.3724\n",
      "step  440 / 480; loss 1.3730\n",
      "Evaluation loss 1.376933489363906, accuracy 0.26324375, acc CATHODE: 0.288, acc CURTAINS: 0.316, acc FETA: 0.372, acc SALAD: 0.076\n",
      "   - - - - -   \n",
      "Epoch 147 / 400\n",
      "step    0 / 480; loss 1.3696\n",
      "step   40 / 480; loss 1.3776\n",
      "step   80 / 480; loss 1.3793\n",
      "step  120 / 480; loss 1.3807\n",
      "step  160 / 480; loss 1.3846\n",
      "step  200 / 480; loss 1.3718\n",
      "step  240 / 480; loss 1.3747\n",
      "step  280 / 480; loss 1.3764\n",
      "step  320 / 480; loss 1.3685\n",
      "step  360 / 480; loss 1.3795\n",
      "step  400 / 480; loss 1.3823\n",
      "step  440 / 480; loss 1.3848\n",
      "Evaluation loss 1.3768274951896382, accuracy 0.26265625, acc CATHODE: 0.279, acc CURTAINS: 0.271, acc FETA: 0.398, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 148 / 400\n",
      "step    0 / 480; loss 1.3826\n",
      "step   40 / 480; loss 1.3802\n",
      "step   80 / 480; loss 1.3832\n",
      "step  120 / 480; loss 1.3825\n",
      "step  160 / 480; loss 1.3807\n",
      "step  200 / 480; loss 1.3847\n",
      "step  240 / 480; loss 1.3813\n",
      "step  280 / 480; loss 1.3754\n",
      "step  320 / 480; loss 1.3782\n",
      "step  360 / 480; loss 1.3900\n",
      "step  400 / 480; loss 1.3765\n",
      "step  440 / 480; loss 1.3677\n",
      "Evaluation loss 1.3769316152151605, accuracy 0.26371875, acc CATHODE: 0.199, acc CURTAINS: 0.379, acc FETA: 0.389, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 149 / 400\n",
      "step    0 / 480; loss 1.3724\n",
      "step   40 / 480; loss 1.3751\n",
      "step   80 / 480; loss 1.3688\n",
      "step  120 / 480; loss 1.3810\n",
      "step  160 / 480; loss 1.3772\n",
      "step  200 / 480; loss 1.3851\n",
      "step  240 / 480; loss 1.3709\n",
      "step  280 / 480; loss 1.3839\n",
      "step  320 / 480; loss 1.3693\n",
      "step  360 / 480; loss 1.3672\n",
      "step  400 / 480; loss 1.3783\n",
      "step  440 / 480; loss 1.3747\n",
      "Evaluation loss 1.3768657567005422, accuracy 0.263146875, acc CATHODE: 0.235, acc CURTAINS: 0.314, acc FETA: 0.413, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 150 / 400\n",
      "step    0 / 480; loss 1.3750\n",
      "step   40 / 480; loss 1.3774\n",
      "step   80 / 480; loss 1.3709\n",
      "step  120 / 480; loss 1.3760\n",
      "step  160 / 480; loss 1.3693\n",
      "step  200 / 480; loss 1.3752\n",
      "step  240 / 480; loss 1.3809\n",
      "step  280 / 480; loss 1.3845\n",
      "step  320 / 480; loss 1.3840\n",
      "step  360 / 480; loss 1.3737\n",
      "step  400 / 480; loss 1.3785\n",
      "step  440 / 480; loss 1.3687\n",
      "Evaluation loss 1.3768561666482606, accuracy 0.262759375, acc CATHODE: 0.182, acc CURTAINS: 0.340, acc FETA: 0.471, acc SALAD: 0.058\n",
      "   - - - - -   \n",
      "Epoch 151 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3719\n",
      "step   80 / 480; loss 1.3824\n",
      "step  120 / 480; loss 1.3717\n",
      "step  160 / 480; loss 1.3826\n",
      "step  200 / 480; loss 1.3714\n",
      "step  240 / 480; loss 1.3777\n",
      "step  280 / 480; loss 1.3673\n",
      "step  320 / 480; loss 1.3714\n",
      "step  360 / 480; loss 1.3714\n",
      "step  400 / 480; loss 1.3683\n",
      "step  440 / 480; loss 1.3753\n",
      "Evaluation loss 1.3769336029388346, accuracy 0.2632375, acc CATHODE: 0.217, acc CURTAINS: 0.392, acc FETA: 0.375, acc SALAD: 0.069\n",
      "   - - - - -   \n",
      "Epoch 152 / 400\n",
      "step    0 / 480; loss 1.3703\n",
      "step   40 / 480; loss 1.3733\n",
      "step   80 / 480; loss 1.3831\n",
      "step  120 / 480; loss 1.3769\n",
      "step  160 / 480; loss 1.3720\n",
      "step  200 / 480; loss 1.3721\n",
      "step  240 / 480; loss 1.3728\n",
      "step  280 / 480; loss 1.3734\n",
      "step  320 / 480; loss 1.3759\n",
      "step  360 / 480; loss 1.3690\n",
      "step  400 / 480; loss 1.3807\n",
      "step  440 / 480; loss 1.3849\n",
      "Evaluation loss 1.3768534762334956, accuracy 0.2640875, acc CATHODE: 0.276, acc CURTAINS: 0.364, acc FETA: 0.314, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 153 / 400\n",
      "step    0 / 480; loss 1.3673\n",
      "step   40 / 480; loss 1.3742\n",
      "step   80 / 480; loss 1.3705\n",
      "step  120 / 480; loss 1.3732\n",
      "step  160 / 480; loss 1.3734\n",
      "step  200 / 480; loss 1.3693\n",
      "step  240 / 480; loss 1.3706\n",
      "step  280 / 480; loss 1.3792\n",
      "step  320 / 480; loss 1.3754\n",
      "step  360 / 480; loss 1.3725\n",
      "step  400 / 480; loss 1.3635\n",
      "step  440 / 480; loss 1.3806\n",
      "Evaluation loss 1.376983080948567, accuracy 0.2631875, acc CATHODE: 0.214, acc CURTAINS: 0.317, acc FETA: 0.441, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 154 / 400\n",
      "step    0 / 480; loss 1.3700\n",
      "step   40 / 480; loss 1.3713\n",
      "step   80 / 480; loss 1.3765\n",
      "step  120 / 480; loss 1.3706\n",
      "step  160 / 480; loss 1.3692\n",
      "step  200 / 480; loss 1.3746\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3701\n",
      "step  320 / 480; loss 1.3735\n",
      "step  360 / 480; loss 1.3710\n",
      "step  400 / 480; loss 1.3800\n",
      "step  440 / 480; loss 1.3823\n",
      "Evaluation loss 1.3769904978897833, accuracy 0.261325, acc CATHODE: 0.171, acc CURTAINS: 0.332, acc FETA: 0.434, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 155 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3771\n",
      "step   80 / 480; loss 1.3677\n",
      "step  120 / 480; loss 1.3680\n",
      "step  160 / 480; loss 1.3861\n",
      "step  200 / 480; loss 1.3755\n",
      "step  240 / 480; loss 1.3819\n",
      "step  280 / 480; loss 1.3769\n",
      "step  320 / 480; loss 1.3807\n",
      "step  360 / 480; loss 1.3755\n",
      "step  400 / 480; loss 1.3583\n",
      "step  440 / 480; loss 1.3793\n",
      "Evaluation loss 1.3769528458386795, accuracy 0.2630875, acc CATHODE: 0.266, acc CURTAINS: 0.262, acc FETA: 0.372, acc SALAD: 0.151\n",
      "   - - - - -   \n",
      "Epoch 156 / 400\n",
      "step    0 / 480; loss 1.3759\n",
      "step   40 / 480; loss 1.3741\n",
      "step   80 / 480; loss 1.3717\n",
      "step  120 / 480; loss 1.3696\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3861\n",
      "step  240 / 480; loss 1.3720\n",
      "step  280 / 480; loss 1.3760\n",
      "step  320 / 480; loss 1.3827\n",
      "step  360 / 480; loss 1.3742\n",
      "step  400 / 480; loss 1.3676\n",
      "step  440 / 480; loss 1.3631\n",
      "Evaluation loss 1.3769075519345138, accuracy 0.264596875, acc CATHODE: 0.183, acc CURTAINS: 0.408, acc FETA: 0.350, acc SALAD: 0.117\n",
      "   - - - - -   \n",
      "Epoch 157 / 400\n",
      "step    0 / 480; loss 1.3728\n",
      "step   40 / 480; loss 1.3661\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3641\n",
      "step  160 / 480; loss 1.3754\n",
      "step  200 / 480; loss 1.3738\n",
      "step  240 / 480; loss 1.3708\n",
      "step  280 / 480; loss 1.3758\n",
      "step  320 / 480; loss 1.3727\n",
      "step  360 / 480; loss 1.3704\n",
      "step  400 / 480; loss 1.3752\n",
      "step  440 / 480; loss 1.3757\n",
      "Evaluation loss 1.3769735349838939, accuracy 0.263546875, acc CATHODE: 0.207, acc CURTAINS: 0.324, acc FETA: 0.427, acc SALAD: 0.096\n",
      "   - - - - -   \n",
      "Epoch 158 / 400\n",
      "step    0 / 480; loss 1.3716\n",
      "step   40 / 480; loss 1.3804\n",
      "step   80 / 480; loss 1.3842\n",
      "step  120 / 480; loss 1.3769\n",
      "step  160 / 480; loss 1.3779\n",
      "step  200 / 480; loss 1.3789\n",
      "step  240 / 480; loss 1.3742\n",
      "step  280 / 480; loss 1.3702\n",
      "step  320 / 480; loss 1.3693\n",
      "step  360 / 480; loss 1.3712\n",
      "step  400 / 480; loss 1.3887\n",
      "step  440 / 480; loss 1.3708\n",
      "Evaluation loss 1.3769442388999011, accuracy 0.262846875, acc CATHODE: 0.179, acc CURTAINS: 0.386, acc FETA: 0.375, acc SALAD: 0.111\n",
      "   - - - - -   \n",
      "Epoch 159 / 400\n",
      "step    0 / 480; loss 1.3789\n",
      "step   40 / 480; loss 1.3814\n",
      "step   80 / 480; loss 1.3679\n",
      "step  120 / 480; loss 1.3645\n",
      "step  160 / 480; loss 1.3705\n",
      "step  200 / 480; loss 1.3812\n",
      "step  240 / 480; loss 1.3769\n",
      "step  280 / 480; loss 1.3844\n",
      "step  320 / 480; loss 1.3739\n",
      "step  360 / 480; loss 1.3779\n",
      "step  400 / 480; loss 1.3707\n",
      "step  440 / 480; loss 1.3851\n",
      "Evaluation loss 1.3770494512856968, accuracy 0.2633375, acc CATHODE: 0.155, acc CURTAINS: 0.459, acc FETA: 0.338, acc SALAD: 0.102\n",
      "   - - - - -   \n",
      "Epoch 160 / 400\n",
      "step    0 / 480; loss 1.3804\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3740\n",
      "step  120 / 480; loss 1.3761\n",
      "step  160 / 480; loss 1.3830\n",
      "step  200 / 480; loss 1.3792\n",
      "step  240 / 480; loss 1.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  280 / 480; loss 1.3743\n",
      "step  320 / 480; loss 1.3676\n",
      "step  360 / 480; loss 1.3755\n",
      "step  400 / 480; loss 1.3742\n",
      "step  440 / 480; loss 1.3720\n",
      "Evaluation loss 1.3769237894361845, accuracy 0.2637875, acc CATHODE: 0.223, acc CURTAINS: 0.325, acc FETA: 0.404, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 161 / 400\n",
      "step    0 / 480; loss 1.3720\n",
      "step   40 / 480; loss 1.3763\n",
      "step   80 / 480; loss 1.3783\n",
      "step  120 / 480; loss 1.3822\n",
      "step  160 / 480; loss 1.3727\n",
      "step  200 / 480; loss 1.3712\n",
      "step  240 / 480; loss 1.3764\n",
      "step  280 / 480; loss 1.3747\n",
      "step  320 / 480; loss 1.3744\n",
      "step  360 / 480; loss 1.3696\n",
      "step  400 / 480; loss 1.3743\n",
      "step  440 / 480; loss 1.3679\n",
      "Evaluation loss 1.3769767418676477, accuracy 0.26401875, acc CATHODE: 0.203, acc CURTAINS: 0.363, acc FETA: 0.378, acc SALAD: 0.112\n",
      "   - - - - -   \n",
      "Epoch 162 / 400\n",
      "step    0 / 480; loss 1.3802\n",
      "step   40 / 480; loss 1.3708\n",
      "step   80 / 480; loss 1.3794\n",
      "step  120 / 480; loss 1.3677\n",
      "step  160 / 480; loss 1.3791\n",
      "step  200 / 480; loss 1.3678\n",
      "step  240 / 480; loss 1.3866\n",
      "step  280 / 480; loss 1.3878\n",
      "step  320 / 480; loss 1.3795\n",
      "step  360 / 480; loss 1.3699\n",
      "step  400 / 480; loss 1.3912\n",
      "step  440 / 480; loss 1.3704\n",
      "Evaluation loss 1.3770204654718596, accuracy 0.261271875, acc CATHODE: 0.192, acc CURTAINS: 0.247, acc FETA: 0.524, acc SALAD: 0.081\n",
      "   - - - - -   \n",
      "Epoch 163 / 400\n",
      "step    0 / 480; loss 1.3784\n",
      "step   40 / 480; loss 1.3743\n",
      "step   80 / 480; loss 1.3716\n",
      "step  120 / 480; loss 1.3791\n",
      "step  160 / 480; loss 1.3614\n",
      "step  200 / 480; loss 1.3752\n",
      "step  240 / 480; loss 1.3764\n",
      "step  280 / 480; loss 1.3814\n",
      "step  320 / 480; loss 1.3682\n",
      "step  360 / 480; loss 1.3810\n",
      "step  400 / 480; loss 1.3728\n",
      "step  440 / 480; loss 1.3679\n",
      "Evaluation loss 1.3768544756595387, accuracy 0.262846875, acc CATHODE: 0.251, acc CURTAINS: 0.329, acc FETA: 0.342, acc SALAD: 0.129\n",
      "   - - - - -   \n",
      "Epoch 164 / 400\n",
      "step    0 / 480; loss 1.3725\n",
      "step   40 / 480; loss 1.3781\n",
      "step   80 / 480; loss 1.3681\n",
      "step  120 / 480; loss 1.3718\n",
      "step  160 / 480; loss 1.3669\n",
      "step  200 / 480; loss 1.3846\n",
      "step  240 / 480; loss 1.3745\n",
      "step  280 / 480; loss 1.3688\n",
      "step  320 / 480; loss 1.3716\n",
      "step  360 / 480; loss 1.3594\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3843\n",
      "Evaluation loss 1.377064988893863, accuracy 0.261303125, acc CATHODE: 0.269, acc CURTAINS: 0.317, acc FETA: 0.395, acc SALAD: 0.064\n",
      "   - - - - -   \n",
      "Epoch 165 / 400\n",
      "step    0 / 480; loss 1.3800\n",
      "step   40 / 480; loss 1.3823\n",
      "step   80 / 480; loss 1.3746\n",
      "step  120 / 480; loss 1.3742\n",
      "step  160 / 480; loss 1.3795\n",
      "step  200 / 480; loss 1.3659\n",
      "step  240 / 480; loss 1.3645\n",
      "step  280 / 480; loss 1.3781\n",
      "step  320 / 480; loss 1.3702\n",
      "step  360 / 480; loss 1.3781\n",
      "step  400 / 480; loss 1.3781\n",
      "step  440 / 480; loss 1.3687\n",
      "Evaluation loss 1.3768522326902306, accuracy 0.2624875, acc CATHODE: 0.214, acc CURTAINS: 0.382, acc FETA: 0.359, acc SALAD: 0.095\n",
      "   - - - - -   \n",
      "Epoch 166 / 400\n",
      "step    0 / 480; loss 1.3775\n",
      "step   40 / 480; loss 1.3727\n",
      "step   80 / 480; loss 1.3821\n",
      "step  120 / 480; loss 1.3799\n",
      "step  160 / 480; loss 1.3744\n",
      "step  200 / 480; loss 1.3751\n",
      "step  240 / 480; loss 1.3850\n",
      "step  280 / 480; loss 1.3661\n",
      "step  320 / 480; loss 1.3760\n",
      "step  360 / 480; loss 1.3764\n",
      "step  400 / 480; loss 1.3723\n",
      "step  440 / 480; loss 1.3701\n",
      "Evaluation loss 1.376994558930716, accuracy 0.262771875, acc CATHODE: 0.219, acc CURTAINS: 0.322, acc FETA: 0.407, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 167 / 400\n",
      "step    0 / 480; loss 1.3746\n",
      "step   40 / 480; loss 1.3700\n",
      "step   80 / 480; loss 1.3757\n",
      "step  120 / 480; loss 1.3794\n",
      "step  160 / 480; loss 1.3766\n",
      "step  200 / 480; loss 1.3713\n",
      "step  240 / 480; loss 1.3737\n",
      "step  280 / 480; loss 1.3758\n",
      "step  320 / 480; loss 1.3876\n",
      "step  360 / 480; loss 1.3805\n",
      "step  400 / 480; loss 1.3731\n",
      "step  440 / 480; loss 1.3755\n",
      "Evaluation loss 1.3770139332773403, accuracy 0.262703125, acc CATHODE: 0.334, acc CURTAINS: 0.329, acc FETA: 0.302, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 168 / 400\n",
      "step    0 / 480; loss 1.3635\n",
      "step   40 / 480; loss 1.3789\n",
      "step   80 / 480; loss 1.3820\n",
      "step  120 / 480; loss 1.3811\n",
      "step  160 / 480; loss 1.3779\n",
      "step  200 / 480; loss 1.3765\n",
      "step  240 / 480; loss 1.3865\n",
      "step  280 / 480; loss 1.3803\n",
      "step  320 / 480; loss 1.3743\n",
      "step  360 / 480; loss 1.3786\n",
      "step  400 / 480; loss 1.3744\n",
      "step  440 / 480; loss 1.3767\n",
      "Evaluation loss 1.3769216406095683, accuracy 0.2630875, acc CATHODE: 0.191, acc CURTAINS: 0.299, acc FETA: 0.449, acc SALAD: 0.114\n",
      "   - - - - -   \n",
      "Epoch 169 / 400\n",
      "step    0 / 480; loss 1.3616\n",
      "step   40 / 480; loss 1.3753\n",
      "step   80 / 480; loss 1.3719\n",
      "step  120 / 480; loss 1.3690\n",
      "step  160 / 480; loss 1.3781\n",
      "step  200 / 480; loss 1.3680\n",
      "step  240 / 480; loss 1.3727\n",
      "step  280 / 480; loss 1.3816\n",
      "step  320 / 480; loss 1.3704\n",
      "step  360 / 480; loss 1.3680\n",
      "step  400 / 480; loss 1.3751\n",
      "step  440 / 480; loss 1.3758\n",
      "Evaluation loss 1.3769814275192427, accuracy 0.262928125, acc CATHODE: 0.241, acc CURTAINS: 0.354, acc FETA: 0.385, acc SALAD: 0.072\n",
      "   - - - - -   \n",
      "Epoch 170 / 400\n",
      "step    0 / 480; loss 1.3676\n",
      "step   40 / 480; loss 1.3719\n",
      "step   80 / 480; loss 1.3766\n",
      "step  120 / 480; loss 1.3806\n",
      "step  160 / 480; loss 1.3763\n",
      "step  200 / 480; loss 1.3790\n",
      "step  240 / 480; loss 1.3802\n",
      "step  280 / 480; loss 1.3705\n",
      "step  320 / 480; loss 1.3732\n",
      "step  360 / 480; loss 1.3727\n",
      "step  400 / 480; loss 1.3721\n",
      "step  440 / 480; loss 1.3704\n",
      "Evaluation loss 1.3769248088041572, accuracy 0.263740625, acc CATHODE: 0.165, acc CURTAINS: 0.465, acc FETA: 0.339, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 171 / 400\n",
      "step    0 / 480; loss 1.3708\n",
      "step   40 / 480; loss 1.3764\n",
      "step   80 / 480; loss 1.3815\n",
      "step  120 / 480; loss 1.3792\n",
      "step  160 / 480; loss 1.3782\n",
      "step  200 / 480; loss 1.3773\n",
      "step  240 / 480; loss 1.3779\n",
      "step  280 / 480; loss 1.3764\n",
      "step  320 / 480; loss 1.3774\n",
      "step  360 / 480; loss 1.3797\n",
      "step  400 / 480; loss 1.3741\n",
      "step  440 / 480; loss 1.3716\n",
      "Evaluation loss 1.3769522623125783, accuracy 0.26296875, acc CATHODE: 0.154, acc CURTAINS: 0.399, acc FETA: 0.373, acc SALAD: 0.126\n",
      "   - - - - -   \n",
      "Epoch 172 / 400\n",
      "step    0 / 480; loss 1.3744\n",
      "step   40 / 480; loss 1.3767\n",
      "step   80 / 480; loss 1.3779\n",
      "step  120 / 480; loss 1.3704\n",
      "step  160 / 480; loss 1.3751\n",
      "step  200 / 480; loss 1.3805\n",
      "step  240 / 480; loss 1.3790\n",
      "step  280 / 480; loss 1.3735\n",
      "step  320 / 480; loss 1.3776\n",
      "step  360 / 480; loss 1.3745\n",
      "step  400 / 480; loss 1.3750\n",
      "step  440 / 480; loss 1.3767\n",
      "Evaluation loss 1.3770226257666613, accuracy 0.263490625, acc CATHODE: 0.185, acc CURTAINS: 0.347, acc FETA: 0.431, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 173 / 400\n",
      "step    0 / 480; loss 1.3684\n",
      "step   40 / 480; loss 1.3785\n",
      "step   80 / 480; loss 1.3899\n",
      "step  120 / 480; loss 1.3793\n",
      "step  160 / 480; loss 1.3749\n",
      "step  200 / 480; loss 1.3794\n",
      "step  240 / 480; loss 1.3782\n",
      "step  280 / 480; loss 1.3799\n",
      "step  320 / 480; loss 1.3641\n",
      "step  360 / 480; loss 1.3742\n",
      "step  400 / 480; loss 1.3757\n",
      "step  440 / 480; loss 1.3723\n",
      "Evaluation loss 1.3770777797974971, accuracy 0.2633625, acc CATHODE: 0.175, acc CURTAINS: 0.417, acc FETA: 0.381, acc SALAD: 0.081\n",
      "   - - - - -   \n",
      "Epoch 174 / 400\n",
      "step    0 / 480; loss 1.3850\n",
      "step   40 / 480; loss 1.3821\n",
      "step   80 / 480; loss 1.3686\n",
      "step  120 / 480; loss 1.3777\n",
      "step  160 / 480; loss 1.3764\n",
      "step  200 / 480; loss 1.3773\n",
      "step  240 / 480; loss 1.3730\n",
      "step  280 / 480; loss 1.3817\n",
      "step  320 / 480; loss 1.3775\n",
      "step  360 / 480; loss 1.3733\n",
      "step  400 / 480; loss 1.3809\n",
      "step  440 / 480; loss 1.3726\n",
      "Evaluation loss 1.3770031927009851, accuracy 0.265340625, acc CATHODE: 0.222, acc CURTAINS: 0.315, acc FETA: 0.416, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 175 / 400\n",
      "step    0 / 480; loss 1.3838\n",
      "step   40 / 480; loss 1.3711\n",
      "step   80 / 480; loss 1.3724\n",
      "step  120 / 480; loss 1.3761\n",
      "step  160 / 480; loss 1.3736\n",
      "step  200 / 480; loss 1.3812\n",
      "step  240 / 480; loss 1.3760\n",
      "step  280 / 480; loss 1.3746\n",
      "step  320 / 480; loss 1.3802\n",
      "step  360 / 480; loss 1.3868\n",
      "step  400 / 480; loss 1.3750\n",
      "step  440 / 480; loss 1.3774\n",
      "Evaluation loss 1.3770369828088658, accuracy 0.263153125, acc CATHODE: 0.181, acc CURTAINS: 0.314, acc FETA: 0.448, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 176 / 400\n",
      "step    0 / 480; loss 1.3773\n",
      "step   40 / 480; loss 1.3749\n",
      "step   80 / 480; loss 1.3755\n",
      "step  120 / 480; loss 1.3655\n",
      "step  160 / 480; loss 1.3780\n",
      "step  200 / 480; loss 1.3854\n",
      "step  240 / 480; loss 1.3705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  280 / 480; loss 1.3822\n",
      "step  320 / 480; loss 1.3817\n",
      "step  360 / 480; loss 1.3664\n",
      "step  400 / 480; loss 1.3784\n",
      "step  440 / 480; loss 1.3775\n",
      "Evaluation loss 1.3769275555969744, accuracy 0.2627, acc CATHODE: 0.240, acc CURTAINS: 0.315, acc FETA: 0.396, acc SALAD: 0.100\n",
      "   - - - - -   \n",
      "Epoch 177 / 400\n",
      "step    0 / 480; loss 1.3666\n",
      "step   40 / 480; loss 1.3724\n",
      "step   80 / 480; loss 1.3762\n",
      "step  120 / 480; loss 1.3751\n",
      "step  160 / 480; loss 1.3681\n",
      "step  200 / 480; loss 1.3748\n",
      "step  240 / 480; loss 1.3775\n",
      "step  280 / 480; loss 1.3846\n",
      "step  320 / 480; loss 1.3685\n",
      "step  360 / 480; loss 1.3809\n",
      "step  400 / 480; loss 1.3807\n",
      "step  440 / 480; loss 1.3779\n",
      "Evaluation loss 1.3771456802620903, accuracy 0.263065625, acc CATHODE: 0.213, acc CURTAINS: 0.374, acc FETA: 0.393, acc SALAD: 0.073\n",
      "   - - - - -   \n",
      "Epoch 178 / 400\n",
      "step    0 / 480; loss 1.3772\n",
      "step   40 / 480; loss 1.3768\n",
      "step   80 / 480; loss 1.3712\n",
      "step  120 / 480; loss 1.3760\n",
      "step  160 / 480; loss 1.3790\n",
      "step  200 / 480; loss 1.3734\n",
      "step  240 / 480; loss 1.3834\n",
      "step  280 / 480; loss 1.3751\n",
      "step  320 / 480; loss 1.3780\n",
      "step  360 / 480; loss 1.3782\n",
      "step  400 / 480; loss 1.3656\n",
      "step  440 / 480; loss 1.3757\n",
      "Evaluation loss 1.3768975838759847, accuracy 0.263853125, acc CATHODE: 0.203, acc CURTAINS: 0.376, acc FETA: 0.387, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 179 / 400\n",
      "step    0 / 480; loss 1.3744\n",
      "step   40 / 480; loss 1.3821\n",
      "step   80 / 480; loss 1.3691\n",
      "step  120 / 480; loss 1.3804\n",
      "step  160 / 480; loss 1.3771\n",
      "step  200 / 480; loss 1.3822\n",
      "step  240 / 480; loss 1.3777\n",
      "step  280 / 480; loss 1.3783\n",
      "step  320 / 480; loss 1.3728\n",
      "step  360 / 480; loss 1.3722\n",
      "step  400 / 480; loss 1.3751\n",
      "step  440 / 480; loss 1.3620\n",
      "Evaluation loss 1.376995839134198, accuracy 0.263690625, acc CATHODE: 0.144, acc CURTAINS: 0.465, acc FETA: 0.329, acc SALAD: 0.116\n",
      "   - - - - -   \n",
      "Epoch 180 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3738\n",
      "step   80 / 480; loss 1.3637\n",
      "step  120 / 480; loss 1.3656\n",
      "step  160 / 480; loss 1.3594\n",
      "step  200 / 480; loss 1.3778\n",
      "step  240 / 480; loss 1.3769\n",
      "step  280 / 480; loss 1.3832\n",
      "step  320 / 480; loss 1.3790\n",
      "step  360 / 480; loss 1.3648\n",
      "step  400 / 480; loss 1.3734\n",
      "step  440 / 480; loss 1.3868\n",
      "Evaluation loss 1.3769360220079856, accuracy 0.263665625, acc CATHODE: 0.260, acc CURTAINS: 0.362, acc FETA: 0.332, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 181 / 400\n",
      "step    0 / 480; loss 1.3765\n",
      "step   40 / 480; loss 1.3751\n",
      "step   80 / 480; loss 1.3777\n",
      "step  120 / 480; loss 1.3857\n",
      "step  160 / 480; loss 1.3781\n",
      "step  200 / 480; loss 1.3804\n",
      "step  240 / 480; loss 1.3707\n",
      "step  280 / 480; loss 1.3789\n",
      "step  320 / 480; loss 1.3649\n",
      "step  360 / 480; loss 1.3857\n",
      "step  400 / 480; loss 1.3766\n",
      "step  440 / 480; loss 1.3718\n",
      "Evaluation loss 1.376922265423677, accuracy 0.264209375, acc CATHODE: 0.204, acc CURTAINS: 0.352, acc FETA: 0.395, acc SALAD: 0.106\n",
      "   - - - - -   \n",
      "Epoch 182 / 400\n",
      "step    0 / 480; loss 1.3688\n",
      "step   40 / 480; loss 1.3817\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3865\n",
      "step  160 / 480; loss 1.3832\n",
      "step  200 / 480; loss 1.3667\n",
      "step  240 / 480; loss 1.3839\n",
      "step  280 / 480; loss 1.3850\n",
      "step  320 / 480; loss 1.3784\n",
      "step  360 / 480; loss 1.3758\n",
      "step  400 / 480; loss 1.3776\n",
      "step  440 / 480; loss 1.3683\n",
      "Evaluation loss 1.3770748467206488, accuracy 0.26303125, acc CATHODE: 0.238, acc CURTAINS: 0.346, acc FETA: 0.400, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 183 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3743\n",
      "step   80 / 480; loss 1.3753\n",
      "step  120 / 480; loss 1.3697\n",
      "step  160 / 480; loss 1.3705\n",
      "step  200 / 480; loss 1.3778\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3801\n",
      "step  320 / 480; loss 1.3731\n",
      "step  360 / 480; loss 1.3845\n",
      "step  400 / 480; loss 1.3728\n",
      "step  440 / 480; loss 1.3740\n",
      "Evaluation loss 1.3770321717030043, accuracy 0.26268125, acc CATHODE: 0.229, acc CURTAINS: 0.390, acc FETA: 0.376, acc SALAD: 0.057\n",
      "   - - - - -   \n",
      "Epoch 184 / 400\n",
      "step    0 / 480; loss 1.3658\n",
      "step   40 / 480; loss 1.3680\n",
      "step   80 / 480; loss 1.3707\n",
      "step  120 / 480; loss 1.3648\n",
      "step  160 / 480; loss 1.3763\n",
      "step  200 / 480; loss 1.3860\n",
      "step  240 / 480; loss 1.3810\n",
      "step  280 / 480; loss 1.3696\n",
      "step  320 / 480; loss 1.3778\n",
      "step  360 / 480; loss 1.3804\n",
      "step  400 / 480; loss 1.3704\n",
      "step  440 / 480; loss 1.3705\n",
      "Evaluation loss 1.3769845668597704, accuracy 0.2632125, acc CATHODE: 0.265, acc CURTAINS: 0.388, acc FETA: 0.309, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 185 / 400\n",
      "step    0 / 480; loss 1.3811\n",
      "step   40 / 480; loss 1.3765\n",
      "step   80 / 480; loss 1.3701\n",
      "step  120 / 480; loss 1.3847\n",
      "step  160 / 480; loss 1.3712\n",
      "step  200 / 480; loss 1.3817\n",
      "step  240 / 480; loss 1.3755\n",
      "step  280 / 480; loss 1.3848\n",
      "step  320 / 480; loss 1.3723\n",
      "step  360 / 480; loss 1.3777\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3738\n",
      "Evaluation loss 1.3770060486286704, accuracy 0.263478125, acc CATHODE: 0.273, acc CURTAINS: 0.325, acc FETA: 0.326, acc SALAD: 0.131\n",
      "   - - - - -   \n",
      "Epoch 186 / 400\n",
      "step    0 / 480; loss 1.3772\n",
      "step   40 / 480; loss 1.3694\n",
      "step   80 / 480; loss 1.3735\n",
      "step  120 / 480; loss 1.3774\n",
      "step  160 / 480; loss 1.3726\n",
      "step  200 / 480; loss 1.3655\n",
      "step  240 / 480; loss 1.3843\n",
      "step  280 / 480; loss 1.3781\n",
      "step  320 / 480; loss 1.3725\n",
      "step  360 / 480; loss 1.3839\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3752\n",
      "Evaluation loss 1.377110191510476, accuracy 0.2617125, acc CATHODE: 0.315, acc CURTAINS: 0.298, acc FETA: 0.329, acc SALAD: 0.104\n",
      "   - - - - -   \n",
      "Epoch 187 / 400\n",
      "step    0 / 480; loss 1.3809\n",
      "step   40 / 480; loss 1.3689\n",
      "step   80 / 480; loss 1.3761\n",
      "step  120 / 480; loss 1.3800\n",
      "step  160 / 480; loss 1.3680\n",
      "step  200 / 480; loss 1.3869\n",
      "step  240 / 480; loss 1.3785\n",
      "step  280 / 480; loss 1.3778\n",
      "step  320 / 480; loss 1.3803\n",
      "step  360 / 480; loss 1.3814\n",
      "step  400 / 480; loss 1.3681\n",
      "step  440 / 480; loss 1.3673\n",
      "Evaluation loss 1.377066060410272, accuracy 0.263290625, acc CATHODE: 0.208, acc CURTAINS: 0.354, acc FETA: 0.412, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 188 / 400\n",
      "step    0 / 480; loss 1.3670\n",
      "step   40 / 480; loss 1.3763\n",
      "step   80 / 480; loss 1.3759\n",
      "step  120 / 480; loss 1.3711\n",
      "step  160 / 480; loss 1.3745\n",
      "step  200 / 480; loss 1.3751\n",
      "step  240 / 480; loss 1.3764\n",
      "step  280 / 480; loss 1.3701\n",
      "step  320 / 480; loss 1.3760\n",
      "step  360 / 480; loss 1.3871\n",
      "step  400 / 480; loss 1.3766\n",
      "step  440 / 480; loss 1.3756\n",
      "Evaluation loss 1.3769732756661799, accuracy 0.262675, acc CATHODE: 0.223, acc CURTAINS: 0.416, acc FETA: 0.312, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 189 / 400\n",
      "step    0 / 480; loss 1.3776\n",
      "step   40 / 480; loss 1.3868\n",
      "step   80 / 480; loss 1.3764\n",
      "step  120 / 480; loss 1.3762\n",
      "step  160 / 480; loss 1.3807\n",
      "step  200 / 480; loss 1.3877\n",
      "step  240 / 480; loss 1.3765\n",
      "step  280 / 480; loss 1.3720\n",
      "step  320 / 480; loss 1.3665\n",
      "step  360 / 480; loss 1.3828\n",
      "step  400 / 480; loss 1.3720\n",
      "step  440 / 480; loss 1.3742\n",
      "Evaluation loss 1.3769692929324957, accuracy 0.263246875, acc CATHODE: 0.202, acc CURTAINS: 0.360, acc FETA: 0.408, acc SALAD: 0.083\n",
      "   - - - - -   \n",
      "Epoch 190 / 400\n",
      "step    0 / 480; loss 1.3852\n",
      "step   40 / 480; loss 1.3696\n",
      "step   80 / 480; loss 1.3816\n",
      "step  120 / 480; loss 1.3818\n",
      "step  160 / 480; loss 1.3845\n",
      "step  200 / 480; loss 1.3746\n",
      "step  240 / 480; loss 1.3748\n",
      "step  280 / 480; loss 1.3683\n",
      "step  320 / 480; loss 1.3752\n",
      "step  360 / 480; loss 1.3711\n",
      "step  400 / 480; loss 1.3774\n",
      "step  440 / 480; loss 1.3684\n",
      "Evaluation loss 1.3770142095710165, accuracy 0.262890625, acc CATHODE: 0.226, acc CURTAINS: 0.427, acc FETA: 0.299, acc SALAD: 0.100\n",
      "   - - - - -   \n",
      "Epoch 191 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3768\n",
      "step   80 / 480; loss 1.3626\n",
      "step  120 / 480; loss 1.3717\n",
      "step  160 / 480; loss 1.3752\n",
      "step  200 / 480; loss 1.3801\n",
      "step  240 / 480; loss 1.3754\n",
      "step  280 / 480; loss 1.3778\n",
      "step  320 / 480; loss 1.3682\n",
      "step  360 / 480; loss 1.3748\n",
      "step  400 / 480; loss 1.3778\n",
      "step  440 / 480; loss 1.3753\n",
      "Evaluation loss 1.37688388970557, accuracy 0.262609375, acc CATHODE: 0.177, acc CURTAINS: 0.387, acc FETA: 0.412, acc SALAD: 0.074\n",
      "   - - - - -   \n",
      "Epoch 192 / 400\n",
      "step    0 / 480; loss 1.3735\n",
      "step   40 / 480; loss 1.3711\n",
      "step   80 / 480; loss 1.3697\n",
      "step  120 / 480; loss 1.3833\n",
      "step  160 / 480; loss 1.3653\n",
      "step  200 / 480; loss 1.3736\n",
      "step  240 / 480; loss 1.3813\n",
      "step  280 / 480; loss 1.3767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  320 / 480; loss 1.3659\n",
      "step  360 / 480; loss 1.3725\n",
      "step  400 / 480; loss 1.3742\n",
      "step  440 / 480; loss 1.3685\n",
      "Evaluation loss 1.3770131215133572, accuracy 0.2629625, acc CATHODE: 0.131, acc CURTAINS: 0.438, acc FETA: 0.382, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 193 / 400\n",
      "step    0 / 480; loss 1.3742\n",
      "step   40 / 480; loss 1.3745\n",
      "step   80 / 480; loss 1.3782\n",
      "step  120 / 480; loss 1.3706\n",
      "step  160 / 480; loss 1.3785\n",
      "step  200 / 480; loss 1.3715\n",
      "step  240 / 480; loss 1.3765\n",
      "step  280 / 480; loss 1.3694\n",
      "step  320 / 480; loss 1.3764\n",
      "step  360 / 480; loss 1.3685\n",
      "step  400 / 480; loss 1.3734\n",
      "step  440 / 480; loss 1.3697\n",
      "Evaluation loss 1.3770597705853118, accuracy 0.263665625, acc CATHODE: 0.206, acc CURTAINS: 0.299, acc FETA: 0.417, acc SALAD: 0.133\n",
      "   - - - - -   \n",
      "Epoch 194 / 400\n",
      "step    0 / 480; loss 1.3714\n",
      "step   40 / 480; loss 1.3758\n",
      "step   80 / 480; loss 1.3750\n",
      "step  120 / 480; loss 1.3817\n",
      "step  160 / 480; loss 1.3777\n",
      "step  200 / 480; loss 1.3808\n",
      "step  240 / 480; loss 1.3824\n",
      "step  280 / 480; loss 1.3705\n",
      "step  320 / 480; loss 1.3820\n",
      "step  360 / 480; loss 1.3696\n",
      "step  400 / 480; loss 1.3676\n",
      "step  440 / 480; loss 1.3758\n",
      "Evaluation loss 1.3769219231814338, accuracy 0.26324375, acc CATHODE: 0.177, acc CURTAINS: 0.410, acc FETA: 0.381, acc SALAD: 0.084\n",
      "   - - - - -   \n",
      "Epoch 195 / 400\n",
      "step    0 / 480; loss 1.3739\n",
      "step   40 / 480; loss 1.3794\n",
      "step   80 / 480; loss 1.3735\n",
      "step  120 / 480; loss 1.3786\n",
      "step  160 / 480; loss 1.3791\n",
      "step  200 / 480; loss 1.3824\n",
      "step  240 / 480; loss 1.3715\n",
      "step  280 / 480; loss 1.3804\n",
      "step  320 / 480; loss 1.3816\n",
      "step  360 / 480; loss 1.3782\n",
      "step  400 / 480; loss 1.3803\n",
      "step  440 / 480; loss 1.3694\n",
      "Evaluation loss 1.376951889524368, accuracy 0.264103125, acc CATHODE: 0.132, acc CURTAINS: 0.403, acc FETA: 0.372, acc SALAD: 0.150\n",
      "   - - - - -   \n",
      "Epoch 196 / 400\n",
      "step    0 / 480; loss 1.3686\n",
      "step   40 / 480; loss 1.3651\n",
      "step   80 / 480; loss 1.3888\n",
      "step  120 / 480; loss 1.3850\n",
      "step  160 / 480; loss 1.3856\n",
      "step  200 / 480; loss 1.3791\n",
      "step  240 / 480; loss 1.3834\n",
      "step  280 / 480; loss 1.3822\n",
      "step  320 / 480; loss 1.3685\n",
      "step  360 / 480; loss 1.3675\n",
      "step  400 / 480; loss 1.3799\n",
      "step  440 / 480; loss 1.3720\n",
      "Evaluation loss 1.3769761025835294, accuracy 0.264640625, acc CATHODE: 0.180, acc CURTAINS: 0.394, acc FETA: 0.397, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 197 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3809\n",
      "step   80 / 480; loss 1.3697\n",
      "step  120 / 480; loss 1.3797\n",
      "step  160 / 480; loss 1.3823\n",
      "step  200 / 480; loss 1.3707\n",
      "step  240 / 480; loss 1.3757\n",
      "step  280 / 480; loss 1.3816\n",
      "step  320 / 480; loss 1.3753\n",
      "step  360 / 480; loss 1.3739\n",
      "step  400 / 480; loss 1.3734\n",
      "step  440 / 480; loss 1.3690\n",
      "Evaluation loss 1.376913586340439, accuracy 0.264009375, acc CATHODE: 0.170, acc CURTAINS: 0.343, acc FETA: 0.450, acc SALAD: 0.093\n",
      "   - - - - -   \n",
      "Epoch 198 / 400\n",
      "step    0 / 480; loss 1.3790\n",
      "step   40 / 480; loss 1.3734\n",
      "step   80 / 480; loss 1.3797\n",
      "step  120 / 480; loss 1.3800\n",
      "step  160 / 480; loss 1.3751\n",
      "step  200 / 480; loss 1.3599\n",
      "step  240 / 480; loss 1.3826\n",
      "step  280 / 480; loss 1.3743\n",
      "step  320 / 480; loss 1.3745\n",
      "step  360 / 480; loss 1.3725\n",
      "step  400 / 480; loss 1.3695\n",
      "step  440 / 480; loss 1.3764\n",
      "Evaluation loss 1.3771884397860161, accuracy 0.263125, acc CATHODE: 0.200, acc CURTAINS: 0.368, acc FETA: 0.399, acc SALAD: 0.086\n",
      "   - - - - -   \n",
      "Epoch 199 / 400\n",
      "step    0 / 480; loss 1.3600\n",
      "step   40 / 480; loss 1.3776\n",
      "step   80 / 480; loss 1.3748\n",
      "step  120 / 480; loss 1.3751\n",
      "step  160 / 480; loss 1.3691\n",
      "step  200 / 480; loss 1.3670\n",
      "step  240 / 480; loss 1.3641\n",
      "step  280 / 480; loss 1.3800\n",
      "step  320 / 480; loss 1.3868\n",
      "step  360 / 480; loss 1.3722\n",
      "step  400 / 480; loss 1.3674\n",
      "step  440 / 480; loss 1.3706\n",
      "Evaluation loss 1.3769274944576475, accuracy 0.263690625, acc CATHODE: 0.235, acc CURTAINS: 0.310, acc FETA: 0.375, acc SALAD: 0.135\n",
      "   - - - - -   \n",
      "Epoch 200 / 400\n",
      "step    0 / 480; loss 1.3709\n",
      "step   40 / 480; loss 1.3747\n",
      "step   80 / 480; loss 1.3744\n",
      "step  120 / 480; loss 1.3785\n",
      "step  160 / 480; loss 1.3784\n",
      "step  200 / 480; loss 1.3799\n",
      "step  240 / 480; loss 1.3671\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3720\n",
      "step  360 / 480; loss 1.3730\n",
      "step  400 / 480; loss 1.3783\n",
      "step  440 / 480; loss 1.3705\n",
      "Evaluation loss 1.376997911854495, accuracy 0.261434375, acc CATHODE: 0.205, acc CURTAINS: 0.329, acc FETA: 0.427, acc SALAD: 0.084\n",
      "   - - - - -   \n",
      "Epoch 201 / 400\n",
      "step    0 / 480; loss 1.3806\n",
      "step   40 / 480; loss 1.3806\n",
      "step   80 / 480; loss 1.3709\n",
      "step  120 / 480; loss 1.3807\n",
      "step  160 / 480; loss 1.3753\n",
      "step  200 / 480; loss 1.3733\n",
      "step  240 / 480; loss 1.3793\n",
      "step  280 / 480; loss 1.3755\n",
      "step  320 / 480; loss 1.3721\n",
      "step  360 / 480; loss 1.3691\n",
      "step  400 / 480; loss 1.3752\n",
      "step  440 / 480; loss 1.3784\n",
      "Evaluation loss 1.376963534774614, accuracy 0.26271875, acc CATHODE: 0.240, acc CURTAINS: 0.399, acc FETA: 0.317, acc SALAD: 0.096\n",
      "   - - - - -   \n",
      "Epoch 202 / 400\n",
      "step    0 / 480; loss 1.3739\n",
      "step   40 / 480; loss 1.3795\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3743\n",
      "step  160 / 480; loss 1.3768\n",
      "step  200 / 480; loss 1.3779\n",
      "step  240 / 480; loss 1.3691\n",
      "step  280 / 480; loss 1.3665\n",
      "step  320 / 480; loss 1.3815\n",
      "step  360 / 480; loss 1.3661\n",
      "step  400 / 480; loss 1.3714\n",
      "step  440 / 480; loss 1.3740\n",
      "Evaluation loss 1.3770064018173078, accuracy 0.2634125, acc CATHODE: 0.167, acc CURTAINS: 0.363, acc FETA: 0.389, acc SALAD: 0.135\n",
      "   - - - - -   \n",
      "Epoch 203 / 400\n",
      "step    0 / 480; loss 1.3573\n",
      "step   40 / 480; loss 1.3830\n",
      "step   80 / 480; loss 1.3704\n",
      "step  120 / 480; loss 1.3836\n",
      "step  160 / 480; loss 1.3712\n",
      "step  200 / 480; loss 1.3827\n",
      "step  240 / 480; loss 1.3847\n",
      "step  280 / 480; loss 1.3599\n",
      "step  320 / 480; loss 1.3705\n",
      "step  360 / 480; loss 1.3695\n",
      "step  400 / 480; loss 1.3881\n",
      "step  440 / 480; loss 1.3753\n",
      "Evaluation loss 1.3771299849351892, accuracy 0.26259375, acc CATHODE: 0.296, acc CURTAINS: 0.276, acc FETA: 0.368, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 204 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3721\n",
      "step   80 / 480; loss 1.3702\n",
      "step  120 / 480; loss 1.3742\n",
      "step  160 / 480; loss 1.3732\n",
      "step  200 / 480; loss 1.3757\n",
      "step  240 / 480; loss 1.3808\n",
      "step  280 / 480; loss 1.3807\n",
      "step  320 / 480; loss 1.3827\n",
      "step  360 / 480; loss 1.3728\n",
      "step  400 / 480; loss 1.3757\n",
      "step  440 / 480; loss 1.3793\n",
      "Evaluation loss 1.3770328034417485, accuracy 0.2627, acc CATHODE: 0.290, acc CURTAINS: 0.310, acc FETA: 0.357, acc SALAD: 0.094\n",
      "   - - - - -   \n",
      "Epoch 205 / 400\n",
      "step    0 / 480; loss 1.3707\n",
      "step   40 / 480; loss 1.3688\n",
      "step   80 / 480; loss 1.3730\n",
      "step  120 / 480; loss 1.3674\n",
      "step  160 / 480; loss 1.3718\n",
      "step  200 / 480; loss 1.3716\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3703\n",
      "step  320 / 480; loss 1.3721\n",
      "step  360 / 480; loss 1.3665\n",
      "step  400 / 480; loss 1.3705\n",
      "step  440 / 480; loss 1.3737\n",
      "Evaluation loss 1.377096956728674, accuracy 0.262515625, acc CATHODE: 0.164, acc CURTAINS: 0.335, acc FETA: 0.441, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 206 / 400\n",
      "step    0 / 480; loss 1.3682\n",
      "step   40 / 480; loss 1.3665\n",
      "step   80 / 480; loss 1.3683\n",
      "step  120 / 480; loss 1.3811\n",
      "step  160 / 480; loss 1.3788\n",
      "step  200 / 480; loss 1.3804\n",
      "step  240 / 480; loss 1.3778\n",
      "step  280 / 480; loss 1.3802\n",
      "step  320 / 480; loss 1.3671\n",
      "step  360 / 480; loss 1.3673\n",
      "step  400 / 480; loss 1.3695\n",
      "step  440 / 480; loss 1.3757\n",
      "Evaluation loss 1.3772574395081405, accuracy 0.261378125, acc CATHODE: 0.208, acc CURTAINS: 0.354, acc FETA: 0.313, acc SALAD: 0.171\n",
      "   - - - - -   \n",
      "Epoch 207 / 400\n",
      "step    0 / 480; loss 1.3785\n",
      "step   40 / 480; loss 1.3723\n",
      "step   80 / 480; loss 1.3811\n",
      "step  120 / 480; loss 1.3819\n",
      "step  160 / 480; loss 1.3719\n",
      "step  200 / 480; loss 1.3638\n",
      "step  240 / 480; loss 1.3759\n",
      "step  280 / 480; loss 1.3797\n",
      "step  320 / 480; loss 1.3735\n",
      "step  360 / 480; loss 1.3680\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3721\n",
      "Evaluation loss 1.3770139976587277, accuracy 0.263415625, acc CATHODE: 0.277, acc CURTAINS: 0.352, acc FETA: 0.347, acc SALAD: 0.079\n",
      "   - - - - -   \n",
      "Epoch 208 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3846\n",
      "step   80 / 480; loss 1.3595\n",
      "step  120 / 480; loss 1.3783\n",
      "step  160 / 480; loss 1.3697\n",
      "step  200 / 480; loss 1.3681\n",
      "step  240 / 480; loss 1.3743\n",
      "step  280 / 480; loss 1.3719\n",
      "step  320 / 480; loss 1.3762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  360 / 480; loss 1.3658\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3788\n",
      "Evaluation loss 1.3770088966259, accuracy 0.2621375, acc CATHODE: 0.207, acc CURTAINS: 0.413, acc FETA: 0.277, acc SALAD: 0.152\n",
      "   - - - - -   \n",
      "Epoch 209 / 400\n",
      "step    0 / 480; loss 1.3755\n",
      "step   40 / 480; loss 1.3717\n",
      "step   80 / 480; loss 1.3864\n",
      "step  120 / 480; loss 1.3806\n",
      "step  160 / 480; loss 1.3723\n",
      "step  200 / 480; loss 1.3789\n",
      "step  240 / 480; loss 1.3635\n",
      "step  280 / 480; loss 1.3857\n",
      "step  320 / 480; loss 1.3726\n",
      "step  360 / 480; loss 1.3684\n",
      "step  400 / 480; loss 1.3756\n",
      "step  440 / 480; loss 1.3766\n",
      "Evaluation loss 1.3769454084270716, accuracy 0.26221875, acc CATHODE: 0.190, acc CURTAINS: 0.326, acc FETA: 0.446, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 210 / 400\n",
      "step    0 / 480; loss 1.3784\n",
      "step   40 / 480; loss 1.3735\n",
      "step   80 / 480; loss 1.3727\n",
      "step  120 / 480; loss 1.3809\n",
      "step  160 / 480; loss 1.3794\n",
      "step  200 / 480; loss 1.3705\n",
      "step  240 / 480; loss 1.3636\n",
      "step  280 / 480; loss 1.3734\n",
      "step  320 / 480; loss 1.3806\n",
      "step  360 / 480; loss 1.3745\n",
      "step  400 / 480; loss 1.3814\n",
      "step  440 / 480; loss 1.3788\n",
      "Evaluation loss 1.3770836298194058, accuracy 0.262471875, acc CATHODE: 0.261, acc CURTAINS: 0.258, acc FETA: 0.382, acc SALAD: 0.150\n",
      "   - - - - -   \n",
      "Epoch 211 / 400\n",
      "step    0 / 480; loss 1.3715\n",
      "step   40 / 480; loss 1.3812\n",
      "step   80 / 480; loss 1.3813\n",
      "step  120 / 480; loss 1.3769\n",
      "step  160 / 480; loss 1.3732\n",
      "step  200 / 480; loss 1.3724\n",
      "step  240 / 480; loss 1.3751\n",
      "step  280 / 480; loss 1.3710\n",
      "step  320 / 480; loss 1.3699\n",
      "step  360 / 480; loss 1.3806\n",
      "step  400 / 480; loss 1.3654\n",
      "step  440 / 480; loss 1.3716\n",
      "Evaluation loss 1.3770714511652384, accuracy 0.2625875, acc CATHODE: 0.204, acc CURTAINS: 0.378, acc FETA: 0.393, acc SALAD: 0.075\n",
      "   - - - - -   \n",
      "Epoch 212 / 400\n",
      "step    0 / 480; loss 1.3806\n",
      "step   40 / 480; loss 1.3686\n",
      "step   80 / 480; loss 1.3778\n",
      "step  120 / 480; loss 1.3688\n",
      "step  160 / 480; loss 1.3823\n",
      "step  200 / 480; loss 1.3768\n",
      "step  240 / 480; loss 1.3804\n",
      "step  280 / 480; loss 1.3769\n",
      "step  320 / 480; loss 1.3756\n",
      "step  360 / 480; loss 1.3778\n",
      "step  400 / 480; loss 1.3748\n",
      "step  440 / 480; loss 1.3784\n",
      "Evaluation loss 1.3771434489100383, accuracy 0.261353125, acc CATHODE: 0.178, acc CURTAINS: 0.335, acc FETA: 0.345, acc SALAD: 0.187\n",
      "   - - - - -   \n",
      "Epoch 213 / 400\n",
      "step    0 / 480; loss 1.3750\n",
      "step   40 / 480; loss 1.3729\n",
      "step   80 / 480; loss 1.3700\n",
      "step  120 / 480; loss 1.3731\n",
      "step  160 / 480; loss 1.3813\n",
      "step  200 / 480; loss 1.3769\n",
      "step  240 / 480; loss 1.3783\n",
      "step  280 / 480; loss 1.3776\n",
      "step  320 / 480; loss 1.3747\n",
      "step  360 / 480; loss 1.3710\n",
      "step  400 / 480; loss 1.3757\n",
      "step  440 / 480; loss 1.3814\n",
      "Evaluation loss 1.3770336976972883, accuracy 0.261625, acc CATHODE: 0.175, acc CURTAINS: 0.348, acc FETA: 0.402, acc SALAD: 0.121\n",
      "   - - - - -   \n",
      "Epoch 214 / 400\n",
      "step    0 / 480; loss 1.3788\n",
      "step   40 / 480; loss 1.3675\n",
      "step   80 / 480; loss 1.3806\n",
      "step  120 / 480; loss 1.3705\n",
      "step  160 / 480; loss 1.3813\n",
      "step  200 / 480; loss 1.3683\n",
      "step  240 / 480; loss 1.3797\n",
      "step  280 / 480; loss 1.3609\n",
      "step  320 / 480; loss 1.3752\n",
      "step  360 / 480; loss 1.3697\n",
      "step  400 / 480; loss 1.3814\n",
      "step  440 / 480; loss 1.3679\n",
      "Evaluation loss 1.3771735757016201, accuracy 0.26236875, acc CATHODE: 0.267, acc CURTAINS: 0.383, acc FETA: 0.263, acc SALAD: 0.137\n",
      "   - - - - -   \n",
      "Epoch 215 / 400\n",
      "step    0 / 480; loss 1.3799\n",
      "step   40 / 480; loss 1.3688\n",
      "step   80 / 480; loss 1.3775\n",
      "step  120 / 480; loss 1.3837\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3699\n",
      "step  240 / 480; loss 1.3761\n",
      "step  280 / 480; loss 1.3741\n",
      "step  320 / 480; loss 1.3764\n",
      "step  360 / 480; loss 1.3697\n",
      "step  400 / 480; loss 1.3724\n",
      "step  440 / 480; loss 1.3710\n",
      "Evaluation loss 1.3770871205971105, accuracy 0.26315625, acc CATHODE: 0.276, acc CURTAINS: 0.317, acc FETA: 0.325, acc SALAD: 0.134\n",
      "   - - - - -   \n",
      "Epoch 216 / 400\n",
      "step    0 / 480; loss 1.3742\n",
      "step   40 / 480; loss 1.3684\n",
      "step   80 / 480; loss 1.3707\n",
      "step  120 / 480; loss 1.3726\n",
      "step  160 / 480; loss 1.3661\n",
      "step  200 / 480; loss 1.3743\n",
      "step  240 / 480; loss 1.3717\n",
      "step  280 / 480; loss 1.3668\n",
      "step  320 / 480; loss 1.3742\n",
      "step  360 / 480; loss 1.3817\n",
      "step  400 / 480; loss 1.3736\n",
      "step  440 / 480; loss 1.3784\n",
      "Evaluation loss 1.377242047424135, accuracy 0.264246875, acc CATHODE: 0.189, acc CURTAINS: 0.316, acc FETA: 0.404, acc SALAD: 0.148\n",
      "   - - - - -   \n",
      "Epoch 217 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3683\n",
      "step   80 / 480; loss 1.3841\n",
      "step  120 / 480; loss 1.3720\n",
      "step  160 / 480; loss 1.3757\n",
      "step  200 / 480; loss 1.3789\n",
      "step  240 / 480; loss 1.3711\n",
      "step  280 / 480; loss 1.3748\n",
      "step  320 / 480; loss 1.3815\n",
      "step  360 / 480; loss 1.3761\n",
      "step  400 / 480; loss 1.3810\n",
      "step  440 / 480; loss 1.3753\n",
      "Evaluation loss 1.377246383852236, accuracy 0.263575, acc CATHODE: 0.254, acc CURTAINS: 0.353, acc FETA: 0.348, acc SALAD: 0.099\n",
      "   - - - - -   \n",
      "Epoch 218 / 400\n",
      "step    0 / 480; loss 1.3708\n",
      "step   40 / 480; loss 1.3778\n",
      "step   80 / 480; loss 1.3751\n",
      "step  120 / 480; loss 1.3745\n",
      "step  160 / 480; loss 1.3716\n",
      "step  200 / 480; loss 1.3713\n",
      "step  240 / 480; loss 1.3877\n",
      "step  280 / 480; loss 1.3691\n",
      "step  320 / 480; loss 1.3708\n",
      "step  360 / 480; loss 1.3871\n",
      "step  400 / 480; loss 1.3787\n",
      "step  440 / 480; loss 1.3761\n",
      "Evaluation loss 1.3769477460587076, accuracy 0.262446875, acc CATHODE: 0.172, acc CURTAINS: 0.456, acc FETA: 0.354, acc SALAD: 0.067\n",
      "   - - - - -   \n",
      "Epoch 219 / 400\n",
      "step    0 / 480; loss 1.3749\n",
      "step   40 / 480; loss 1.3767\n",
      "step   80 / 480; loss 1.3793\n",
      "step  120 / 480; loss 1.3689\n",
      "step  160 / 480; loss 1.3696\n",
      "step  200 / 480; loss 1.3791\n",
      "step  240 / 480; loss 1.3761\n",
      "step  280 / 480; loss 1.3673\n",
      "step  320 / 480; loss 1.3719\n",
      "step  360 / 480; loss 1.3750\n",
      "step  400 / 480; loss 1.3755\n",
      "step  440 / 480; loss 1.3750\n",
      "Evaluation loss 1.3770712907602591, accuracy 0.26323125, acc CATHODE: 0.233, acc CURTAINS: 0.344, acc FETA: 0.377, acc SALAD: 0.098\n",
      "   - - - - -   \n",
      "Epoch 220 / 400\n",
      "step    0 / 480; loss 1.3756\n",
      "step   40 / 480; loss 1.3685\n",
      "step   80 / 480; loss 1.3773\n",
      "step  120 / 480; loss 1.3658\n",
      "step  160 / 480; loss 1.3791\n",
      "step  200 / 480; loss 1.3676\n",
      "step  240 / 480; loss 1.3759\n",
      "step  280 / 480; loss 1.3726\n",
      "step  320 / 480; loss 1.3796\n",
      "step  360 / 480; loss 1.3728\n",
      "step  400 / 480; loss 1.3872\n",
      "step  440 / 480; loss 1.3768\n",
      "Evaluation loss 1.377100943230394, accuracy 0.2624, acc CATHODE: 0.254, acc CURTAINS: 0.355, acc FETA: 0.336, acc SALAD: 0.104\n",
      "   - - - - -   \n",
      "Epoch 221 / 400\n",
      "step    0 / 480; loss 1.3767\n",
      "step   40 / 480; loss 1.3732\n",
      "step   80 / 480; loss 1.3693\n",
      "step  120 / 480; loss 1.3821\n",
      "step  160 / 480; loss 1.3813\n",
      "step  200 / 480; loss 1.3825\n",
      "step  240 / 480; loss 1.3705\n",
      "step  280 / 480; loss 1.3800\n",
      "step  320 / 480; loss 1.3674\n",
      "step  360 / 480; loss 1.3716\n",
      "step  400 / 480; loss 1.3775\n",
      "step  440 / 480; loss 1.3744\n",
      "Evaluation loss 1.3769096896130875, accuracy 0.263078125, acc CATHODE: 0.197, acc CURTAINS: 0.425, acc FETA: 0.337, acc SALAD: 0.094\n",
      "   - - - - -   \n",
      "Epoch 222 / 400\n",
      "step    0 / 480; loss 1.3805\n",
      "step   40 / 480; loss 1.3706\n",
      "step   80 / 480; loss 1.3806\n",
      "step  120 / 480; loss 1.3828\n",
      "step  160 / 480; loss 1.3717\n",
      "step  200 / 480; loss 1.3657\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3703\n",
      "step  320 / 480; loss 1.3775\n",
      "step  360 / 480; loss 1.3788\n",
      "step  400 / 480; loss 1.3742\n",
      "step  440 / 480; loss 1.3772\n",
      "Evaluation loss 1.377194340169135, accuracy 0.262534375, acc CATHODE: 0.199, acc CURTAINS: 0.277, acc FETA: 0.468, acc SALAD: 0.107\n",
      "   - - - - -   \n",
      "Epoch 223 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3828\n",
      "step   80 / 480; loss 1.3727\n",
      "step  120 / 480; loss 1.3757\n",
      "step  160 / 480; loss 1.3726\n",
      "step  200 / 480; loss 1.3666\n",
      "step  240 / 480; loss 1.3733\n",
      "step  280 / 480; loss 1.3740\n",
      "step  320 / 480; loss 1.3724\n",
      "step  360 / 480; loss 1.3712\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3854\n",
      "Evaluation loss 1.376904160497824, accuracy 0.26289375, acc CATHODE: 0.202, acc CURTAINS: 0.363, acc FETA: 0.381, acc SALAD: 0.105\n",
      "   - - - - -   \n",
      "Epoch 224 / 400\n",
      "step    0 / 480; loss 1.3799\n",
      "step   40 / 480; loss 1.3693\n",
      "step   80 / 480; loss 1.3883\n",
      "step  120 / 480; loss 1.3755\n",
      "step  160 / 480; loss 1.3847\n",
      "step  200 / 480; loss 1.3719\n",
      "step  240 / 480; loss 1.3754\n",
      "step  280 / 480; loss 1.3838\n",
      "step  320 / 480; loss 1.3739\n",
      "step  360 / 480; loss 1.3675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  400 / 480; loss 1.3676\n",
      "step  440 / 480; loss 1.3644\n",
      "Evaluation loss 1.377128415216245, accuracy 0.262884375, acc CATHODE: 0.218, acc CURTAINS: 0.398, acc FETA: 0.355, acc SALAD: 0.081\n",
      "   - - - - -   \n",
      "Epoch 225 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3658\n",
      "step  120 / 480; loss 1.3776\n",
      "step  160 / 480; loss 1.3835\n",
      "step  200 / 480; loss 1.3708\n",
      "step  240 / 480; loss 1.3705\n",
      "step  280 / 480; loss 1.3731\n",
      "step  320 / 480; loss 1.3756\n",
      "step  360 / 480; loss 1.3756\n",
      "step  400 / 480; loss 1.3815\n",
      "step  440 / 480; loss 1.3644\n",
      "Evaluation loss 1.37700321136102, accuracy 0.2637875, acc CATHODE: 0.299, acc CURTAINS: 0.301, acc FETA: 0.368, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 226 / 400\n",
      "step    0 / 480; loss 1.3756\n",
      "step   40 / 480; loss 1.3729\n",
      "step   80 / 480; loss 1.3663\n",
      "step  120 / 480; loss 1.3739\n",
      "step  160 / 480; loss 1.3783\n",
      "step  200 / 480; loss 1.3664\n",
      "step  240 / 480; loss 1.3743\n",
      "step  280 / 480; loss 1.3803\n",
      "step  320 / 480; loss 1.3721\n",
      "step  360 / 480; loss 1.3680\n",
      "step  400 / 480; loss 1.3719\n",
      "step  440 / 480; loss 1.3641\n",
      "Evaluation loss 1.3770375204203866, accuracy 0.26341875, acc CATHODE: 0.252, acc CURTAINS: 0.269, acc FETA: 0.420, acc SALAD: 0.112\n",
      "   - - - - -   \n",
      "Epoch 227 / 400\n",
      "step    0 / 480; loss 1.3712\n",
      "step   40 / 480; loss 1.3662\n",
      "step   80 / 480; loss 1.3765\n",
      "step  120 / 480; loss 1.3743\n",
      "step  160 / 480; loss 1.3772\n",
      "step  200 / 480; loss 1.3815\n",
      "step  240 / 480; loss 1.3713\n",
      "step  280 / 480; loss 1.3788\n",
      "step  320 / 480; loss 1.3825\n",
      "step  360 / 480; loss 1.3732\n",
      "step  400 / 480; loss 1.3711\n",
      "step  440 / 480; loss 1.3766\n",
      "Evaluation loss 1.3770293266497715, accuracy 0.262534375, acc CATHODE: 0.157, acc CURTAINS: 0.367, acc FETA: 0.451, acc SALAD: 0.076\n",
      "   - - - - -   \n",
      "Epoch 228 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3746\n",
      "step   80 / 480; loss 1.3769\n",
      "step  120 / 480; loss 1.3704\n",
      "step  160 / 480; loss 1.3783\n",
      "step  200 / 480; loss 1.3733\n",
      "step  240 / 480; loss 1.3690\n",
      "step  280 / 480; loss 1.3668\n",
      "step  320 / 480; loss 1.3684\n",
      "step  360 / 480; loss 1.3683\n",
      "step  400 / 480; loss 1.3764\n",
      "step  440 / 480; loss 1.3579\n",
      "Evaluation loss 1.3771495099124467, accuracy 0.261965625, acc CATHODE: 0.155, acc CURTAINS: 0.309, acc FETA: 0.461, acc SALAD: 0.122\n",
      "   - - - - -   \n",
      "Epoch 229 / 400\n",
      "step    0 / 480; loss 1.3790\n",
      "step   40 / 480; loss 1.3800\n",
      "step   80 / 480; loss 1.3751\n",
      "step  120 / 480; loss 1.3790\n",
      "step  160 / 480; loss 1.3723\n",
      "step  200 / 480; loss 1.3835\n",
      "step  240 / 480; loss 1.3730\n",
      "step  280 / 480; loss 1.3739\n",
      "step  320 / 480; loss 1.3726\n",
      "step  360 / 480; loss 1.3718\n",
      "step  400 / 480; loss 1.3849\n",
      "step  440 / 480; loss 1.3798\n",
      "Evaluation loss 1.3769957891560494, accuracy 0.2631625, acc CATHODE: 0.205, acc CURTAINS: 0.346, acc FETA: 0.402, acc SALAD: 0.099\n",
      "   - - - - -   \n",
      "Epoch 230 / 400\n",
      "step    0 / 480; loss 1.3659\n",
      "step   40 / 480; loss 1.3829\n",
      "step   80 / 480; loss 1.3732\n",
      "step  120 / 480; loss 1.3731\n",
      "step  160 / 480; loss 1.3792\n",
      "step  200 / 480; loss 1.3698\n",
      "step  240 / 480; loss 1.3777\n",
      "step  280 / 480; loss 1.3687\n",
      "step  320 / 480; loss 1.3727\n",
      "step  360 / 480; loss 1.3722\n",
      "step  400 / 480; loss 1.3855\n",
      "step  440 / 480; loss 1.3771\n",
      "Evaluation loss 1.3771464684223176, accuracy 0.26335, acc CATHODE: 0.184, acc CURTAINS: 0.349, acc FETA: 0.416, acc SALAD: 0.104\n",
      "   - - - - -   \n",
      "Epoch 231 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3739\n",
      "step   80 / 480; loss 1.3792\n",
      "step  120 / 480; loss 1.3748\n",
      "step  160 / 480; loss 1.3790\n",
      "step  200 / 480; loss 1.3864\n",
      "step  240 / 480; loss 1.3843\n",
      "step  280 / 480; loss 1.3674\n",
      "step  320 / 480; loss 1.3831\n",
      "step  360 / 480; loss 1.3659\n",
      "step  400 / 480; loss 1.3684\n",
      "step  440 / 480; loss 1.3741\n",
      "Evaluation loss 1.3769371809869193, accuracy 0.26253125, acc CATHODE: 0.228, acc CURTAINS: 0.332, acc FETA: 0.357, acc SALAD: 0.133\n",
      "   - - - - -   \n",
      "Epoch 232 / 400\n",
      "step    0 / 480; loss 1.3686\n",
      "step   40 / 480; loss 1.3702\n",
      "step   80 / 480; loss 1.3802\n",
      "step  120 / 480; loss 1.3766\n",
      "step  160 / 480; loss 1.3759\n",
      "step  200 / 480; loss 1.3686\n",
      "step  240 / 480; loss 1.3686\n",
      "step  280 / 480; loss 1.3822\n",
      "step  320 / 480; loss 1.3771\n",
      "step  360 / 480; loss 1.3737\n",
      "step  400 / 480; loss 1.3699\n",
      "step  440 / 480; loss 1.3654\n",
      "Evaluation loss 1.3772382514334658, accuracy 0.263128125, acc CATHODE: 0.209, acc CURTAINS: 0.284, acc FETA: 0.429, acc SALAD: 0.130\n",
      "   - - - - -   \n",
      "Epoch 233 / 400\n",
      "step    0 / 480; loss 1.3891\n",
      "step   40 / 480; loss 1.3715\n",
      "step   80 / 480; loss 1.3756\n",
      "step  120 / 480; loss 1.3662\n",
      "step  160 / 480; loss 1.3838\n",
      "step  200 / 480; loss 1.3784\n",
      "step  240 / 480; loss 1.3737\n",
      "step  280 / 480; loss 1.3793\n",
      "step  320 / 480; loss 1.3871\n",
      "step  360 / 480; loss 1.3706\n",
      "step  400 / 480; loss 1.3640\n",
      "step  440 / 480; loss 1.3830\n",
      "Evaluation loss 1.3771988980336616, accuracy 0.263278125, acc CATHODE: 0.193, acc CURTAINS: 0.345, acc FETA: 0.425, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 234 / 400\n",
      "step    0 / 480; loss 1.3828\n",
      "step   40 / 480; loss 1.3739\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3686\n",
      "step  160 / 480; loss 1.3825\n",
      "step  200 / 480; loss 1.3776\n",
      "step  240 / 480; loss 1.3769\n",
      "step  280 / 480; loss 1.3784\n",
      "step  320 / 480; loss 1.3735\n",
      "step  360 / 480; loss 1.3715\n",
      "step  400 / 480; loss 1.3774\n",
      "step  440 / 480; loss 1.3708\n",
      "Evaluation loss 1.3771478578745924, accuracy 0.262465625, acc CATHODE: 0.160, acc CURTAINS: 0.389, acc FETA: 0.419, acc SALAD: 0.081\n",
      "   - - - - -   \n",
      "Epoch 235 / 400\n",
      "step    0 / 480; loss 1.3664\n",
      "step   40 / 480; loss 1.3794\n",
      "step   80 / 480; loss 1.3730\n",
      "step  120 / 480; loss 1.3806\n",
      "step  160 / 480; loss 1.3759\n",
      "step  200 / 480; loss 1.3736\n",
      "step  240 / 480; loss 1.3671\n",
      "step  280 / 480; loss 1.3712\n",
      "step  320 / 480; loss 1.3802\n",
      "step  360 / 480; loss 1.3745\n",
      "step  400 / 480; loss 1.3771\n",
      "step  440 / 480; loss 1.3626\n",
      "Evaluation loss 1.3771443406091521, accuracy 0.262925, acc CATHODE: 0.259, acc CURTAINS: 0.327, acc FETA: 0.367, acc SALAD: 0.098\n",
      "   - - - - -   \n",
      "Epoch 236 / 400\n",
      "step    0 / 480; loss 1.3792\n",
      "step   40 / 480; loss 1.3757\n",
      "step   80 / 480; loss 1.3839\n",
      "step  120 / 480; loss 1.3714\n",
      "step  160 / 480; loss 1.3736\n",
      "step  200 / 480; loss 1.3740\n",
      "step  240 / 480; loss 1.3821\n",
      "step  280 / 480; loss 1.3699\n",
      "step  320 / 480; loss 1.3783\n",
      "step  360 / 480; loss 1.3850\n",
      "step  400 / 480; loss 1.3721\n",
      "step  440 / 480; loss 1.3743\n",
      "Evaluation loss 1.3771162481325874, accuracy 0.263090625, acc CATHODE: 0.231, acc CURTAINS: 0.368, acc FETA: 0.347, acc SALAD: 0.106\n",
      "   - - - - -   \n",
      "Epoch 237 / 400\n",
      "step    0 / 480; loss 1.3737\n",
      "step   40 / 480; loss 1.3910\n",
      "step   80 / 480; loss 1.3751\n",
      "step  120 / 480; loss 1.3846\n",
      "step  160 / 480; loss 1.3611\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3753\n",
      "step  280 / 480; loss 1.3758\n",
      "step  320 / 480; loss 1.3675\n",
      "step  360 / 480; loss 1.3699\n",
      "step  400 / 480; loss 1.3828\n",
      "step  440 / 480; loss 1.3724\n",
      "Evaluation loss 1.3772125183303738, accuracy 0.26258125, acc CATHODE: 0.206, acc CURTAINS: 0.385, acc FETA: 0.349, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 238 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3721\n",
      "step   80 / 480; loss 1.3729\n",
      "step  120 / 480; loss 1.3743\n",
      "step  160 / 480; loss 1.3760\n",
      "step  200 / 480; loss 1.3740\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3851\n",
      "step  320 / 480; loss 1.3726\n",
      "step  360 / 480; loss 1.3791\n",
      "step  400 / 480; loss 1.3679\n",
      "step  440 / 480; loss 1.3722\n",
      "Evaluation loss 1.377149090175753, accuracy 0.2627, acc CATHODE: 0.222, acc CURTAINS: 0.375, acc FETA: 0.350, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 239 / 400\n",
      "step    0 / 480; loss 1.3709\n",
      "step   40 / 480; loss 1.3793\n",
      "step   80 / 480; loss 1.3757\n",
      "step  120 / 480; loss 1.3657\n",
      "step  160 / 480; loss 1.3661\n",
      "step  200 / 480; loss 1.3785\n",
      "step  240 / 480; loss 1.3686\n",
      "step  280 / 480; loss 1.3646\n",
      "step  320 / 480; loss 1.3832\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3773\n",
      "step  440 / 480; loss 1.3795\n",
      "Evaluation loss 1.3771319476395323, accuracy 0.26274375, acc CATHODE: 0.162, acc CURTAINS: 0.421, acc FETA: 0.384, acc SALAD: 0.084\n",
      "   - - - - -   \n",
      "Epoch 240 / 400\n",
      "step    0 / 480; loss 1.3715\n",
      "step   40 / 480; loss 1.3738\n",
      "step   80 / 480; loss 1.3771\n",
      "step  120 / 480; loss 1.3593\n",
      "step  160 / 480; loss 1.3748\n",
      "step  200 / 480; loss 1.3622\n",
      "step  240 / 480; loss 1.3852\n",
      "step  280 / 480; loss 1.3731\n",
      "step  320 / 480; loss 1.3744\n",
      "step  360 / 480; loss 1.3775\n",
      "step  400 / 480; loss 1.3717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  440 / 480; loss 1.3785\n",
      "Evaluation loss 1.3771525402467204, accuracy 0.262428125, acc CATHODE: 0.317, acc CURTAINS: 0.277, acc FETA: 0.355, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 241 / 400\n",
      "step    0 / 480; loss 1.3774\n",
      "step   40 / 480; loss 1.3776\n",
      "step   80 / 480; loss 1.3795\n",
      "step  120 / 480; loss 1.3721\n",
      "step  160 / 480; loss 1.3779\n",
      "step  200 / 480; loss 1.3774\n",
      "step  240 / 480; loss 1.3724\n",
      "step  280 / 480; loss 1.3804\n",
      "step  320 / 480; loss 1.3599\n",
      "step  360 / 480; loss 1.3718\n",
      "step  400 / 480; loss 1.3615\n",
      "step  440 / 480; loss 1.3823\n",
      "Evaluation loss 1.3771141036280772, accuracy 0.26356875, acc CATHODE: 0.144, acc CURTAINS: 0.308, acc FETA: 0.496, acc SALAD: 0.107\n",
      "   - - - - -   \n",
      "Epoch 242 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3694\n",
      "step   80 / 480; loss 1.3677\n",
      "step  120 / 480; loss 1.3688\n",
      "step  160 / 480; loss 1.3823\n",
      "step  200 / 480; loss 1.3642\n",
      "step  240 / 480; loss 1.3798\n",
      "step  280 / 480; loss 1.3704\n",
      "step  320 / 480; loss 1.3729\n",
      "step  360 / 480; loss 1.3813\n",
      "step  400 / 480; loss 1.3740\n",
      "step  440 / 480; loss 1.3777\n",
      "Evaluation loss 1.377111485313757, accuracy 0.261965625, acc CATHODE: 0.227, acc CURTAINS: 0.371, acc FETA: 0.374, acc SALAD: 0.076\n",
      "   - - - - -   \n",
      "Epoch 243 / 400\n",
      "step    0 / 480; loss 1.3687\n",
      "step   40 / 480; loss 1.3776\n",
      "step   80 / 480; loss 1.3664\n",
      "step  120 / 480; loss 1.3775\n",
      "step  160 / 480; loss 1.3722\n",
      "step  200 / 480; loss 1.3728\n",
      "step  240 / 480; loss 1.3806\n",
      "step  280 / 480; loss 1.3778\n",
      "step  320 / 480; loss 1.3656\n",
      "step  360 / 480; loss 1.3859\n",
      "step  400 / 480; loss 1.3759\n",
      "step  440 / 480; loss 1.3688\n",
      "Evaluation loss 1.3768897350702944, accuracy 0.262875, acc CATHODE: 0.234, acc CURTAINS: 0.389, acc FETA: 0.346, acc SALAD: 0.083\n",
      "   - - - - -   \n",
      "Epoch 244 / 400\n",
      "step    0 / 480; loss 1.3704\n",
      "step   40 / 480; loss 1.3818\n",
      "step   80 / 480; loss 1.3672\n",
      "step  120 / 480; loss 1.3822\n",
      "step  160 / 480; loss 1.3654\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3731\n",
      "step  280 / 480; loss 1.3771\n",
      "step  320 / 480; loss 1.3752\n",
      "step  360 / 480; loss 1.3748\n",
      "step  400 / 480; loss 1.3692\n",
      "step  440 / 480; loss 1.3758\n",
      "Evaluation loss 1.3772475342400599, accuracy 0.261453125, acc CATHODE: 0.218, acc CURTAINS: 0.231, acc FETA: 0.476, acc SALAD: 0.120\n",
      "   - - - - -   \n",
      "Epoch 245 / 400\n",
      "step    0 / 480; loss 1.3686\n",
      "step   40 / 480; loss 1.3686\n",
      "step   80 / 480; loss 1.3719\n",
      "step  120 / 480; loss 1.3706\n",
      "step  160 / 480; loss 1.3711\n",
      "step  200 / 480; loss 1.3750\n",
      "step  240 / 480; loss 1.3735\n",
      "step  280 / 480; loss 1.3641\n",
      "step  320 / 480; loss 1.3806\n",
      "step  360 / 480; loss 1.3758\n",
      "step  400 / 480; loss 1.3850\n",
      "step  440 / 480; loss 1.3765\n",
      "Evaluation loss 1.3770572190225505, accuracy 0.262184375, acc CATHODE: 0.211, acc CURTAINS: 0.412, acc FETA: 0.333, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 246 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3832\n",
      "step   80 / 480; loss 1.3769\n",
      "step  120 / 480; loss 1.3719\n",
      "step  160 / 480; loss 1.3804\n",
      "step  200 / 480; loss 1.3788\n",
      "step  240 / 480; loss 1.3751\n",
      "step  280 / 480; loss 1.3759\n",
      "step  320 / 480; loss 1.3784\n",
      "step  360 / 480; loss 1.3762\n",
      "step  400 / 480; loss 1.3714\n",
      "step  440 / 480; loss 1.3639\n",
      "Evaluation loss 1.3770988088726006, accuracy 0.26336875, acc CATHODE: 0.218, acc CURTAINS: 0.393, acc FETA: 0.316, acc SALAD: 0.126\n",
      "   - - - - -   \n",
      "Epoch 247 / 400\n",
      "step    0 / 480; loss 1.3791\n",
      "step   40 / 480; loss 1.3826\n",
      "step   80 / 480; loss 1.3772\n",
      "step  120 / 480; loss 1.3720\n",
      "step  160 / 480; loss 1.3701\n",
      "step  200 / 480; loss 1.3763\n",
      "step  240 / 480; loss 1.3732\n",
      "step  280 / 480; loss 1.3750\n",
      "step  320 / 480; loss 1.3648\n",
      "step  360 / 480; loss 1.3908\n",
      "step  400 / 480; loss 1.3680\n",
      "step  440 / 480; loss 1.3849\n",
      "Evaluation loss 1.377364081585273, accuracy 0.262234375, acc CATHODE: 0.204, acc CURTAINS: 0.316, acc FETA: 0.418, acc SALAD: 0.112\n",
      "   - - - - -   \n",
      "Epoch 248 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3786\n",
      "step   80 / 480; loss 1.3796\n",
      "step  120 / 480; loss 1.3729\n",
      "step  160 / 480; loss 1.3751\n",
      "step  200 / 480; loss 1.3707\n",
      "step  240 / 480; loss 1.3649\n",
      "step  280 / 480; loss 1.3695\n",
      "step  320 / 480; loss 1.3756\n",
      "step  360 / 480; loss 1.3768\n",
      "step  400 / 480; loss 1.3701\n",
      "step  440 / 480; loss 1.3685\n",
      "Evaluation loss 1.377197732480107, accuracy 0.262684375, acc CATHODE: 0.230, acc CURTAINS: 0.345, acc FETA: 0.370, acc SALAD: 0.105\n",
      "   - - - - -   \n",
      "Epoch 249 / 400\n",
      "step    0 / 480; loss 1.3797\n",
      "step   40 / 480; loss 1.3774\n",
      "step   80 / 480; loss 1.3763\n",
      "step  120 / 480; loss 1.3750\n",
      "step  160 / 480; loss 1.3793\n",
      "step  200 / 480; loss 1.3664\n",
      "step  240 / 480; loss 1.3788\n",
      "step  280 / 480; loss 1.3664\n",
      "step  320 / 480; loss 1.3706\n",
      "step  360 / 480; loss 1.3766\n",
      "step  400 / 480; loss 1.3695\n",
      "step  440 / 480; loss 1.3717\n",
      "Evaluation loss 1.3771737006404035, accuracy 0.262184375, acc CATHODE: 0.224, acc CURTAINS: 0.370, acc FETA: 0.371, acc SALAD: 0.085\n",
      "   - - - - -   \n",
      "Epoch 250 / 400\n",
      "step    0 / 480; loss 1.3895\n",
      "step   40 / 480; loss 1.3673\n",
      "step   80 / 480; loss 1.3659\n",
      "step  120 / 480; loss 1.3689\n",
      "step  160 / 480; loss 1.3754\n",
      "step  200 / 480; loss 1.3851\n",
      "step  240 / 480; loss 1.3795\n",
      "step  280 / 480; loss 1.3799\n",
      "step  320 / 480; loss 1.3741\n",
      "step  360 / 480; loss 1.3666\n",
      "step  400 / 480; loss 1.3736\n",
      "step  440 / 480; loss 1.3792\n",
      "Evaluation loss 1.3770714502325554, accuracy 0.262065625, acc CATHODE: 0.201, acc CURTAINS: 0.319, acc FETA: 0.372, acc SALAD: 0.156\n",
      "   - - - - -   \n",
      "Epoch 251 / 400\n",
      "step    0 / 480; loss 1.3782\n",
      "step   40 / 480; loss 1.3736\n",
      "step   80 / 480; loss 1.3778\n",
      "step  120 / 480; loss 1.3661\n",
      "step  160 / 480; loss 1.3656\n",
      "step  200 / 480; loss 1.3749\n",
      "step  240 / 480; loss 1.3809\n",
      "step  280 / 480; loss 1.3761\n",
      "step  320 / 480; loss 1.3755\n",
      "step  360 / 480; loss 1.3738\n",
      "step  400 / 480; loss 1.3881\n",
      "step  440 / 480; loss 1.3779\n",
      "Evaluation loss 1.3770330864116551, accuracy 0.26360625, acc CATHODE: 0.232, acc CURTAINS: 0.368, acc FETA: 0.359, acc SALAD: 0.096\n",
      "   - - - - -   \n",
      "Epoch 252 / 400\n",
      "step    0 / 480; loss 1.3735\n",
      "step   40 / 480; loss 1.3710\n",
      "step   80 / 480; loss 1.3717\n",
      "step  120 / 480; loss 1.3750\n",
      "step  160 / 480; loss 1.3678\n",
      "step  200 / 480; loss 1.3761\n",
      "step  240 / 480; loss 1.3819\n",
      "step  280 / 480; loss 1.3715\n",
      "step  320 / 480; loss 1.3710\n",
      "step  360 / 480; loss 1.3700\n",
      "step  400 / 480; loss 1.3691\n",
      "step  440 / 480; loss 1.3744\n",
      "Evaluation loss 1.376920643834103, accuracy 0.262803125, acc CATHODE: 0.177, acc CURTAINS: 0.408, acc FETA: 0.371, acc SALAD: 0.095\n",
      "   - - - - -   \n",
      "Epoch 253 / 400\n",
      "step    0 / 480; loss 1.3807\n",
      "step   40 / 480; loss 1.3716\n",
      "step   80 / 480; loss 1.3732\n",
      "step  120 / 480; loss 1.3757\n",
      "step  160 / 480; loss 1.3736\n",
      "step  200 / 480; loss 1.3855\n",
      "step  240 / 480; loss 1.3712\n",
      "step  280 / 480; loss 1.3715\n",
      "step  320 / 480; loss 1.3806\n",
      "step  360 / 480; loss 1.3823\n",
      "step  400 / 480; loss 1.3814\n",
      "step  440 / 480; loss 1.3703\n",
      "Evaluation loss 1.3771345705178675, accuracy 0.26304375, acc CATHODE: 0.146, acc CURTAINS: 0.429, acc FETA: 0.328, acc SALAD: 0.149\n",
      "   - - - - -   \n",
      "Epoch 254 / 400\n",
      "step    0 / 480; loss 1.3793\n",
      "step   40 / 480; loss 1.3756\n",
      "step   80 / 480; loss 1.3708\n",
      "step  120 / 480; loss 1.3697\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3734\n",
      "step  240 / 480; loss 1.3787\n",
      "step  280 / 480; loss 1.3710\n",
      "step  320 / 480; loss 1.3711\n",
      "step  360 / 480; loss 1.3798\n",
      "step  400 / 480; loss 1.3743\n",
      "step  440 / 480; loss 1.3732\n",
      "Evaluation loss 1.377154297209568, accuracy 0.262328125, acc CATHODE: 0.297, acc CURTAINS: 0.323, acc FETA: 0.321, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 255 / 400\n",
      "step    0 / 480; loss 1.3664\n",
      "step   40 / 480; loss 1.3767\n",
      "step   80 / 480; loss 1.3781\n",
      "step  120 / 480; loss 1.3795\n",
      "step  160 / 480; loss 1.3769\n",
      "step  200 / 480; loss 1.3803\n",
      "step  240 / 480; loss 1.3727\n",
      "step  280 / 480; loss 1.3786\n",
      "step  320 / 480; loss 1.3812\n",
      "step  360 / 480; loss 1.3752\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3759\n",
      "Evaluation loss 1.3771145287050541, accuracy 0.262040625, acc CATHODE: 0.266, acc CURTAINS: 0.279, acc FETA: 0.335, acc SALAD: 0.168\n",
      "   - - - - -   \n",
      "Epoch 256 / 400\n",
      "step    0 / 480; loss 1.3852\n",
      "step   40 / 480; loss 1.3721\n",
      "step   80 / 480; loss 1.3692\n",
      "step  120 / 480; loss 1.3739\n",
      "step  160 / 480; loss 1.3765\n",
      "step  200 / 480; loss 1.3710\n",
      "step  240 / 480; loss 1.3715\n",
      "step  280 / 480; loss 1.3767\n",
      "step  320 / 480; loss 1.3835\n",
      "step  360 / 480; loss 1.3758\n",
      "step  400 / 480; loss 1.3801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  440 / 480; loss 1.3793\n",
      "Evaluation loss 1.3771430492205614, accuracy 0.262884375, acc CATHODE: 0.286, acc CURTAINS: 0.355, acc FETA: 0.318, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 257 / 400\n",
      "step    0 / 480; loss 1.3734\n",
      "step   40 / 480; loss 1.3822\n",
      "step   80 / 480; loss 1.3810\n",
      "step  120 / 480; loss 1.3707\n",
      "step  160 / 480; loss 1.3843\n",
      "step  200 / 480; loss 1.3750\n",
      "step  240 / 480; loss 1.3781\n",
      "step  280 / 480; loss 1.3802\n",
      "step  320 / 480; loss 1.3699\n",
      "step  360 / 480; loss 1.3866\n",
      "step  400 / 480; loss 1.3772\n",
      "step  440 / 480; loss 1.3674\n",
      "Evaluation loss 1.3770868054491074, accuracy 0.2624875, acc CATHODE: 0.274, acc CURTAINS: 0.350, acc FETA: 0.315, acc SALAD: 0.112\n",
      "   - - - - -   \n",
      "Epoch 258 / 400\n",
      "step    0 / 480; loss 1.3689\n",
      "step   40 / 480; loss 1.3819\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3709\n",
      "step  160 / 480; loss 1.3710\n",
      "step  200 / 480; loss 1.3653\n",
      "step  240 / 480; loss 1.3774\n",
      "step  280 / 480; loss 1.3766\n",
      "step  320 / 480; loss 1.3750\n",
      "step  360 / 480; loss 1.3796\n",
      "step  400 / 480; loss 1.3795\n",
      "step  440 / 480; loss 1.3706\n",
      "Evaluation loss 1.3771928835605318, accuracy 0.262290625, acc CATHODE: 0.201, acc CURTAINS: 0.316, acc FETA: 0.448, acc SALAD: 0.085\n",
      "   - - - - -   \n",
      "Epoch 259 / 400\n",
      "step    0 / 480; loss 1.3691\n",
      "step   40 / 480; loss 1.3737\n",
      "step   80 / 480; loss 1.3746\n",
      "step  120 / 480; loss 1.3683\n",
      "step  160 / 480; loss 1.3740\n",
      "step  200 / 480; loss 1.3654\n",
      "step  240 / 480; loss 1.3845\n",
      "step  280 / 480; loss 1.3648\n",
      "step  320 / 480; loss 1.3738\n",
      "step  360 / 480; loss 1.3654\n",
      "step  400 / 480; loss 1.3791\n",
      "step  440 / 480; loss 1.3766\n",
      "Evaluation loss 1.3770919722994073, accuracy 0.263090625, acc CATHODE: 0.192, acc CURTAINS: 0.373, acc FETA: 0.428, acc SALAD: 0.060\n",
      "   - - - - -   \n",
      "Epoch 260 / 400\n",
      "step    0 / 480; loss 1.3712\n",
      "step   40 / 480; loss 1.3747\n",
      "step   80 / 480; loss 1.3842\n",
      "step  120 / 480; loss 1.3706\n",
      "step  160 / 480; loss 1.3744\n",
      "step  200 / 480; loss 1.3805\n",
      "step  240 / 480; loss 1.3669\n",
      "step  280 / 480; loss 1.3699\n",
      "step  320 / 480; loss 1.3791\n",
      "step  360 / 480; loss 1.3723\n",
      "step  400 / 480; loss 1.3772\n",
      "step  440 / 480; loss 1.3794\n",
      "Evaluation loss 1.3774314138155175, accuracy 0.26225, acc CATHODE: 0.191, acc CURTAINS: 0.347, acc FETA: 0.421, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 261 / 400\n",
      "step    0 / 480; loss 1.3712\n",
      "step   40 / 480; loss 1.3818\n",
      "step   80 / 480; loss 1.3660\n",
      "step  120 / 480; loss 1.3863\n",
      "step  160 / 480; loss 1.3676\n",
      "step  200 / 480; loss 1.3683\n",
      "step  240 / 480; loss 1.3868\n",
      "step  280 / 480; loss 1.3710\n",
      "step  320 / 480; loss 1.3658\n",
      "step  360 / 480; loss 1.3873\n",
      "step  400 / 480; loss 1.3725\n",
      "step  440 / 480; loss 1.3728\n",
      "Evaluation loss 1.3774256439767538, accuracy 0.26255, acc CATHODE: 0.194, acc CURTAINS: 0.381, acc FETA: 0.402, acc SALAD: 0.073\n",
      "   - - - - -   \n",
      "Epoch 262 / 400\n",
      "step    0 / 480; loss 1.3654\n",
      "step   40 / 480; loss 1.3711\n",
      "step   80 / 480; loss 1.3746\n",
      "step  120 / 480; loss 1.3782\n",
      "step  160 / 480; loss 1.3711\n",
      "step  200 / 480; loss 1.3736\n",
      "step  240 / 480; loss 1.3735\n",
      "step  280 / 480; loss 1.3720\n",
      "step  320 / 480; loss 1.3703\n",
      "step  360 / 480; loss 1.3778\n",
      "step  400 / 480; loss 1.3772\n",
      "step  440 / 480; loss 1.3797\n",
      "Evaluation loss 1.377249997773657, accuracy 0.262825, acc CATHODE: 0.169, acc CURTAINS: 0.338, acc FETA: 0.421, acc SALAD: 0.123\n",
      "   - - - - -   \n",
      "Epoch 263 / 400\n",
      "step    0 / 480; loss 1.3717\n",
      "step   40 / 480; loss 1.3695\n",
      "step   80 / 480; loss 1.3740\n",
      "step  120 / 480; loss 1.3767\n",
      "step  160 / 480; loss 1.3737\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3844\n",
      "step  280 / 480; loss 1.3704\n",
      "step  320 / 480; loss 1.3709\n",
      "step  360 / 480; loss 1.3706\n",
      "step  400 / 480; loss 1.3702\n",
      "step  440 / 480; loss 1.3851\n",
      "Evaluation loss 1.3771260665591676, accuracy 0.26199375, acc CATHODE: 0.141, acc CURTAINS: 0.344, acc FETA: 0.495, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 264 / 400\n",
      "step    0 / 480; loss 1.3750\n",
      "step   40 / 480; loss 1.3785\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3766\n",
      "step  160 / 480; loss 1.3805\n",
      "step  200 / 480; loss 1.3773\n",
      "step  240 / 480; loss 1.3761\n",
      "step  280 / 480; loss 1.3632\n",
      "step  320 / 480; loss 1.3794\n",
      "step  360 / 480; loss 1.3651\n",
      "step  400 / 480; loss 1.3735\n",
      "step  440 / 480; loss 1.3695\n",
      "Evaluation loss 1.3770600612275545, accuracy 0.2627, acc CATHODE: 0.238, acc CURTAINS: 0.337, acc FETA: 0.379, acc SALAD: 0.097\n",
      "   - - - - -   \n",
      "Epoch 265 / 400\n",
      "step    0 / 480; loss 1.3748\n",
      "step   40 / 480; loss 1.3723\n",
      "step   80 / 480; loss 1.3777\n",
      "step  120 / 480; loss 1.3747\n",
      "step  160 / 480; loss 1.3775\n",
      "step  200 / 480; loss 1.3595\n",
      "step  240 / 480; loss 1.3723\n",
      "step  280 / 480; loss 1.3759\n",
      "step  320 / 480; loss 1.3688\n",
      "step  360 / 480; loss 1.3779\n",
      "step  400 / 480; loss 1.3862\n",
      "step  440 / 480; loss 1.3734\n",
      "Evaluation loss 1.3773304908649973, accuracy 0.26246875, acc CATHODE: 0.253, acc CURTAINS: 0.277, acc FETA: 0.394, acc SALAD: 0.126\n",
      "   - - - - -   \n",
      "Epoch 266 / 400\n",
      "step    0 / 480; loss 1.3721\n",
      "step   40 / 480; loss 1.3858\n",
      "step   80 / 480; loss 1.3727\n",
      "step  120 / 480; loss 1.3688\n",
      "step  160 / 480; loss 1.3777\n",
      "step  200 / 480; loss 1.3788\n",
      "step  240 / 480; loss 1.3778\n",
      "step  280 / 480; loss 1.3855\n",
      "step  320 / 480; loss 1.3668\n",
      "step  360 / 480; loss 1.3698\n",
      "step  400 / 480; loss 1.3744\n",
      "step  440 / 480; loss 1.3746\n",
      "Evaluation loss 1.3770444430346982, accuracy 0.26424375, acc CATHODE: 0.218, acc CURTAINS: 0.394, acc FETA: 0.342, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 267 / 400\n",
      "step    0 / 480; loss 1.3728\n",
      "step   40 / 480; loss 1.3817\n",
      "step   80 / 480; loss 1.3775\n",
      "step  120 / 480; loss 1.3859\n",
      "step  160 / 480; loss 1.3722\n",
      "step  200 / 480; loss 1.3728\n",
      "step  240 / 480; loss 1.3716\n",
      "step  280 / 480; loss 1.3728\n",
      "step  320 / 480; loss 1.3753\n",
      "step  360 / 480; loss 1.3829\n",
      "step  400 / 480; loss 1.3796\n",
      "step  440 / 480; loss 1.3727\n",
      "Evaluation loss 1.3772712785030194, accuracy 0.26215625, acc CATHODE: 0.244, acc CURTAINS: 0.290, acc FETA: 0.386, acc SALAD: 0.128\n",
      "   - - - - -   \n",
      "Epoch 268 / 400\n",
      "step    0 / 480; loss 1.3796\n",
      "step   40 / 480; loss 1.3861\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3760\n",
      "step  160 / 480; loss 1.3722\n",
      "step  200 / 480; loss 1.3877\n",
      "step  240 / 480; loss 1.3661\n",
      "step  280 / 480; loss 1.3810\n",
      "step  320 / 480; loss 1.3726\n",
      "step  360 / 480; loss 1.3793\n",
      "step  400 / 480; loss 1.3781\n",
      "step  440 / 480; loss 1.3720\n",
      "Evaluation loss 1.3772125282802232, accuracy 0.262875, acc CATHODE: 0.166, acc CURTAINS: 0.384, acc FETA: 0.440, acc SALAD: 0.062\n",
      "   - - - - -   \n",
      "Epoch 269 / 400\n",
      "step    0 / 480; loss 1.3745\n",
      "step   40 / 480; loss 1.3854\n",
      "step   80 / 480; loss 1.3649\n",
      "step  120 / 480; loss 1.3664\n",
      "step  160 / 480; loss 1.3729\n",
      "step  200 / 480; loss 1.3734\n",
      "step  240 / 480; loss 1.3834\n",
      "step  280 / 480; loss 1.3789\n",
      "step  320 / 480; loss 1.3659\n",
      "step  360 / 480; loss 1.3832\n",
      "step  400 / 480; loss 1.3709\n",
      "step  440 / 480; loss 1.3814\n",
      "Evaluation loss 1.3771954145953065, accuracy 0.26204375, acc CATHODE: 0.242, acc CURTAINS: 0.400, acc FETA: 0.314, acc SALAD: 0.093\n",
      "   - - - - -   \n",
      "Epoch 270 / 400\n",
      "step    0 / 480; loss 1.3729\n",
      "step   40 / 480; loss 1.3880\n",
      "step   80 / 480; loss 1.3724\n",
      "step  120 / 480; loss 1.3774\n",
      "step  160 / 480; loss 1.3814\n",
      "step  200 / 480; loss 1.3618\n",
      "step  240 / 480; loss 1.3659\n",
      "step  280 / 480; loss 1.3751\n",
      "step  320 / 480; loss 1.3766\n",
      "step  360 / 480; loss 1.3819\n",
      "step  400 / 480; loss 1.3794\n",
      "step  440 / 480; loss 1.3712\n",
      "Evaluation loss 1.3771635386305592, accuracy 0.262825, acc CATHODE: 0.304, acc CURTAINS: 0.322, acc FETA: 0.344, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 271 / 400\n",
      "step    0 / 480; loss 1.3628\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3745\n",
      "step  120 / 480; loss 1.3744\n",
      "step  160 / 480; loss 1.3786\n",
      "step  200 / 480; loss 1.3800\n",
      "step  240 / 480; loss 1.3768\n",
      "step  280 / 480; loss 1.3681\n",
      "step  320 / 480; loss 1.3754\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3669\n",
      "step  440 / 480; loss 1.3719\n",
      "Evaluation loss 1.3771762169677404, accuracy 0.262815625, acc CATHODE: 0.253, acc CURTAINS: 0.328, acc FETA: 0.354, acc SALAD: 0.117\n",
      "   - - - - -   \n",
      "Epoch 272 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3762\n",
      "step   80 / 480; loss 1.3764\n",
      "step  120 / 480; loss 1.3756\n",
      "step  160 / 480; loss 1.3890\n",
      "step  200 / 480; loss 1.3797\n",
      "step  240 / 480; loss 1.3790\n",
      "step  280 / 480; loss 1.3690\n",
      "step  320 / 480; loss 1.3595\n",
      "step  360 / 480; loss 1.3716\n",
      "step  400 / 480; loss 1.3772\n",
      "step  440 / 480; loss 1.3675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss 1.377159757584154, accuracy 0.262034375, acc CATHODE: 0.223, acc CURTAINS: 0.389, acc FETA: 0.346, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 273 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3855\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3783\n",
      "step  160 / 480; loss 1.3746\n",
      "step  200 / 480; loss 1.3745\n",
      "step  240 / 480; loss 1.3821\n",
      "step  280 / 480; loss 1.3700\n",
      "step  320 / 480; loss 1.3742\n",
      "step  360 / 480; loss 1.3770\n",
      "step  400 / 480; loss 1.3780\n",
      "step  440 / 480; loss 1.3661\n",
      "Evaluation loss 1.3771491791451957, accuracy 0.26224375, acc CATHODE: 0.290, acc CURTAINS: 0.297, acc FETA: 0.371, acc SALAD: 0.091\n",
      "   - - - - -   \n",
      "Epoch 274 / 400\n",
      "step    0 / 480; loss 1.3640\n",
      "step   40 / 480; loss 1.3840\n",
      "step   80 / 480; loss 1.3814\n",
      "step  120 / 480; loss 1.3778\n",
      "step  160 / 480; loss 1.3673\n",
      "step  200 / 480; loss 1.3758\n",
      "step  240 / 480; loss 1.3797\n",
      "step  280 / 480; loss 1.3759\n",
      "step  320 / 480; loss 1.3698\n",
      "step  360 / 480; loss 1.3702\n",
      "step  400 / 480; loss 1.3799\n",
      "step  440 / 480; loss 1.3781\n",
      "Evaluation loss 1.3771759320082306, accuracy 0.263053125, acc CATHODE: 0.168, acc CURTAINS: 0.366, acc FETA: 0.390, acc SALAD: 0.128\n",
      "   - - - - -   \n",
      "Epoch 275 / 400\n",
      "step    0 / 480; loss 1.3829\n",
      "step   40 / 480; loss 1.3701\n",
      "step   80 / 480; loss 1.3750\n",
      "step  120 / 480; loss 1.3752\n",
      "step  160 / 480; loss 1.3754\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3662\n",
      "step  280 / 480; loss 1.3619\n",
      "step  320 / 480; loss 1.3730\n",
      "step  360 / 480; loss 1.3677\n",
      "step  400 / 480; loss 1.3830\n",
      "step  440 / 480; loss 1.3721\n",
      "Evaluation loss 1.3773586461622351, accuracy 0.26288125, acc CATHODE: 0.209, acc CURTAINS: 0.319, acc FETA: 0.413, acc SALAD: 0.111\n",
      "   - - - - -   \n",
      "Epoch 276 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3738\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3766\n",
      "step  160 / 480; loss 1.3705\n",
      "step  200 / 480; loss 1.3750\n",
      "step  240 / 480; loss 1.3804\n",
      "step  280 / 480; loss 1.3773\n",
      "step  320 / 480; loss 1.3685\n",
      "step  360 / 480; loss 1.3727\n",
      "step  400 / 480; loss 1.3632\n",
      "step  440 / 480; loss 1.3659\n",
      "Evaluation loss 1.3771110738317907, accuracy 0.26354375, acc CATHODE: 0.174, acc CURTAINS: 0.338, acc FETA: 0.417, acc SALAD: 0.126\n",
      "   - - - - -   \n",
      "Epoch 277 / 400\n",
      "step    0 / 480; loss 1.3709\n",
      "step   40 / 480; loss 1.3842\n",
      "step   80 / 480; loss 1.3757\n",
      "step  120 / 480; loss 1.3719\n",
      "step  160 / 480; loss 1.3749\n",
      "step  200 / 480; loss 1.3738\n",
      "step  240 / 480; loss 1.3818\n",
      "step  280 / 480; loss 1.3749\n",
      "step  320 / 480; loss 1.3730\n",
      "step  360 / 480; loss 1.3830\n",
      "step  400 / 480; loss 1.3725\n",
      "step  440 / 480; loss 1.3731\n",
      "Evaluation loss 1.3771054464620984, accuracy 0.262940625, acc CATHODE: 0.240, acc CURTAINS: 0.347, acc FETA: 0.362, acc SALAD: 0.102\n",
      "   - - - - -   \n",
      "Epoch 278 / 400\n",
      "step    0 / 480; loss 1.3734\n",
      "step   40 / 480; loss 1.3720\n",
      "step   80 / 480; loss 1.3675\n",
      "step  120 / 480; loss 1.3788\n",
      "step  160 / 480; loss 1.3741\n",
      "step  200 / 480; loss 1.3706\n",
      "step  240 / 480; loss 1.3658\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3783\n",
      "step  360 / 480; loss 1.3684\n",
      "step  400 / 480; loss 1.3798\n",
      "step  440 / 480; loss 1.3660\n",
      "Evaluation loss 1.3773483860228035, accuracy 0.26231875, acc CATHODE: 0.207, acc CURTAINS: 0.433, acc FETA: 0.341, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 279 / 400\n",
      "step    0 / 480; loss 1.3739\n",
      "step   40 / 480; loss 1.3696\n",
      "step   80 / 480; loss 1.3789\n",
      "step  120 / 480; loss 1.3630\n",
      "step  160 / 480; loss 1.3800\n",
      "step  200 / 480; loss 1.3712\n",
      "step  240 / 480; loss 1.3730\n",
      "step  280 / 480; loss 1.3718\n",
      "step  320 / 480; loss 1.3695\n",
      "step  360 / 480; loss 1.3839\n",
      "step  400 / 480; loss 1.3787\n",
      "step  440 / 480; loss 1.3689\n",
      "Evaluation loss 1.3773196132465404, accuracy 0.26334375, acc CATHODE: 0.224, acc CURTAINS: 0.405, acc FETA: 0.347, acc SALAD: 0.077\n",
      "   - - - - -   \n",
      "Epoch 280 / 400\n",
      "step    0 / 480; loss 1.3809\n",
      "step   40 / 480; loss 1.3688\n",
      "step   80 / 480; loss 1.3726\n",
      "step  120 / 480; loss 1.3711\n",
      "step  160 / 480; loss 1.3799\n",
      "step  200 / 480; loss 1.3812\n",
      "step  240 / 480; loss 1.3796\n",
      "step  280 / 480; loss 1.3870\n",
      "step  320 / 480; loss 1.3696\n",
      "step  360 / 480; loss 1.3804\n",
      "step  400 / 480; loss 1.3685\n",
      "step  440 / 480; loss 1.3679\n",
      "Evaluation loss 1.3772342314242105, accuracy 0.261459375, acc CATHODE: 0.229, acc CURTAINS: 0.299, acc FETA: 0.451, acc SALAD: 0.067\n",
      "   - - - - -   \n",
      "Epoch 281 / 400\n",
      "step    0 / 480; loss 1.3796\n",
      "step   40 / 480; loss 1.3770\n",
      "step   80 / 480; loss 1.3821\n",
      "step  120 / 480; loss 1.3641\n",
      "step  160 / 480; loss 1.3757\n",
      "step  200 / 480; loss 1.3808\n",
      "step  240 / 480; loss 1.3851\n",
      "step  280 / 480; loss 1.3695\n",
      "step  320 / 480; loss 1.3740\n",
      "step  360 / 480; loss 1.3684\n",
      "step  400 / 480; loss 1.3751\n",
      "step  440 / 480; loss 1.3737\n",
      "Evaluation loss 1.377281709062262, accuracy 0.262775, acc CATHODE: 0.265, acc CURTAINS: 0.326, acc FETA: 0.368, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 282 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3630\n",
      "step   80 / 480; loss 1.3700\n",
      "step  120 / 480; loss 1.3816\n",
      "step  160 / 480; loss 1.3801\n",
      "step  200 / 480; loss 1.3796\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3739\n",
      "step  320 / 480; loss 1.3734\n",
      "step  360 / 480; loss 1.3919\n",
      "step  400 / 480; loss 1.3778\n",
      "step  440 / 480; loss 1.3800\n",
      "Evaluation loss 1.377281303558409, accuracy 0.26329375, acc CATHODE: 0.186, acc CURTAINS: 0.260, acc FETA: 0.489, acc SALAD: 0.118\n",
      "   - - - - -   \n",
      "Epoch 283 / 400\n",
      "step    0 / 480; loss 1.3664\n",
      "step   40 / 480; loss 1.3702\n",
      "step   80 / 480; loss 1.3761\n",
      "step  120 / 480; loss 1.3627\n",
      "step  160 / 480; loss 1.3721\n",
      "step  200 / 480; loss 1.3802\n",
      "step  240 / 480; loss 1.3733\n",
      "step  280 / 480; loss 1.3670\n",
      "step  320 / 480; loss 1.3731\n",
      "step  360 / 480; loss 1.3710\n",
      "step  400 / 480; loss 1.3682\n",
      "step  440 / 480; loss 1.3736\n",
      "Evaluation loss 1.3774172613348525, accuracy 0.26279375, acc CATHODE: 0.190, acc CURTAINS: 0.329, acc FETA: 0.441, acc SALAD: 0.091\n",
      "   - - - - -   \n",
      "Epoch 284 / 400\n",
      "step    0 / 480; loss 1.3722\n",
      "step   40 / 480; loss 1.3775\n",
      "step   80 / 480; loss 1.3736\n",
      "step  120 / 480; loss 1.3787\n",
      "step  160 / 480; loss 1.3696\n",
      "step  200 / 480; loss 1.3838\n",
      "step  240 / 480; loss 1.3720\n",
      "step  280 / 480; loss 1.3726\n",
      "step  320 / 480; loss 1.3722\n",
      "step  360 / 480; loss 1.3753\n",
      "step  400 / 480; loss 1.3757\n",
      "step  440 / 480; loss 1.3697\n",
      "Evaluation loss 1.3771291967448223, accuracy 0.2639125, acc CATHODE: 0.223, acc CURTAINS: 0.369, acc FETA: 0.340, acc SALAD: 0.123\n",
      "   - - - - -   \n",
      "Epoch 285 / 400\n",
      "step    0 / 480; loss 1.3631\n",
      "step   40 / 480; loss 1.3791\n",
      "step   80 / 480; loss 1.3665\n",
      "step  120 / 480; loss 1.3763\n",
      "step  160 / 480; loss 1.3753\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3731\n",
      "step  280 / 480; loss 1.3850\n",
      "step  320 / 480; loss 1.3718\n",
      "step  360 / 480; loss 1.3741\n",
      "step  400 / 480; loss 1.3687\n",
      "step  440 / 480; loss 1.3653\n",
      "Evaluation loss 1.3772664684336846, accuracy 0.262821875, acc CATHODE: 0.165, acc CURTAINS: 0.288, acc FETA: 0.490, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 286 / 400\n",
      "step    0 / 480; loss 1.3796\n",
      "step   40 / 480; loss 1.3659\n",
      "step   80 / 480; loss 1.3804\n",
      "step  120 / 480; loss 1.3772\n",
      "step  160 / 480; loss 1.3690\n",
      "step  200 / 480; loss 1.3737\n",
      "step  240 / 480; loss 1.3670\n",
      "step  280 / 480; loss 1.3835\n",
      "step  320 / 480; loss 1.3690\n",
      "step  360 / 480; loss 1.3696\n",
      "step  400 / 480; loss 1.3816\n",
      "step  440 / 480; loss 1.3663\n",
      "Evaluation loss 1.3771355613681526, accuracy 0.26185, acc CATHODE: 0.275, acc CURTAINS: 0.313, acc FETA: 0.385, acc SALAD: 0.075\n",
      "   - - - - -   \n",
      "Epoch 287 / 400\n",
      "step    0 / 480; loss 1.3722\n",
      "step   40 / 480; loss 1.3803\n",
      "step   80 / 480; loss 1.3714\n",
      "step  120 / 480; loss 1.3736\n",
      "step  160 / 480; loss 1.3767\n",
      "step  200 / 480; loss 1.3796\n",
      "step  240 / 480; loss 1.3712\n",
      "step  280 / 480; loss 1.3713\n",
      "step  320 / 480; loss 1.3779\n",
      "step  360 / 480; loss 1.3651\n",
      "step  400 / 480; loss 1.3876\n",
      "step  440 / 480; loss 1.3735\n",
      "Evaluation loss 1.3771901732319882, accuracy 0.26260625, acc CATHODE: 0.274, acc CURTAINS: 0.359, acc FETA: 0.314, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 288 / 400\n",
      "step    0 / 480; loss 1.3680\n",
      "step   40 / 480; loss 1.3740\n",
      "step   80 / 480; loss 1.3672\n",
      "step  120 / 480; loss 1.3827\n",
      "step  160 / 480; loss 1.3772\n",
      "step  200 / 480; loss 1.3764\n",
      "step  240 / 480; loss 1.3738\n",
      "step  280 / 480; loss 1.3629\n",
      "step  320 / 480; loss 1.3780\n",
      "step  360 / 480; loss 1.3688\n",
      "step  400 / 480; loss 1.3809\n",
      "step  440 / 480; loss 1.3776\n",
      "Evaluation loss 1.3774069716697706, accuracy 0.26279375, acc CATHODE: 0.178, acc CURTAINS: 0.395, acc FETA: 0.404, acc SALAD: 0.074\n",
      "   - - - - -   \n",
      "Epoch 289 / 400\n",
      "step    0 / 480; loss 1.3776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   40 / 480; loss 1.3724\n",
      "step   80 / 480; loss 1.3756\n",
      "step  120 / 480; loss 1.3701\n",
      "step  160 / 480; loss 1.3790\n",
      "step  200 / 480; loss 1.3810\n",
      "step  240 / 480; loss 1.3746\n",
      "step  280 / 480; loss 1.3823\n",
      "step  320 / 480; loss 1.3749\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3869\n",
      "step  440 / 480; loss 1.3760\n",
      "Evaluation loss 1.377324633072082, accuracy 0.262609375, acc CATHODE: 0.182, acc CURTAINS: 0.344, acc FETA: 0.390, acc SALAD: 0.135\n",
      "   - - - - -   \n",
      "Epoch 290 / 400\n",
      "step    0 / 480; loss 1.3656\n",
      "step   40 / 480; loss 1.3752\n",
      "step   80 / 480; loss 1.3825\n",
      "step  120 / 480; loss 1.3674\n",
      "step  160 / 480; loss 1.3786\n",
      "step  200 / 480; loss 1.3713\n",
      "step  240 / 480; loss 1.3658\n",
      "step  280 / 480; loss 1.3753\n",
      "step  320 / 480; loss 1.3845\n",
      "step  360 / 480; loss 1.3788\n",
      "step  400 / 480; loss 1.3644\n",
      "step  440 / 480; loss 1.3805\n",
      "Evaluation loss 1.3773265503315848, accuracy 0.262978125, acc CATHODE: 0.207, acc CURTAINS: 0.351, acc FETA: 0.432, acc SALAD: 0.062\n",
      "   - - - - -   \n",
      "Epoch 291 / 400\n",
      "step    0 / 480; loss 1.3739\n",
      "step   40 / 480; loss 1.3729\n",
      "step   80 / 480; loss 1.3680\n",
      "step  120 / 480; loss 1.3690\n",
      "step  160 / 480; loss 1.3745\n",
      "step  200 / 480; loss 1.3753\n",
      "step  240 / 480; loss 1.3737\n",
      "step  280 / 480; loss 1.3718\n",
      "step  320 / 480; loss 1.3725\n",
      "step  360 / 480; loss 1.3743\n",
      "step  400 / 480; loss 1.3699\n",
      "step  440 / 480; loss 1.3631\n",
      "Evaluation loss 1.3772145336368078, accuracy 0.26304375, acc CATHODE: 0.206, acc CURTAINS: 0.342, acc FETA: 0.377, acc SALAD: 0.127\n",
      "   - - - - -   \n",
      "Epoch 292 / 400\n",
      "step    0 / 480; loss 1.3779\n",
      "step   40 / 480; loss 1.3690\n",
      "step   80 / 480; loss 1.3657\n",
      "step  120 / 480; loss 1.3791\n",
      "step  160 / 480; loss 1.3733\n",
      "step  200 / 480; loss 1.3681\n",
      "step  240 / 480; loss 1.3709\n",
      "step  280 / 480; loss 1.3672\n",
      "step  320 / 480; loss 1.3747\n",
      "step  360 / 480; loss 1.3707\n",
      "step  400 / 480; loss 1.3801\n",
      "step  440 / 480; loss 1.3766\n",
      "Evaluation loss 1.3773167793149261, accuracy 0.263828125, acc CATHODE: 0.298, acc CURTAINS: 0.267, acc FETA: 0.413, acc SALAD: 0.077\n",
      "   - - - - -   \n",
      "Epoch 293 / 400\n",
      "step    0 / 480; loss 1.3704\n",
      "step   40 / 480; loss 1.3665\n",
      "step   80 / 480; loss 1.3806\n",
      "step  120 / 480; loss 1.3736\n",
      "step  160 / 480; loss 1.3668\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3752\n",
      "step  280 / 480; loss 1.3790\n",
      "step  320 / 480; loss 1.3731\n",
      "step  360 / 480; loss 1.3882\n",
      "step  400 / 480; loss 1.3730\n",
      "step  440 / 480; loss 1.3744\n",
      "Evaluation loss 1.3772482975062112, accuracy 0.26289375, acc CATHODE: 0.220, acc CURTAINS: 0.322, acc FETA: 0.420, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 294 / 400\n",
      "step    0 / 480; loss 1.3747\n",
      "step   40 / 480; loss 1.3808\n",
      "step   80 / 480; loss 1.3731\n",
      "step  120 / 480; loss 1.3844\n",
      "step  160 / 480; loss 1.3694\n",
      "step  200 / 480; loss 1.3828\n",
      "step  240 / 480; loss 1.3781\n",
      "step  280 / 480; loss 1.3792\n",
      "step  320 / 480; loss 1.3812\n",
      "step  360 / 480; loss 1.3748\n",
      "step  400 / 480; loss 1.3726\n",
      "step  440 / 480; loss 1.3770\n",
      "Evaluation loss 1.3771882604865993, accuracy 0.263553125, acc CATHODE: 0.197, acc CURTAINS: 0.377, acc FETA: 0.372, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 295 / 400\n",
      "step    0 / 480; loss 1.3684\n",
      "step   40 / 480; loss 1.3822\n",
      "step   80 / 480; loss 1.3802\n",
      "step  120 / 480; loss 1.3746\n",
      "step  160 / 480; loss 1.3743\n",
      "step  200 / 480; loss 1.3803\n",
      "step  240 / 480; loss 1.3693\n",
      "step  280 / 480; loss 1.3692\n",
      "step  320 / 480; loss 1.3801\n",
      "step  360 / 480; loss 1.3724\n",
      "step  400 / 480; loss 1.3752\n",
      "step  440 / 480; loss 1.3716\n",
      "Evaluation loss 1.3772400569386123, accuracy 0.263325, acc CATHODE: 0.205, acc CURTAINS: 0.385, acc FETA: 0.362, acc SALAD: 0.101\n",
      "   - - - - -   \n",
      "Epoch 296 / 400\n",
      "step    0 / 480; loss 1.3741\n",
      "step   40 / 480; loss 1.3678\n",
      "step   80 / 480; loss 1.3632\n",
      "step  120 / 480; loss 1.3824\n",
      "step  160 / 480; loss 1.3730\n",
      "step  200 / 480; loss 1.3718\n",
      "step  240 / 480; loss 1.3679\n",
      "step  280 / 480; loss 1.3779\n",
      "step  320 / 480; loss 1.3633\n",
      "step  360 / 480; loss 1.3722\n",
      "step  400 / 480; loss 1.3721\n",
      "step  440 / 480; loss 1.3702\n",
      "Evaluation loss 1.3774919561473153, accuracy 0.26340625, acc CATHODE: 0.239, acc CURTAINS: 0.391, acc FETA: 0.357, acc SALAD: 0.066\n",
      "   - - - - -   \n",
      "Epoch 297 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3649\n",
      "step   80 / 480; loss 1.3710\n",
      "step  120 / 480; loss 1.3803\n",
      "step  160 / 480; loss 1.3752\n",
      "step  200 / 480; loss 1.3761\n",
      "step  240 / 480; loss 1.3659\n",
      "step  280 / 480; loss 1.3668\n",
      "step  320 / 480; loss 1.3720\n",
      "step  360 / 480; loss 1.3755\n",
      "step  400 / 480; loss 1.3812\n",
      "step  440 / 480; loss 1.3901\n",
      "Evaluation loss 1.377178756193701, accuracy 0.262671875, acc CATHODE: 0.150, acc CURTAINS: 0.406, acc FETA: 0.397, acc SALAD: 0.098\n",
      "   - - - - -   \n",
      "Epoch 298 / 400\n",
      "step    0 / 480; loss 1.3749\n",
      "step   40 / 480; loss 1.3704\n",
      "step   80 / 480; loss 1.3793\n",
      "step  120 / 480; loss 1.3690\n",
      "step  160 / 480; loss 1.3744\n",
      "step  200 / 480; loss 1.3724\n",
      "step  240 / 480; loss 1.3785\n",
      "step  280 / 480; loss 1.3666\n",
      "step  320 / 480; loss 1.3792\n",
      "step  360 / 480; loss 1.3788\n",
      "step  400 / 480; loss 1.3820\n",
      "step  440 / 480; loss 1.3815\n",
      "Evaluation loss 1.377224300934714, accuracy 0.26138125, acc CATHODE: 0.236, acc CURTAINS: 0.352, acc FETA: 0.351, acc SALAD: 0.107\n",
      "   - - - - -   \n",
      "Epoch 299 / 400\n",
      "step    0 / 480; loss 1.3791\n",
      "step   40 / 480; loss 1.3723\n",
      "step   80 / 480; loss 1.3774\n",
      "step  120 / 480; loss 1.3717\n",
      "step  160 / 480; loss 1.3751\n",
      "step  200 / 480; loss 1.3788\n",
      "step  240 / 480; loss 1.3656\n",
      "step  280 / 480; loss 1.3700\n",
      "step  320 / 480; loss 1.3856\n",
      "step  360 / 480; loss 1.3635\n",
      "step  400 / 480; loss 1.3767\n",
      "step  440 / 480; loss 1.3725\n",
      "Evaluation loss 1.377444054691518, accuracy 0.26345625, acc CATHODE: 0.191, acc CURTAINS: 0.356, acc FETA: 0.386, acc SALAD: 0.120\n",
      "   - - - - -   \n",
      "Epoch 300 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3684\n",
      "step   80 / 480; loss 1.3745\n",
      "step  120 / 480; loss 1.3767\n",
      "step  160 / 480; loss 1.3676\n",
      "step  200 / 480; loss 1.3709\n",
      "step  240 / 480; loss 1.3767\n",
      "step  280 / 480; loss 1.3750\n",
      "step  320 / 480; loss 1.3689\n",
      "step  360 / 480; loss 1.3809\n",
      "step  400 / 480; loss 1.3677\n",
      "step  440 / 480; loss 1.3808\n",
      "Evaluation loss 1.3774918228375015, accuracy 0.261878125, acc CATHODE: 0.256, acc CURTAINS: 0.327, acc FETA: 0.323, acc SALAD: 0.141\n",
      "   - - - - -   \n",
      "Epoch 301 / 400\n",
      "step    0 / 480; loss 1.3727\n",
      "step   40 / 480; loss 1.3778\n",
      "step   80 / 480; loss 1.3862\n",
      "step  120 / 480; loss 1.3701\n",
      "step  160 / 480; loss 1.3732\n",
      "step  200 / 480; loss 1.3823\n",
      "step  240 / 480; loss 1.3764\n",
      "step  280 / 480; loss 1.3689\n",
      "step  320 / 480; loss 1.3779\n",
      "step  360 / 480; loss 1.3675\n",
      "step  400 / 480; loss 1.3739\n",
      "step  440 / 480; loss 1.3707\n",
      "Evaluation loss 1.3772148936109678, accuracy 0.26291875, acc CATHODE: 0.191, acc CURTAINS: 0.320, acc FETA: 0.441, acc SALAD: 0.099\n",
      "   - - - - -   \n",
      "Epoch 302 / 400\n",
      "step    0 / 480; loss 1.3710\n",
      "step   40 / 480; loss 1.3658\n",
      "step   80 / 480; loss 1.3760\n",
      "step  120 / 480; loss 1.3715\n",
      "step  160 / 480; loss 1.3748\n",
      "step  200 / 480; loss 1.3767\n",
      "step  240 / 480; loss 1.3691\n",
      "step  280 / 480; loss 1.3728\n",
      "step  320 / 480; loss 1.3832\n",
      "step  360 / 480; loss 1.3792\n",
      "step  400 / 480; loss 1.3811\n",
      "step  440 / 480; loss 1.3796\n",
      "Evaluation loss 1.377225374757892, accuracy 0.26265625, acc CATHODE: 0.247, acc CURTAINS: 0.369, acc FETA: 0.328, acc SALAD: 0.106\n",
      "   - - - - -   \n",
      "Epoch 303 / 400\n",
      "step    0 / 480; loss 1.3718\n",
      "step   40 / 480; loss 1.3750\n",
      "step   80 / 480; loss 1.3672\n",
      "step  120 / 480; loss 1.3765\n",
      "step  160 / 480; loss 1.3750\n",
      "step  200 / 480; loss 1.3755\n",
      "step  240 / 480; loss 1.3689\n",
      "step  280 / 480; loss 1.3718\n",
      "step  320 / 480; loss 1.3688\n",
      "step  360 / 480; loss 1.3659\n",
      "step  400 / 480; loss 1.3679\n",
      "step  440 / 480; loss 1.3797\n",
      "Evaluation loss 1.3771572389595956, accuracy 0.2634875, acc CATHODE: 0.224, acc CURTAINS: 0.359, acc FETA: 0.388, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 304 / 400\n",
      "step    0 / 480; loss 1.3760\n",
      "step   40 / 480; loss 1.3749\n",
      "step   80 / 480; loss 1.3669\n",
      "step  120 / 480; loss 1.3759\n",
      "step  160 / 480; loss 1.3716\n",
      "step  200 / 480; loss 1.3807\n",
      "step  240 / 480; loss 1.3821\n",
      "step  280 / 480; loss 1.3677\n",
      "step  320 / 480; loss 1.3668\n",
      "step  360 / 480; loss 1.3638\n",
      "step  400 / 480; loss 1.3749\n",
      "step  440 / 480; loss 1.3688\n",
      "Evaluation loss 1.377303364240813, accuracy 0.26118125, acc CATHODE: 0.235, acc CURTAINS: 0.374, acc FETA: 0.344, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 305 / 400\n",
      "step    0 / 480; loss 1.3725\n",
      "step   40 / 480; loss 1.3781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   80 / 480; loss 1.3782\n",
      "step  120 / 480; loss 1.3715\n",
      "step  160 / 480; loss 1.3771\n",
      "step  200 / 480; loss 1.3615\n",
      "step  240 / 480; loss 1.3829\n",
      "step  280 / 480; loss 1.3687\n",
      "step  320 / 480; loss 1.3659\n",
      "step  360 / 480; loss 1.3712\n",
      "step  400 / 480; loss 1.3785\n",
      "step  440 / 480; loss 1.3789\n",
      "Evaluation loss 1.3772594380556888, accuracy 0.26329375, acc CATHODE: 0.235, acc CURTAINS: 0.339, acc FETA: 0.381, acc SALAD: 0.099\n",
      "   - - - - -   \n",
      "Epoch 306 / 400\n",
      "step    0 / 480; loss 1.3761\n",
      "step   40 / 480; loss 1.3717\n",
      "step   80 / 480; loss 1.3747\n",
      "step  120 / 480; loss 1.3668\n",
      "step  160 / 480; loss 1.3742\n",
      "step  200 / 480; loss 1.3687\n",
      "step  240 / 480; loss 1.3801\n",
      "step  280 / 480; loss 1.3732\n",
      "step  320 / 480; loss 1.3841\n",
      "step  360 / 480; loss 1.3680\n",
      "step  400 / 480; loss 1.3748\n",
      "step  440 / 480; loss 1.3653\n",
      "Evaluation loss 1.37733975228154, accuracy 0.263725, acc CATHODE: 0.257, acc CURTAINS: 0.345, acc FETA: 0.322, acc SALAD: 0.131\n",
      "   - - - - -   \n",
      "Epoch 307 / 400\n",
      "step    0 / 480; loss 1.3731\n",
      "step   40 / 480; loss 1.3735\n",
      "step   80 / 480; loss 1.3796\n",
      "step  120 / 480; loss 1.3747\n",
      "step  160 / 480; loss 1.3642\n",
      "step  200 / 480; loss 1.3740\n",
      "step  240 / 480; loss 1.3685\n",
      "step  280 / 480; loss 1.3688\n",
      "step  320 / 480; loss 1.3706\n",
      "step  360 / 480; loss 1.3685\n",
      "step  400 / 480; loss 1.3752\n",
      "step  440 / 480; loss 1.3726\n",
      "Evaluation loss 1.3771712739378528, accuracy 0.263778125, acc CATHODE: 0.231, acc CURTAINS: 0.335, acc FETA: 0.402, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 308 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3679\n",
      "step   80 / 480; loss 1.3733\n",
      "step  120 / 480; loss 1.3614\n",
      "step  160 / 480; loss 1.3744\n",
      "step  200 / 480; loss 1.3760\n",
      "step  240 / 480; loss 1.3796\n",
      "step  280 / 480; loss 1.3843\n",
      "step  320 / 480; loss 1.3824\n",
      "step  360 / 480; loss 1.3735\n",
      "step  400 / 480; loss 1.3696\n",
      "step  440 / 480; loss 1.3605\n",
      "Evaluation loss 1.377383592064189, accuracy 0.26285625, acc CATHODE: 0.348, acc CURTAINS: 0.233, acc FETA: 0.328, acc SALAD: 0.143\n",
      "   - - - - -   \n",
      "Epoch 309 / 400\n",
      "step    0 / 480; loss 1.3782\n",
      "step   40 / 480; loss 1.3712\n",
      "step   80 / 480; loss 1.3723\n",
      "step  120 / 480; loss 1.3740\n",
      "step  160 / 480; loss 1.3711\n",
      "step  200 / 480; loss 1.3720\n",
      "step  240 / 480; loss 1.3747\n",
      "step  280 / 480; loss 1.3798\n",
      "step  320 / 480; loss 1.3772\n",
      "step  360 / 480; loss 1.3887\n",
      "step  400 / 480; loss 1.3768\n",
      "step  440 / 480; loss 1.3681\n",
      "Evaluation loss 1.3771878324443563, accuracy 0.262059375, acc CATHODE: 0.198, acc CURTAINS: 0.449, acc FETA: 0.334, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 310 / 400\n",
      "step    0 / 480; loss 1.3625\n",
      "step   40 / 480; loss 1.3719\n",
      "step   80 / 480; loss 1.3701\n",
      "step  120 / 480; loss 1.3707\n",
      "step  160 / 480; loss 1.3701\n",
      "step  200 / 480; loss 1.3776\n",
      "step  240 / 480; loss 1.3824\n",
      "step  280 / 480; loss 1.3793\n",
      "step  320 / 480; loss 1.3706\n",
      "step  360 / 480; loss 1.3638\n",
      "step  400 / 480; loss 1.3729\n",
      "step  440 / 480; loss 1.3693\n",
      "Evaluation loss 1.3772864949026036, accuracy 0.262784375, acc CATHODE: 0.287, acc CURTAINS: 0.322, acc FETA: 0.335, acc SALAD: 0.107\n",
      "   - - - - -   \n",
      "Epoch 311 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3741\n",
      "step   80 / 480; loss 1.3728\n",
      "step  120 / 480; loss 1.3774\n",
      "step  160 / 480; loss 1.3745\n",
      "step  200 / 480; loss 1.3712\n",
      "step  240 / 480; loss 1.3642\n",
      "step  280 / 480; loss 1.3827\n",
      "step  320 / 480; loss 1.3751\n",
      "step  360 / 480; loss 1.3750\n",
      "step  400 / 480; loss 1.3713\n",
      "step  440 / 480; loss 1.3729\n",
      "Evaluation loss 1.3772071326976882, accuracy 0.26285, acc CATHODE: 0.345, acc CURTAINS: 0.310, acc FETA: 0.302, acc SALAD: 0.095\n",
      "   - - - - -   \n",
      "Epoch 312 / 400\n",
      "step    0 / 480; loss 1.3760\n",
      "step   40 / 480; loss 1.3850\n",
      "step   80 / 480; loss 1.3762\n",
      "step  120 / 480; loss 1.3705\n",
      "step  160 / 480; loss 1.3799\n",
      "step  200 / 480; loss 1.3833\n",
      "step  240 / 480; loss 1.3632\n",
      "step  280 / 480; loss 1.3821\n",
      "step  320 / 480; loss 1.3755\n",
      "step  360 / 480; loss 1.3726\n",
      "step  400 / 480; loss 1.3809\n",
      "step  440 / 480; loss 1.3826\n",
      "Evaluation loss 1.3772393583198688, accuracy 0.2619, acc CATHODE: 0.229, acc CURTAINS: 0.387, acc FETA: 0.314, acc SALAD: 0.117\n",
      "   - - - - -   \n",
      "Epoch 313 / 400\n",
      "step    0 / 480; loss 1.3828\n",
      "step   40 / 480; loss 1.3750\n",
      "step   80 / 480; loss 1.3797\n",
      "step  120 / 480; loss 1.3646\n",
      "step  160 / 480; loss 1.3712\n",
      "step  200 / 480; loss 1.3819\n",
      "step  240 / 480; loss 1.3696\n",
      "step  280 / 480; loss 1.3722\n",
      "step  320 / 480; loss 1.3690\n",
      "step  360 / 480; loss 1.3747\n",
      "step  400 / 480; loss 1.3809\n",
      "step  440 / 480; loss 1.3673\n",
      "Evaluation loss 1.3773450244681025, accuracy 0.262790625, acc CATHODE: 0.159, acc CURTAINS: 0.349, acc FETA: 0.448, acc SALAD: 0.095\n",
      "   - - - - -   \n",
      "Epoch 314 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3764\n",
      "step   80 / 480; loss 1.3734\n",
      "step  120 / 480; loss 1.3674\n",
      "step  160 / 480; loss 1.3778\n",
      "step  200 / 480; loss 1.3788\n",
      "step  240 / 480; loss 1.3772\n",
      "step  280 / 480; loss 1.3806\n",
      "step  320 / 480; loss 1.3688\n",
      "step  360 / 480; loss 1.3661\n",
      "step  400 / 480; loss 1.3796\n",
      "step  440 / 480; loss 1.3762\n",
      "Evaluation loss 1.3772245558454246, accuracy 0.26245, acc CATHODE: 0.236, acc CURTAINS: 0.331, acc FETA: 0.351, acc SALAD: 0.132\n",
      "   - - - - -   \n",
      "Epoch 315 / 400\n",
      "step    0 / 480; loss 1.3741\n",
      "step   40 / 480; loss 1.3704\n",
      "step   80 / 480; loss 1.3768\n",
      "step  120 / 480; loss 1.3746\n",
      "step  160 / 480; loss 1.3829\n",
      "step  200 / 480; loss 1.3641\n",
      "step  240 / 480; loss 1.3757\n",
      "step  280 / 480; loss 1.3759\n",
      "step  320 / 480; loss 1.3694\n",
      "step  360 / 480; loss 1.3796\n",
      "step  400 / 480; loss 1.3835\n",
      "step  440 / 480; loss 1.3809\n",
      "Evaluation loss 1.3770896778289148, accuracy 0.2618375, acc CATHODE: 0.264, acc CURTAINS: 0.316, acc FETA: 0.378, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 316 / 400\n",
      "step    0 / 480; loss 1.3788\n",
      "step   40 / 480; loss 1.3716\n",
      "step   80 / 480; loss 1.3716\n",
      "step  120 / 480; loss 1.3776\n",
      "step  160 / 480; loss 1.3678\n",
      "step  200 / 480; loss 1.3754\n",
      "step  240 / 480; loss 1.3666\n",
      "step  280 / 480; loss 1.3800\n",
      "step  320 / 480; loss 1.3747\n",
      "step  360 / 480; loss 1.3752\n",
      "step  400 / 480; loss 1.3651\n",
      "step  440 / 480; loss 1.3794\n",
      "Evaluation loss 1.3772684548363332, accuracy 0.26351875, acc CATHODE: 0.201, acc CURTAINS: 0.355, acc FETA: 0.411, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 317 / 400\n",
      "step    0 / 480; loss 1.3717\n",
      "step   40 / 480; loss 1.3785\n",
      "step   80 / 480; loss 1.3732\n",
      "step  120 / 480; loss 1.3725\n",
      "step  160 / 480; loss 1.3770\n",
      "step  200 / 480; loss 1.3771\n",
      "step  240 / 480; loss 1.3761\n",
      "step  280 / 480; loss 1.3818\n",
      "step  320 / 480; loss 1.3791\n",
      "step  360 / 480; loss 1.3680\n",
      "step  400 / 480; loss 1.3737\n",
      "step  440 / 480; loss 1.3736\n",
      "Evaluation loss 1.377256015739975, accuracy 0.263309375, acc CATHODE: 0.214, acc CURTAINS: 0.340, acc FETA: 0.397, acc SALAD: 0.102\n",
      "   - - - - -   \n",
      "Epoch 318 / 400\n",
      "step    0 / 480; loss 1.3706\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3734\n",
      "step  120 / 480; loss 1.3716\n",
      "step  160 / 480; loss 1.3729\n",
      "step  200 / 480; loss 1.3677\n",
      "step  240 / 480; loss 1.3684\n",
      "step  280 / 480; loss 1.3771\n",
      "step  320 / 480; loss 1.3767\n",
      "step  360 / 480; loss 1.3690\n",
      "step  400 / 480; loss 1.3702\n",
      "step  440 / 480; loss 1.3815\n",
      "Evaluation loss 1.3773096517731716, accuracy 0.26256875, acc CATHODE: 0.275, acc CURTAINS: 0.362, acc FETA: 0.324, acc SALAD: 0.088\n",
      "   - - - - -   \n",
      "Epoch 319 / 400\n",
      "step    0 / 480; loss 1.3731\n",
      "step   40 / 480; loss 1.3652\n",
      "step   80 / 480; loss 1.3748\n",
      "step  120 / 480; loss 1.3734\n",
      "step  160 / 480; loss 1.3785\n",
      "step  200 / 480; loss 1.3788\n",
      "step  240 / 480; loss 1.3768\n",
      "step  280 / 480; loss 1.3759\n",
      "step  320 / 480; loss 1.3855\n",
      "step  360 / 480; loss 1.3747\n",
      "step  400 / 480; loss 1.3786\n",
      "step  440 / 480; loss 1.3821\n",
      "Evaluation loss 1.3772289502760406, accuracy 0.263528125, acc CATHODE: 0.202, acc CURTAINS: 0.344, acc FETA: 0.389, acc SALAD: 0.118\n",
      "   - - - - -   \n",
      "Epoch 320 / 400\n",
      "step    0 / 480; loss 1.3733\n",
      "step   40 / 480; loss 1.3798\n",
      "step   80 / 480; loss 1.3711\n",
      "step  120 / 480; loss 1.3795\n",
      "step  160 / 480; loss 1.3859\n",
      "step  200 / 480; loss 1.3730\n",
      "step  240 / 480; loss 1.3766\n",
      "step  280 / 480; loss 1.3651\n",
      "step  320 / 480; loss 1.3747\n",
      "step  360 / 480; loss 1.3714\n",
      "step  400 / 480; loss 1.3732\n",
      "step  440 / 480; loss 1.3776\n",
      "Evaluation loss 1.377185353885611, accuracy 0.26250625, acc CATHODE: 0.243, acc CURTAINS: 0.440, acc FETA: 0.287, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 321 / 400\n",
      "step    0 / 480; loss 1.3684\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  120 / 480; loss 1.3869\n",
      "step  160 / 480; loss 1.3739\n",
      "step  200 / 480; loss 1.3692\n",
      "step  240 / 480; loss 1.3735\n",
      "step  280 / 480; loss 1.3665\n",
      "step  320 / 480; loss 1.3799\n",
      "step  360 / 480; loss 1.3846\n",
      "step  400 / 480; loss 1.3698\n",
      "step  440 / 480; loss 1.3693\n",
      "Evaluation loss 1.3773413274342954, accuracy 0.263146875, acc CATHODE: 0.258, acc CURTAINS: 0.332, acc FETA: 0.370, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 322 / 400\n",
      "step    0 / 480; loss 1.3723\n",
      "step   40 / 480; loss 1.3850\n",
      "step   80 / 480; loss 1.3789\n",
      "step  120 / 480; loss 1.3738\n",
      "step  160 / 480; loss 1.3796\n",
      "step  200 / 480; loss 1.3831\n",
      "step  240 / 480; loss 1.3852\n",
      "step  280 / 480; loss 1.3767\n",
      "step  320 / 480; loss 1.3796\n",
      "step  360 / 480; loss 1.3683\n",
      "step  400 / 480; loss 1.3865\n",
      "step  440 / 480; loss 1.3817\n",
      "Evaluation loss 1.3771655026002338, accuracy 0.263775, acc CATHODE: 0.191, acc CURTAINS: 0.450, acc FETA: 0.313, acc SALAD: 0.102\n",
      "   - - - - -   \n",
      "Epoch 323 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3869\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3742\n",
      "step  160 / 480; loss 1.3717\n",
      "step  200 / 480; loss 1.3801\n",
      "step  240 / 480; loss 1.3644\n",
      "step  280 / 480; loss 1.3688\n",
      "step  320 / 480; loss 1.3657\n",
      "step  360 / 480; loss 1.3871\n",
      "step  400 / 480; loss 1.3725\n",
      "step  440 / 480; loss 1.3713\n",
      "Evaluation loss 1.3772874062787115, accuracy 0.26239375, acc CATHODE: 0.228, acc CURTAINS: 0.309, acc FETA: 0.404, acc SALAD: 0.109\n",
      "   - - - - -   \n",
      "Epoch 324 / 400\n",
      "step    0 / 480; loss 1.3720\n",
      "step   40 / 480; loss 1.3751\n",
      "step   80 / 480; loss 1.3785\n",
      "step  120 / 480; loss 1.3753\n",
      "step  160 / 480; loss 1.3774\n",
      "step  200 / 480; loss 1.3755\n",
      "step  240 / 480; loss 1.3727\n",
      "step  280 / 480; loss 1.3700\n",
      "step  320 / 480; loss 1.3781\n",
      "step  360 / 480; loss 1.3813\n",
      "step  400 / 480; loss 1.3745\n",
      "step  440 / 480; loss 1.3584\n",
      "Evaluation loss 1.377245822422089, accuracy 0.26404375, acc CATHODE: 0.215, acc CURTAINS: 0.306, acc FETA: 0.446, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 325 / 400\n",
      "step    0 / 480; loss 1.3720\n",
      "step   40 / 480; loss 1.3806\n",
      "step   80 / 480; loss 1.3675\n",
      "step  120 / 480; loss 1.3704\n",
      "step  160 / 480; loss 1.3695\n",
      "step  200 / 480; loss 1.3806\n",
      "step  240 / 480; loss 1.3757\n",
      "step  280 / 480; loss 1.3736\n",
      "step  320 / 480; loss 1.3618\n",
      "step  360 / 480; loss 1.3719\n",
      "step  400 / 480; loss 1.3854\n",
      "step  440 / 480; loss 1.3817\n",
      "Evaluation loss 1.3773118664192077, accuracy 0.262046875, acc CATHODE: 0.200, acc CURTAINS: 0.384, acc FETA: 0.341, acc SALAD: 0.124\n",
      "   - - - - -   \n",
      "Epoch 326 / 400\n",
      "step    0 / 480; loss 1.3637\n",
      "step   40 / 480; loss 1.3823\n",
      "step   80 / 480; loss 1.3703\n",
      "step  120 / 480; loss 1.3808\n",
      "step  160 / 480; loss 1.3818\n",
      "step  200 / 480; loss 1.3753\n",
      "step  240 / 480; loss 1.3722\n",
      "step  280 / 480; loss 1.3726\n",
      "step  320 / 480; loss 1.3722\n",
      "step  360 / 480; loss 1.3731\n",
      "step  400 / 480; loss 1.3843\n",
      "step  440 / 480; loss 1.3716\n",
      "Evaluation loss 1.377347961423344, accuracy 0.263940625, acc CATHODE: 0.211, acc CURTAINS: 0.371, acc FETA: 0.390, acc SALAD: 0.084\n",
      "   - - - - -   \n",
      "Epoch 327 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3783\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3685\n",
      "step  160 / 480; loss 1.3764\n",
      "step  200 / 480; loss 1.3765\n",
      "step  240 / 480; loss 1.3731\n",
      "step  280 / 480; loss 1.3734\n",
      "step  320 / 480; loss 1.3775\n",
      "step  360 / 480; loss 1.3752\n",
      "step  400 / 480; loss 1.3820\n",
      "step  440 / 480; loss 1.3653\n",
      "Evaluation loss 1.3773298050009453, accuracy 0.263509375, acc CATHODE: 0.165, acc CURTAINS: 0.364, acc FETA: 0.402, acc SALAD: 0.123\n",
      "   - - - - -   \n",
      "Epoch 328 / 400\n",
      "step    0 / 480; loss 1.3681\n",
      "step   40 / 480; loss 1.3731\n",
      "step   80 / 480; loss 1.3719\n",
      "step  120 / 480; loss 1.3661\n",
      "step  160 / 480; loss 1.3751\n",
      "step  200 / 480; loss 1.3761\n",
      "step  240 / 480; loss 1.3740\n",
      "step  280 / 480; loss 1.3891\n",
      "step  320 / 480; loss 1.3715\n",
      "step  360 / 480; loss 1.3675\n",
      "step  400 / 480; loss 1.3709\n",
      "step  440 / 480; loss 1.3732\n",
      "Evaluation loss 1.3771789586469145, accuracy 0.262725, acc CATHODE: 0.213, acc CURTAINS: 0.365, acc FETA: 0.387, acc SALAD: 0.086\n",
      "   - - - - -   \n",
      "Epoch 329 / 400\n",
      "step    0 / 480; loss 1.3744\n",
      "step   40 / 480; loss 1.3602\n",
      "step   80 / 480; loss 1.3741\n",
      "step  120 / 480; loss 1.3763\n",
      "step  160 / 480; loss 1.3751\n",
      "step  200 / 480; loss 1.3783\n",
      "step  240 / 480; loss 1.3730\n",
      "step  280 / 480; loss 1.3732\n",
      "step  320 / 480; loss 1.3793\n",
      "step  360 / 480; loss 1.3827\n",
      "step  400 / 480; loss 1.3634\n",
      "step  440 / 480; loss 1.3668\n",
      "Evaluation loss 1.3772623396621317, accuracy 0.263565625, acc CATHODE: 0.262, acc CURTAINS: 0.379, acc FETA: 0.309, acc SALAD: 0.104\n",
      "   - - - - -   \n",
      "Epoch 330 / 400\n",
      "step    0 / 480; loss 1.3728\n",
      "step   40 / 480; loss 1.3768\n",
      "step   80 / 480; loss 1.3749\n",
      "step  120 / 480; loss 1.3695\n",
      "step  160 / 480; loss 1.3741\n",
      "step  200 / 480; loss 1.3681\n",
      "step  240 / 480; loss 1.3729\n",
      "step  280 / 480; loss 1.3763\n",
      "step  320 / 480; loss 1.3714\n",
      "step  360 / 480; loss 1.3821\n",
      "step  400 / 480; loss 1.3791\n",
      "step  440 / 480; loss 1.3805\n",
      "Evaluation loss 1.3772372442724519, accuracy 0.262875, acc CATHODE: 0.212, acc CURTAINS: 0.348, acc FETA: 0.375, acc SALAD: 0.117\n",
      "   - - - - -   \n",
      "Epoch 331 / 400\n",
      "step    0 / 480; loss 1.3740\n",
      "step   40 / 480; loss 1.3783\n",
      "step   80 / 480; loss 1.3766\n",
      "step  120 / 480; loss 1.3672\n",
      "step  160 / 480; loss 1.3713\n",
      "step  200 / 480; loss 1.3788\n",
      "step  240 / 480; loss 1.3749\n",
      "step  280 / 480; loss 1.3709\n",
      "step  320 / 480; loss 1.3704\n",
      "step  360 / 480; loss 1.3729\n",
      "step  400 / 480; loss 1.3873\n",
      "step  440 / 480; loss 1.3812\n",
      "Evaluation loss 1.3771563719587043, accuracy 0.263053125, acc CATHODE: 0.218, acc CURTAINS: 0.348, acc FETA: 0.365, acc SALAD: 0.122\n",
      "   - - - - -   \n",
      "Epoch 332 / 400\n",
      "step    0 / 480; loss 1.3654\n",
      "step   40 / 480; loss 1.3720\n",
      "step   80 / 480; loss 1.3710\n",
      "step  120 / 480; loss 1.3836\n",
      "step  160 / 480; loss 1.3724\n",
      "step  200 / 480; loss 1.3722\n",
      "step  240 / 480; loss 1.3805\n",
      "step  280 / 480; loss 1.3730\n",
      "step  320 / 480; loss 1.3810\n",
      "step  360 / 480; loss 1.3846\n",
      "step  400 / 480; loss 1.3778\n",
      "step  440 / 480; loss 1.3866\n",
      "Evaluation loss 1.3772636859762954, accuracy 0.2639625, acc CATHODE: 0.221, acc CURTAINS: 0.313, acc FETA: 0.372, acc SALAD: 0.149\n",
      "   - - - - -   \n",
      "Epoch 333 / 400\n",
      "step    0 / 480; loss 1.3806\n",
      "step   40 / 480; loss 1.3809\n",
      "step   80 / 480; loss 1.3803\n",
      "step  120 / 480; loss 1.3647\n",
      "step  160 / 480; loss 1.3708\n",
      "step  200 / 480; loss 1.3815\n",
      "step  240 / 480; loss 1.3808\n",
      "step  280 / 480; loss 1.3782\n",
      "step  320 / 480; loss 1.3749\n",
      "step  360 / 480; loss 1.3678\n",
      "step  400 / 480; loss 1.3822\n",
      "step  440 / 480; loss 1.3752\n",
      "Evaluation loss 1.377266125396626, accuracy 0.26393125, acc CATHODE: 0.228, acc CURTAINS: 0.319, acc FETA: 0.396, acc SALAD: 0.113\n",
      "   - - - - -   \n",
      "Epoch 334 / 400\n",
      "step    0 / 480; loss 1.3734\n",
      "step   40 / 480; loss 1.3643\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3728\n",
      "step  160 / 480; loss 1.3763\n",
      "step  200 / 480; loss 1.3782\n",
      "step  240 / 480; loss 1.3814\n",
      "step  280 / 480; loss 1.3676\n",
      "step  320 / 480; loss 1.3710\n",
      "step  360 / 480; loss 1.3801\n",
      "step  400 / 480; loss 1.3713\n",
      "step  440 / 480; loss 1.3873\n",
      "Evaluation loss 1.377222777725152, accuracy 0.262453125, acc CATHODE: 0.190, acc CURTAINS: 0.284, acc FETA: 0.439, acc SALAD: 0.137\n",
      "   - - - - -   \n",
      "Epoch 335 / 400\n",
      "step    0 / 480; loss 1.3748\n",
      "step   40 / 480; loss 1.3718\n",
      "step   80 / 480; loss 1.3729\n",
      "step  120 / 480; loss 1.3745\n",
      "step  160 / 480; loss 1.3744\n",
      "step  200 / 480; loss 1.3844\n",
      "step  240 / 480; loss 1.3791\n",
      "step  280 / 480; loss 1.3829\n",
      "step  320 / 480; loss 1.3728\n",
      "step  360 / 480; loss 1.3706\n",
      "step  400 / 480; loss 1.3745\n",
      "step  440 / 480; loss 1.3817\n",
      "Evaluation loss 1.3774810209076347, accuracy 0.263853125, acc CATHODE: 0.221, acc CURTAINS: 0.345, acc FETA: 0.388, acc SALAD: 0.102\n",
      "   - - - - -   \n",
      "Epoch 336 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3772\n",
      "step   80 / 480; loss 1.3671\n",
      "step  120 / 480; loss 1.3719\n",
      "step  160 / 480; loss 1.3667\n",
      "step  200 / 480; loss 1.3724\n",
      "step  240 / 480; loss 1.3713\n",
      "step  280 / 480; loss 1.3847\n",
      "step  320 / 480; loss 1.3745\n",
      "step  360 / 480; loss 1.3716\n",
      "step  400 / 480; loss 1.3790\n",
      "step  440 / 480; loss 1.3750\n",
      "Evaluation loss 1.3773496540113563, accuracy 0.262615625, acc CATHODE: 0.177, acc CURTAINS: 0.396, acc FETA: 0.364, acc SALAD: 0.113\n",
      "   - - - - -   \n",
      "Epoch 337 / 400\n",
      "step    0 / 480; loss 1.3738\n",
      "step   40 / 480; loss 1.3728\n",
      "step   80 / 480; loss 1.3660\n",
      "step  120 / 480; loss 1.3678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  160 / 480; loss 1.3679\n",
      "step  200 / 480; loss 1.3813\n",
      "step  240 / 480; loss 1.3845\n",
      "step  280 / 480; loss 1.3756\n",
      "step  320 / 480; loss 1.3771\n",
      "step  360 / 480; loss 1.3656\n",
      "step  400 / 480; loss 1.3715\n",
      "step  440 / 480; loss 1.3688\n",
      "Evaluation loss 1.377394619236194, accuracy 0.264346875, acc CATHODE: 0.211, acc CURTAINS: 0.380, acc FETA: 0.364, acc SALAD: 0.103\n",
      "   - - - - -   \n",
      "Epoch 338 / 400\n",
      "step    0 / 480; loss 1.3722\n",
      "step   40 / 480; loss 1.3787\n",
      "step   80 / 480; loss 1.3802\n",
      "step  120 / 480; loss 1.3744\n",
      "step  160 / 480; loss 1.3764\n",
      "step  200 / 480; loss 1.3618\n",
      "step  240 / 480; loss 1.3756\n",
      "step  280 / 480; loss 1.3760\n",
      "step  320 / 480; loss 1.3665\n",
      "step  360 / 480; loss 1.3719\n",
      "step  400 / 480; loss 1.3735\n",
      "step  440 / 480; loss 1.3660\n",
      "Evaluation loss 1.3773136700408046, accuracy 0.262803125, acc CATHODE: 0.194, acc CURTAINS: 0.350, acc FETA: 0.427, acc SALAD: 0.080\n",
      "   - - - - -   \n",
      "Epoch 339 / 400\n",
      "step    0 / 480; loss 1.3742\n",
      "step   40 / 480; loss 1.3646\n",
      "step   80 / 480; loss 1.3735\n",
      "step  120 / 480; loss 1.3803\n",
      "step  160 / 480; loss 1.3827\n",
      "step  200 / 480; loss 1.3671\n",
      "step  240 / 480; loss 1.3673\n",
      "step  280 / 480; loss 1.3722\n",
      "step  320 / 480; loss 1.3863\n",
      "step  360 / 480; loss 1.3680\n",
      "step  400 / 480; loss 1.3691\n",
      "step  440 / 480; loss 1.3778\n",
      "Evaluation loss 1.3773221317329318, accuracy 0.26333125, acc CATHODE: 0.227, acc CURTAINS: 0.354, acc FETA: 0.389, acc SALAD: 0.083\n",
      "   - - - - -   \n",
      "Epoch 340 / 400\n",
      "step    0 / 480; loss 1.3773\n",
      "step   40 / 480; loss 1.3801\n",
      "step   80 / 480; loss 1.3771\n",
      "step  120 / 480; loss 1.3728\n",
      "step  160 / 480; loss 1.3754\n",
      "step  200 / 480; loss 1.3738\n",
      "step  240 / 480; loss 1.3692\n",
      "step  280 / 480; loss 1.3731\n",
      "step  320 / 480; loss 1.3746\n",
      "step  360 / 480; loss 1.3742\n",
      "step  400 / 480; loss 1.3756\n",
      "step  440 / 480; loss 1.3694\n",
      "Evaluation loss 1.377312050931969, accuracy 0.26381875, acc CATHODE: 0.196, acc CURTAINS: 0.316, acc FETA: 0.424, acc SALAD: 0.118\n",
      "   - - - - -   \n",
      "Epoch 341 / 400\n",
      "step    0 / 480; loss 1.3712\n",
      "step   40 / 480; loss 1.3805\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3716\n",
      "step  160 / 480; loss 1.3738\n",
      "step  200 / 480; loss 1.3659\n",
      "step  240 / 480; loss 1.3713\n",
      "step  280 / 480; loss 1.3815\n",
      "step  320 / 480; loss 1.3750\n",
      "step  360 / 480; loss 1.3754\n",
      "step  400 / 480; loss 1.3831\n",
      "step  440 / 480; loss 1.3746\n",
      "Evaluation loss 1.3772879557482387, accuracy 0.263621875, acc CATHODE: 0.229, acc CURTAINS: 0.338, acc FETA: 0.397, acc SALAD: 0.090\n",
      "   - - - - -   \n",
      "Epoch 342 / 400\n",
      "step    0 / 480; loss 1.3854\n",
      "step   40 / 480; loss 1.3641\n",
      "step   80 / 480; loss 1.3678\n",
      "step  120 / 480; loss 1.3674\n",
      "step  160 / 480; loss 1.3715\n",
      "step  200 / 480; loss 1.3801\n",
      "step  240 / 480; loss 1.3705\n",
      "step  280 / 480; loss 1.3868\n",
      "step  320 / 480; loss 1.3717\n",
      "step  360 / 480; loss 1.3780\n",
      "step  400 / 480; loss 1.3719\n",
      "step  440 / 480; loss 1.3690\n",
      "Evaluation loss 1.377134778912642, accuracy 0.263659375, acc CATHODE: 0.250, acc CURTAINS: 0.347, acc FETA: 0.353, acc SALAD: 0.105\n",
      "   - - - - -   \n",
      "Epoch 343 / 400\n",
      "step    0 / 480; loss 1.3711\n",
      "step   40 / 480; loss 1.3836\n",
      "step   80 / 480; loss 1.3830\n",
      "step  120 / 480; loss 1.3819\n",
      "step  160 / 480; loss 1.3780\n",
      "step  200 / 480; loss 1.3757\n",
      "step  240 / 480; loss 1.3652\n",
      "step  280 / 480; loss 1.3751\n",
      "step  320 / 480; loss 1.3733\n",
      "step  360 / 480; loss 1.3704\n",
      "step  400 / 480; loss 1.3678\n",
      "step  440 / 480; loss 1.3697\n",
      "Evaluation loss 1.377311171467875, accuracy 0.262515625, acc CATHODE: 0.245, acc CURTAINS: 0.310, acc FETA: 0.422, acc SALAD: 0.073\n",
      "   - - - - -   \n",
      "Epoch 344 / 400\n",
      "step    0 / 480; loss 1.3895\n",
      "step   40 / 480; loss 1.3759\n",
      "step   80 / 480; loss 1.3731\n",
      "step  120 / 480; loss 1.3723\n",
      "step  160 / 480; loss 1.3803\n",
      "step  200 / 480; loss 1.3748\n",
      "step  240 / 480; loss 1.3752\n",
      "step  280 / 480; loss 1.3764\n",
      "step  320 / 480; loss 1.3763\n",
      "step  360 / 480; loss 1.3625\n",
      "step  400 / 480; loss 1.3678\n",
      "step  440 / 480; loss 1.3743\n",
      "Evaluation loss 1.377334036472775, accuracy 0.263478125, acc CATHODE: 0.229, acc CURTAINS: 0.378, acc FETA: 0.337, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 345 / 400\n",
      "step    0 / 480; loss 1.3699\n",
      "step   40 / 480; loss 1.3847\n",
      "step   80 / 480; loss 1.3688\n",
      "step  120 / 480; loss 1.3695\n",
      "step  160 / 480; loss 1.3770\n",
      "step  200 / 480; loss 1.3786\n",
      "step  240 / 480; loss 1.3650\n",
      "step  280 / 480; loss 1.3749\n",
      "step  320 / 480; loss 1.3666\n",
      "step  360 / 480; loss 1.3756\n",
      "step  400 / 480; loss 1.3714\n",
      "step  440 / 480; loss 1.3736\n",
      "Evaluation loss 1.3773576906510707, accuracy 0.2621375, acc CATHODE: 0.144, acc CURTAINS: 0.388, acc FETA: 0.424, acc SALAD: 0.092\n",
      "   - - - - -   \n",
      "Epoch 346 / 400\n",
      "step    0 / 480; loss 1.3873\n",
      "step   40 / 480; loss 1.3816\n",
      "step   80 / 480; loss 1.3789\n",
      "step  120 / 480; loss 1.3770\n",
      "step  160 / 480; loss 1.3694\n",
      "step  200 / 480; loss 1.3666\n",
      "step  240 / 480; loss 1.3705\n",
      "step  280 / 480; loss 1.3716\n",
      "step  320 / 480; loss 1.3787\n",
      "step  360 / 480; loss 1.3682\n",
      "step  400 / 480; loss 1.3611\n",
      "step  440 / 480; loss 1.3656\n",
      "Evaluation loss 1.3772267773976192, accuracy 0.26335, acc CATHODE: 0.179, acc CURTAINS: 0.365, acc FETA: 0.422, acc SALAD: 0.087\n",
      "   - - - - -   \n",
      "Epoch 347 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3817\n",
      "step   80 / 480; loss 1.3749\n",
      "step  120 / 480; loss 1.3734\n",
      "step  160 / 480; loss 1.3788\n",
      "step  200 / 480; loss 1.3758\n",
      "step  240 / 480; loss 1.3805\n",
      "step  280 / 480; loss 1.3747\n",
      "step  320 / 480; loss 1.3744\n",
      "step  360 / 480; loss 1.3774\n",
      "step  400 / 480; loss 1.3739\n",
      "step  440 / 480; loss 1.3834\n",
      "Evaluation loss 1.3771852784425127, accuracy 0.263628125, acc CATHODE: 0.218, acc CURTAINS: 0.326, acc FETA: 0.350, acc SALAD: 0.160\n",
      "   - - - - -   \n",
      "Epoch 348 / 400\n",
      "step    0 / 480; loss 1.3714\n",
      "step   40 / 480; loss 1.3684\n",
      "step   80 / 480; loss 1.3798\n",
      "step  120 / 480; loss 1.3720\n",
      "step  160 / 480; loss 1.3735\n",
      "step  200 / 480; loss 1.3790\n",
      "step  240 / 480; loss 1.3728\n",
      "step  280 / 480; loss 1.3793\n",
      "step  320 / 480; loss 1.3736\n",
      "step  360 / 480; loss 1.3828\n",
      "step  400 / 480; loss 1.3763\n",
      "step  440 / 480; loss 1.3743\n",
      "Evaluation loss 1.3770710584243302, accuracy 0.263040625, acc CATHODE: 0.190, acc CURTAINS: 0.416, acc FETA: 0.365, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 349 / 400\n",
      "step    0 / 480; loss 1.3618\n",
      "step   40 / 480; loss 1.3769\n",
      "step   80 / 480; loss 1.3738\n",
      "step  120 / 480; loss 1.3767\n",
      "step  160 / 480; loss 1.3682\n",
      "step  200 / 480; loss 1.3762\n",
      "step  240 / 480; loss 1.3701\n",
      "step  280 / 480; loss 1.3809\n",
      "step  320 / 480; loss 1.3775\n",
      "step  360 / 480; loss 1.3747\n",
      "step  400 / 480; loss 1.3752\n",
      "step  440 / 480; loss 1.3665\n",
      "Evaluation loss 1.3772005786411765, accuracy 0.26364375, acc CATHODE: 0.217, acc CURTAINS: 0.364, acc FETA: 0.376, acc SALAD: 0.098\n",
      "   - - - - -   \n",
      "Epoch 350 / 400\n",
      "step    0 / 480; loss 1.3750\n",
      "step   40 / 480; loss 1.3722\n",
      "step   80 / 480; loss 1.3801\n",
      "step  120 / 480; loss 1.3674\n",
      "step  160 / 480; loss 1.3782\n",
      "step  200 / 480; loss 1.3717\n",
      "step  240 / 480; loss 1.3701\n",
      "step  280 / 480; loss 1.3799\n",
      "step  320 / 480; loss 1.3728\n",
      "step  360 / 480; loss 1.3707\n",
      "step  400 / 480; loss 1.3704\n",
      "step  440 / 480; loss 1.3732\n",
      "Evaluation loss 1.377210335778661, accuracy 0.2621875, acc CATHODE: 0.231, acc CURTAINS: 0.345, acc FETA: 0.353, acc SALAD: 0.120\n",
      "   - - - - -   \n",
      "Epoch 351 / 400\n",
      "step    0 / 480; loss 1.3819\n",
      "step   40 / 480; loss 1.3730\n",
      "step   80 / 480; loss 1.3720\n",
      "step  120 / 480; loss 1.3729\n",
      "step  160 / 480; loss 1.3673\n",
      "step  200 / 480; loss 1.3759\n",
      "step  240 / 480; loss 1.3707\n",
      "step  280 / 480; loss 1.3641\n",
      "step  320 / 480; loss 1.3769\n",
      "step  360 / 480; loss 1.3716\n",
      "step  400 / 480; loss 1.3729\n",
      "step  440 / 480; loss 1.3675\n",
      "Evaluation loss 1.3772595096418823, accuracy 0.26346875, acc CATHODE: 0.267, acc CURTAINS: 0.303, acc FETA: 0.391, acc SALAD: 0.093\n",
      "   - - - - -   \n",
      "Epoch 352 / 400\n",
      "step    0 / 480; loss 1.3599\n",
      "step   40 / 480; loss 1.3758\n",
      "step   80 / 480; loss 1.3807\n",
      "step  120 / 480; loss 1.3803\n",
      "step  160 / 480; loss 1.3739\n",
      "step  200 / 480; loss 1.3820\n",
      "step  240 / 480; loss 1.3775\n",
      "step  280 / 480; loss 1.3763\n",
      "step  320 / 480; loss 1.3665\n",
      "step  360 / 480; loss 1.3696\n",
      "step  400 / 480; loss 1.3699\n",
      "step  440 / 480; loss 1.3663\n",
      "Evaluation loss 1.3771422254867476, accuracy 0.2630125, acc CATHODE: 0.223, acc CURTAINS: 0.311, acc FETA: 0.410, acc SALAD: 0.109\n",
      "   - - - - -   \n",
      "Epoch 353 / 400\n",
      "step    0 / 480; loss 1.3826\n",
      "step   40 / 480; loss 1.3704\n",
      "step   80 / 480; loss 1.3729\n",
      "step  120 / 480; loss 1.3741\n",
      "step  160 / 480; loss 1.3809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200 / 480; loss 1.3761\n",
      "step  240 / 480; loss 1.3703\n",
      "step  280 / 480; loss 1.3707\n",
      "step  320 / 480; loss 1.3694\n",
      "step  360 / 480; loss 1.3834\n",
      "step  400 / 480; loss 1.3770\n",
      "step  440 / 480; loss 1.3628\n",
      "Evaluation loss 1.3773450040875839, accuracy 0.261196875, acc CATHODE: 0.230, acc CURTAINS: 0.300, acc FETA: 0.457, acc SALAD: 0.058\n",
      "   - - - - -   \n",
      "Epoch 354 / 400\n",
      "step    0 / 480; loss 1.3780\n",
      "step   40 / 480; loss 1.3836\n",
      "step   80 / 480; loss 1.3675\n",
      "step  120 / 480; loss 1.3754\n",
      "step  160 / 480; loss 1.3740\n",
      "step  200 / 480; loss 1.3688\n",
      "step  240 / 480; loss 1.3819\n",
      "step  280 / 480; loss 1.3681\n",
      "step  320 / 480; loss 1.3722\n",
      "step  360 / 480; loss 1.3689\n",
      "step  400 / 480; loss 1.3661\n",
      "step  440 / 480; loss 1.3686\n",
      "Evaluation loss 1.3771964843344637, accuracy 0.263509375, acc CATHODE: 0.186, acc CURTAINS: 0.408, acc FETA: 0.354, acc SALAD: 0.106\n",
      "   - - - - -   \n",
      "Epoch 355 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3745\n",
      "step   80 / 480; loss 1.3666\n",
      "step  120 / 480; loss 1.3696\n",
      "step  160 / 480; loss 1.3759\n",
      "step  200 / 480; loss 1.3679\n",
      "step  240 / 480; loss 1.3691\n",
      "step  280 / 480; loss 1.3749\n",
      "step  320 / 480; loss 1.3707\n",
      "step  360 / 480; loss 1.3707\n",
      "step  400 / 480; loss 1.3869\n",
      "step  440 / 480; loss 1.3780\n",
      "Evaluation loss 1.3771325388856983, accuracy 0.262596875, acc CATHODE: 0.231, acc CURTAINS: 0.316, acc FETA: 0.382, acc SALAD: 0.122\n",
      "   - - - - -   \n",
      "Epoch 356 / 400\n",
      "step    0 / 480; loss 1.3721\n",
      "step   40 / 480; loss 1.3733\n",
      "step   80 / 480; loss 1.3729\n",
      "step  120 / 480; loss 1.3738\n",
      "step  160 / 480; loss 1.3742\n",
      "step  200 / 480; loss 1.3818\n",
      "step  240 / 480; loss 1.3638\n",
      "step  280 / 480; loss 1.3723\n",
      "step  320 / 480; loss 1.3693\n",
      "step  360 / 480; loss 1.3775\n",
      "step  400 / 480; loss 1.3787\n",
      "step  440 / 480; loss 1.3771\n",
      "Evaluation loss 1.3771595658530411, accuracy 0.263090625, acc CATHODE: 0.211, acc CURTAINS: 0.364, acc FETA: 0.341, acc SALAD: 0.137\n",
      "   - - - - -   \n",
      "Epoch 357 / 400\n",
      "step    0 / 480; loss 1.3663\n",
      "step   40 / 480; loss 1.3650\n",
      "step   80 / 480; loss 1.3873\n",
      "step  120 / 480; loss 1.3747\n",
      "step  160 / 480; loss 1.3662\n",
      "step  200 / 480; loss 1.3730\n",
      "step  240 / 480; loss 1.3689\n",
      "step  280 / 480; loss 1.3707\n",
      "step  320 / 480; loss 1.3764\n",
      "step  360 / 480; loss 1.3687\n",
      "step  400 / 480; loss 1.3792\n",
      "step  440 / 480; loss 1.3790\n",
      "Evaluation loss 1.3773042926113683, accuracy 0.26336875, acc CATHODE: 0.223, acc CURTAINS: 0.328, acc FETA: 0.432, acc SALAD: 0.071\n",
      "   - - - - -   \n",
      "Epoch 358 / 400\n",
      "step    0 / 480; loss 1.3748\n",
      "step   40 / 480; loss 1.3702\n",
      "step   80 / 480; loss 1.3696\n",
      "step  120 / 480; loss 1.3730\n",
      "step  160 / 480; loss 1.3739\n",
      "step  200 / 480; loss 1.3741\n",
      "step  240 / 480; loss 1.3780\n",
      "step  280 / 480; loss 1.3761\n",
      "step  320 / 480; loss 1.3668\n",
      "step  360 / 480; loss 1.3775\n",
      "step  400 / 480; loss 1.3786\n",
      "step  440 / 480; loss 1.3671\n",
      "Evaluation loss 1.3774689391795616, accuracy 0.261665625, acc CATHODE: 0.174, acc CURTAINS: 0.322, acc FETA: 0.368, acc SALAD: 0.183\n",
      "   - - - - -   \n",
      "Epoch 359 / 400\n",
      "step    0 / 480; loss 1.3812\n",
      "step   40 / 480; loss 1.3717\n",
      "step   80 / 480; loss 1.3772\n",
      "step  120 / 480; loss 1.3766\n",
      "step  160 / 480; loss 1.3731\n",
      "step  200 / 480; loss 1.3781\n",
      "step  240 / 480; loss 1.3717\n",
      "step  280 / 480; loss 1.3726\n",
      "step  320 / 480; loss 1.3624\n",
      "step  360 / 480; loss 1.3799\n",
      "step  400 / 480; loss 1.3671\n",
      "step  440 / 480; loss 1.3795\n",
      "Evaluation loss 1.377253565858972, accuracy 0.263771875, acc CATHODE: 0.210, acc CURTAINS: 0.365, acc FETA: 0.361, acc SALAD: 0.119\n",
      "   - - - - -   \n",
      "Epoch 360 / 400\n",
      "step    0 / 480; loss 1.3730\n",
      "step   40 / 480; loss 1.3685\n",
      "step   80 / 480; loss 1.3736\n",
      "step  120 / 480; loss 1.3778\n",
      "step  160 / 480; loss 1.3808\n",
      "step  200 / 480; loss 1.3762\n",
      "step  240 / 480; loss 1.3752\n",
      "step  280 / 480; loss 1.3859\n",
      "step  320 / 480; loss 1.3816\n",
      "step  360 / 480; loss 1.3753\n",
      "step  400 / 480; loss 1.3603\n",
      "step  440 / 480; loss 1.3701\n",
      "Evaluation loss 1.377272759017967, accuracy 0.264015625, acc CATHODE: 0.209, acc CURTAINS: 0.315, acc FETA: 0.424, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 361 / 400\n",
      "step    0 / 480; loss 1.3688\n",
      "step   40 / 480; loss 1.3679\n",
      "step   80 / 480; loss 1.3860\n",
      "step  120 / 480; loss 1.3754\n",
      "step  160 / 480; loss 1.3749\n",
      "step  200 / 480; loss 1.3798\n",
      "step  240 / 480; loss 1.3814\n",
      "step  280 / 480; loss 1.3679\n",
      "step  320 / 480; loss 1.3648\n",
      "step  360 / 480; loss 1.3822\n",
      "step  400 / 480; loss 1.3706\n",
      "step  440 / 480; loss 1.3811\n",
      "Evaluation loss 1.3772406313836216, accuracy 0.262321875, acc CATHODE: 0.272, acc CURTAINS: 0.355, acc FETA: 0.355, acc SALAD: 0.067\n",
      "   - - - - -   \n",
      "Epoch 362 / 400\n",
      "step    0 / 480; loss 1.3707\n",
      "step   40 / 480; loss 1.3835\n",
      "step   80 / 480; loss 1.3735\n",
      "step  120 / 480; loss 1.3798\n",
      "step  160 / 480; loss 1.3868\n",
      "step  200 / 480; loss 1.3782\n",
      "step  240 / 480; loss 1.3731\n",
      "step  280 / 480; loss 1.3780\n",
      "step  320 / 480; loss 1.3802\n",
      "step  360 / 480; loss 1.3613\n",
      "step  400 / 480; loss 1.3760\n",
      "step  440 / 480; loss 1.3725\n",
      "Evaluation loss 1.377153612725785, accuracy 0.26255625, acc CATHODE: 0.212, acc CURTAINS: 0.410, acc FETA: 0.346, acc SALAD: 0.082\n",
      "   - - - - -   \n",
      "Epoch 363 / 400\n",
      "step    0 / 480; loss 1.3702\n",
      "step   40 / 480; loss 1.3725\n",
      "step   80 / 480; loss 1.3669\n",
      "step  120 / 480; loss 1.3726\n",
      "step  160 / 480; loss 1.3650\n",
      "step  200 / 480; loss 1.3739\n",
      "step  240 / 480; loss 1.3798\n",
      "step  280 / 480; loss 1.3724\n",
      "step  320 / 480; loss 1.3759\n",
      "step  360 / 480; loss 1.3696\n",
      "step  400 / 480; loss 1.3671\n",
      "step  440 / 480; loss 1.3677\n",
      "Evaluation loss 1.3771748324376167, accuracy 0.2638125, acc CATHODE: 0.234, acc CURTAINS: 0.353, acc FETA: 0.373, acc SALAD: 0.096\n",
      "   - - - - -   \n",
      "Epoch 364 / 400\n",
      "step    0 / 480; loss 1.3705\n",
      "step   40 / 480; loss 1.3825\n",
      "step   80 / 480; loss 1.3769\n",
      "step  120 / 480; loss 1.3668\n",
      "step  160 / 480; loss 1.3878\n",
      "step  200 / 480; loss 1.3708\n",
      "step  240 / 480; loss 1.3605\n",
      "step  280 / 480; loss 1.3732\n",
      "step  320 / 480; loss 1.3723\n",
      "step  360 / 480; loss 1.3667\n",
      "step  400 / 480; loss 1.3844\n",
      "step  440 / 480; loss 1.3741\n",
      "Evaluation loss 1.3773360991223498, accuracy 0.262915625, acc CATHODE: 0.231, acc CURTAINS: 0.318, acc FETA: 0.399, acc SALAD: 0.104\n",
      "   - - - - -   \n",
      "Epoch 365 / 400\n",
      "step    0 / 480; loss 1.3742\n",
      "step   40 / 480; loss 1.3824\n",
      "step   80 / 480; loss 1.3727\n",
      "step  120 / 480; loss 1.3699\n",
      "step  160 / 480; loss 1.3784\n",
      "step  200 / 480; loss 1.3702\n",
      "step  240 / 480; loss 1.3715\n",
      "step  280 / 480; loss 1.3780\n",
      "step  320 / 480; loss 1.3787\n",
      "step  360 / 480; loss 1.3703\n",
      "step  400 / 480; loss 1.3669\n",
      "step  440 / 480; loss 1.3646\n",
      "Evaluation loss 1.377224987236117, accuracy 0.263084375, acc CATHODE: 0.192, acc CURTAINS: 0.346, acc FETA: 0.397, acc SALAD: 0.117\n",
      "   - - - - -   \n",
      "Epoch 366 / 400\n",
      "step    0 / 480; loss 1.3778\n",
      "step   40 / 480; loss 1.3796\n",
      "step   80 / 480; loss 1.3792\n",
      "step  120 / 480; loss 1.3727\n",
      "step  160 / 480; loss 1.3706\n",
      "step  200 / 480; loss 1.3782\n",
      "step  240 / 480; loss 1.3702\n",
      "step  280 / 480; loss 1.3739\n",
      "step  320 / 480; loss 1.3672\n",
      "step  360 / 480; loss 1.3705\n",
      "step  400 / 480; loss 1.3776\n",
      "step  440 / 480; loss 1.3729\n",
      "Evaluation loss 1.377288585010249, accuracy 0.26236875, acc CATHODE: 0.234, acc CURTAINS: 0.327, acc FETA: 0.320, acc SALAD: 0.168\n",
      "   - - - - -   \n",
      "Epoch 367 / 400\n",
      "step    0 / 480; loss 1.3764\n",
      "step   40 / 480; loss 1.3720\n",
      "step   80 / 480; loss 1.3682\n",
      "step  120 / 480; loss 1.3694\n",
      "step  160 / 480; loss 1.3805\n",
      "step  200 / 480; loss 1.3741\n",
      "step  240 / 480; loss 1.3683\n",
      "step  280 / 480; loss 1.3756\n",
      "step  320 / 480; loss 1.3760\n",
      "step  360 / 480; loss 1.3659\n",
      "step  400 / 480; loss 1.3772\n",
      "step  440 / 480; loss 1.3701\n",
      "Evaluation loss 1.377394665312905, accuracy 0.26263125, acc CATHODE: 0.253, acc CURTAINS: 0.331, acc FETA: 0.396, acc SALAD: 0.070\n",
      "   - - - - -   \n",
      "Epoch 368 / 400\n",
      "step    0 / 480; loss 1.3806\n",
      "step   40 / 480; loss 1.3817\n",
      "step   80 / 480; loss 1.3754\n",
      "step  120 / 480; loss 1.3784\n",
      "step  160 / 480; loss 1.3771\n",
      "step  200 / 480; loss 1.3836\n",
      "step  240 / 480; loss 1.3687\n",
      "step  280 / 480; loss 1.3749\n",
      "step  320 / 480; loss 1.3637\n",
      "step  360 / 480; loss 1.3810\n",
      "step  400 / 480; loss 1.3651\n",
      "step  440 / 480; loss 1.3743\n",
      "Evaluation loss 1.3772564996922882, accuracy 0.26319375, acc CATHODE: 0.198, acc CURTAINS: 0.436, acc FETA: 0.321, acc SALAD: 0.098\n",
      "   - - - - -   \n",
      "Epoch 369 / 400\n",
      "step    0 / 480; loss 1.3763\n",
      "step   40 / 480; loss 1.3754\n",
      "step   80 / 480; loss 1.3741\n",
      "step  120 / 480; loss 1.3799\n",
      "step  160 / 480; loss 1.3751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200 / 480; loss 1.3650\n",
      "step  240 / 480; loss 1.3703\n",
      "step  280 / 480; loss 1.3729\n",
      "step  320 / 480; loss 1.3662\n",
      "step  360 / 480; loss 1.3727\n",
      "step  400 / 480; loss 1.3766\n",
      "step  440 / 480; loss 1.3667\n",
      "Evaluation loss 1.377265199975986, accuracy 0.2633625, acc CATHODE: 0.271, acc CURTAINS: 0.312, acc FETA: 0.384, acc SALAD: 0.086\n",
      "   - - - - -   \n",
      "Epoch 370 / 400\n",
      "step    0 / 480; loss 1.3792\n",
      "step   40 / 480; loss 1.3808\n",
      "step   80 / 480; loss 1.3692\n",
      "step  120 / 480; loss 1.3759\n",
      "step  160 / 480; loss 1.3783\n",
      "step  200 / 480; loss 1.3862\n",
      "step  240 / 480; loss 1.3659\n",
      "step  280 / 480; loss 1.3733\n",
      "step  320 / 480; loss 1.3837\n",
      "step  360 / 480; loss 1.3907\n",
      "step  400 / 480; loss 1.3764\n",
      "step  440 / 480; loss 1.3712\n",
      "Evaluation loss 1.3772327784225904, accuracy 0.26350625, acc CATHODE: 0.244, acc CURTAINS: 0.308, acc FETA: 0.352, acc SALAD: 0.150\n",
      "   - - - - -   \n",
      "Epoch 371 / 400\n",
      "step    0 / 480; loss 1.3741\n",
      "step   40 / 480; loss 1.3808\n",
      "step   80 / 480; loss 1.3737\n",
      "step  120 / 480; loss 1.3757\n",
      "step  160 / 480; loss 1.3708\n",
      "step  200 / 480; loss 1.3768\n",
      "step  240 / 480; loss 1.3789\n",
      "step  280 / 480; loss 1.3708\n",
      "step  320 / 480; loss 1.3664\n",
      "step  360 / 480; loss 1.3721\n",
      "step  400 / 480; loss 1.3751\n",
      "step  440 / 480; loss 1.3772\n",
      "Evaluation loss 1.377198764490231, accuracy 0.262721875, acc CATHODE: 0.214, acc CURTAINS: 0.362, acc FETA: 0.375, acc SALAD: 0.100\n",
      "   - - - - -   \n",
      "Epoch 372 / 400\n",
      "step    0 / 480; loss 1.3769\n",
      "step   40 / 480; loss 1.3814\n",
      "step   80 / 480; loss 1.3792\n",
      "step  120 / 480; loss 1.3739\n",
      "step  160 / 480; loss 1.3797\n",
      "step  200 / 480; loss 1.3654\n",
      "step  240 / 480; loss 1.3619\n",
      "step  280 / 480; loss 1.3669\n",
      "step  320 / 480; loss 1.3811\n",
      "step  360 / 480; loss 1.3736\n",
      "step  400 / 480; loss 1.3741\n",
      "step  440 / 480; loss 1.3700\n",
      "Evaluation loss 1.3771821324366793, accuracy 0.26364375, acc CATHODE: 0.197, acc CURTAINS: 0.278, acc FETA: 0.489, acc SALAD: 0.091\n",
      "   - - - - -   \n",
      "Epoch 373 / 400\n",
      "step    0 / 480; loss 1.3748\n",
      "step   40 / 480; loss 1.3765\n",
      "step   80 / 480; loss 1.3732\n",
      "step  120 / 480; loss 1.3743\n",
      "step  160 / 480; loss 1.3700\n",
      "step  200 / 480; loss 1.3793\n",
      "step  240 / 480; loss 1.3698\n",
      "step  280 / 480; loss 1.3800\n",
      "step  320 / 480; loss 1.3673\n",
      "step  360 / 480; loss 1.3732\n",
      "step  400 / 480; loss 1.3622\n",
      "step  440 / 480; loss 1.3701\n",
      "Evaluation loss 1.377336796158055, accuracy 0.261853125, acc CATHODE: 0.283, acc CURTAINS: 0.270, acc FETA: 0.370, acc SALAD: 0.125\n",
      "   - - - - -   \n",
      "Epoch 374 / 400\n",
      "step    0 / 480; loss 1.3783\n",
      "step   40 / 480; loss 1.3808\n",
      "step   80 / 480; loss 1.3720\n",
      "step  120 / 480; loss 1.3667\n",
      "step  160 / 480; loss 1.3740\n",
      "step  200 / 480; loss 1.3827\n",
      "step  240 / 480; loss 1.3738\n",
      "step  280 / 480; loss 1.3816\n",
      "step  320 / 480; loss 1.3663\n",
      "step  360 / 480; loss 1.3792\n",
      "step  400 / 480; loss 1.3794\n",
      "step  440 / 480; loss 1.3824\n",
      "Evaluation loss 1.3773176983424045, accuracy 0.263090625, acc CATHODE: 0.239, acc CURTAINS: 0.321, acc FETA: 0.358, acc SALAD: 0.134\n",
      "   - - - - -   \n",
      "Epoch 375 / 400\n",
      "step    0 / 480; loss 1.3714\n",
      "step   40 / 480; loss 1.3740\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3789\n",
      "step  160 / 480; loss 1.3697\n",
      "step  200 / 480; loss 1.3725\n",
      "step  240 / 480; loss 1.3786\n",
      "step  280 / 480; loss 1.3752\n",
      "step  320 / 480; loss 1.3713\n",
      "step  360 / 480; loss 1.3806\n",
      "step  400 / 480; loss 1.3741\n",
      "step  440 / 480; loss 1.3710\n",
      "Evaluation loss 1.3772921408078207, accuracy 0.262134375, acc CATHODE: 0.231, acc CURTAINS: 0.305, acc FETA: 0.405, acc SALAD: 0.108\n",
      "   - - - - -   \n",
      "Epoch 376 / 400\n",
      "step    0 / 480; loss 1.3754\n",
      "step   40 / 480; loss 1.3710\n",
      "step   80 / 480; loss 1.3847\n",
      "step  120 / 480; loss 1.3763\n",
      "step  160 / 480; loss 1.3776\n",
      "step  200 / 480; loss 1.3722\n",
      "step  240 / 480; loss 1.3784\n",
      "step  280 / 480; loss 1.3752\n",
      "step  320 / 480; loss 1.3833\n",
      "step  360 / 480; loss 1.3689\n",
      "step  400 / 480; loss 1.3691\n",
      "step  440 / 480; loss 1.3821\n",
      "Evaluation loss 1.377237433891526, accuracy 0.26285, acc CATHODE: 0.206, acc CURTAINS: 0.370, acc FETA: 0.407, acc SALAD: 0.068\n",
      "   - - - - -   \n",
      "Epoch 377 / 400\n",
      "step    0 / 480; loss 1.3743\n",
      "step   40 / 480; loss 1.3730\n",
      "step   80 / 480; loss 1.3737\n",
      "step  120 / 480; loss 1.3732\n",
      "step  160 / 480; loss 1.3777\n",
      "step  200 / 480; loss 1.3683\n",
      "step  240 / 480; loss 1.3761\n",
      "step  280 / 480; loss 1.3735\n",
      "step  320 / 480; loss 1.3689\n",
      "step  360 / 480; loss 1.3672\n",
      "step  400 / 480; loss 1.3813\n",
      "step  440 / 480; loss 1.3682\n",
      "Evaluation loss 1.3773273376715598, accuracy 0.2623375, acc CATHODE: 0.280, acc CURTAINS: 0.321, acc FETA: 0.334, acc SALAD: 0.113\n",
      "   - - - - -   \n",
      "Epoch 378 / 400\n",
      "step    0 / 480; loss 1.3713\n",
      "step   40 / 480; loss 1.3789\n",
      "step   80 / 480; loss 1.3756\n",
      "step  120 / 480; loss 1.3805\n",
      "step  160 / 480; loss 1.3714\n",
      "step  200 / 480; loss 1.3785\n",
      "step  240 / 480; loss 1.3722\n",
      "step  280 / 480; loss 1.3695\n",
      "step  320 / 480; loss 1.3820\n",
      "step  360 / 480; loss 1.3777\n",
      "step  400 / 480; loss 1.3728\n",
      "step  440 / 480; loss 1.3757\n",
      "Evaluation loss 1.3773109095724398, accuracy 0.26334375, acc CATHODE: 0.231, acc CURTAINS: 0.336, acc FETA: 0.397, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 379 / 400\n",
      "step    0 / 480; loss 1.3784\n",
      "step   40 / 480; loss 1.3729\n",
      "step   80 / 480; loss 1.3757\n",
      "step  120 / 480; loss 1.3735\n",
      "step  160 / 480; loss 1.3708\n",
      "step  200 / 480; loss 1.3747\n",
      "step  240 / 480; loss 1.3705\n",
      "step  280 / 480; loss 1.3672\n",
      "step  320 / 480; loss 1.3617\n",
      "step  360 / 480; loss 1.3729\n",
      "step  400 / 480; loss 1.3660\n",
      "step  440 / 480; loss 1.3669\n",
      "Evaluation loss 1.3773996264276667, accuracy 0.263084375, acc CATHODE: 0.146, acc CURTAINS: 0.375, acc FETA: 0.365, acc SALAD: 0.166\n",
      "   - - - - -   \n",
      "Epoch 380 / 400\n",
      "step    0 / 480; loss 1.3714\n",
      "step   40 / 480; loss 1.3671\n",
      "step   80 / 480; loss 1.3759\n",
      "step  120 / 480; loss 1.3752\n",
      "step  160 / 480; loss 1.3699\n",
      "step  200 / 480; loss 1.3850\n",
      "step  240 / 480; loss 1.3691\n",
      "step  280 / 480; loss 1.3724\n",
      "step  320 / 480; loss 1.3805\n",
      "step  360 / 480; loss 1.3759\n",
      "step  400 / 480; loss 1.3804\n",
      "step  440 / 480; loss 1.3767\n",
      "Evaluation loss 1.3773026864666655, accuracy 0.262990625, acc CATHODE: 0.186, acc CURTAINS: 0.401, acc FETA: 0.354, acc SALAD: 0.110\n",
      "   - - - - -   \n",
      "Epoch 381 / 400\n",
      "step    0 / 480; loss 1.3672\n",
      "step   40 / 480; loss 1.3724\n",
      "step   80 / 480; loss 1.3756\n",
      "step  120 / 480; loss 1.3760\n",
      "step  160 / 480; loss 1.3613\n",
      "step  200 / 480; loss 1.3735\n",
      "step  240 / 480; loss 1.3805\n",
      "step  280 / 480; loss 1.3668\n",
      "step  320 / 480; loss 1.3682\n",
      "step  360 / 480; loss 1.3871\n",
      "step  400 / 480; loss 1.3758\n",
      "step  440 / 480; loss 1.3740\n",
      "Evaluation loss 1.377309537565129, accuracy 0.2656625, acc CATHODE: 0.208, acc CURTAINS: 0.385, acc FETA: 0.356, acc SALAD: 0.112\n",
      "   - - - - -   \n",
      "Epoch 382 / 400\n",
      "step    0 / 480; loss 1.3719\n",
      "step   40 / 480; loss 1.3715\n",
      "step   80 / 480; loss 1.3742\n",
      "step  120 / 480; loss 1.3769\n",
      "step  160 / 480; loss 1.3720\n",
      "step  200 / 480; loss 1.3766\n",
      "step  240 / 480; loss 1.3703\n",
      "step  280 / 480; loss 1.3771\n",
      "step  320 / 480; loss 1.3774\n",
      "step  360 / 480; loss 1.3851\n",
      "step  400 / 480; loss 1.3656\n",
      "step  440 / 480; loss 1.3767\n",
      "Evaluation loss 1.3772963825022928, accuracy 0.26289375, acc CATHODE: 0.255, acc CURTAINS: 0.391, acc FETA: 0.316, acc SALAD: 0.089\n",
      "   - - - - -   \n",
      "Epoch 383 / 400\n",
      "step    0 / 480; loss 1.3810\n",
      "step   40 / 480; loss 1.3734\n",
      "step   80 / 480; loss 1.3745\n",
      "step  120 / 480; loss 1.3746\n",
      "step  160 / 480; loss 1.3701\n",
      "step  200 / 480; loss 1.3754\n",
      "step  240 / 480; loss 1.3798\n",
      "step  280 / 480; loss 1.3690\n",
      "step  320 / 480; loss 1.3838\n",
      "step  360 / 480; loss 1.3646\n",
      "step  400 / 480; loss 1.3691\n",
      "step  440 / 480; loss 1.3730\n",
      "Evaluation loss 1.3774463546583977, accuracy 0.26511875, acc CATHODE: 0.218, acc CURTAINS: 0.353, acc FETA: 0.363, acc SALAD: 0.126\n",
      "   - - - - -   \n",
      "Epoch 384 / 400\n",
      "step    0 / 480; loss 1.3686\n",
      "step   40 / 480; loss 1.3795\n",
      "step   80 / 480; loss 1.3704\n",
      "step  120 / 480; loss 1.3802\n",
      "step  160 / 480; loss 1.3701\n",
      "step  200 / 480; loss 1.3739\n",
      "step  240 / 480; loss 1.3754\n",
      "step  280 / 480; loss 1.3777\n",
      "step  320 / 480; loss 1.3678\n",
      "step  360 / 480; loss 1.3696\n",
      "step  400 / 480; loss 1.3753\n",
      "step  440 / 480; loss 1.3767\n",
      "Evaluation loss 1.3774107107608833, accuracy 0.26270625, acc CATHODE: 0.174, acc CURTAINS: 0.353, acc FETA: 0.408, acc SALAD: 0.117\n",
      "   - - - - -   \n",
      "Epoch 385 / 400\n",
      "step    0 / 480; loss 1.3794\n",
      "step   40 / 480; loss 1.3748\n",
      "step   80 / 480; loss 1.3690\n",
      "step  120 / 480; loss 1.3756\n",
      "step  160 / 480; loss 1.3729\n",
      "step  200 / 480; loss 1.3650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  240 / 480; loss 1.3676\n",
      "step  280 / 480; loss 1.3776\n",
      "step  320 / 480; loss 1.3704\n",
      "step  360 / 480; loss 1.3734\n",
      "step  400 / 480; loss 1.3773\n",
      "step  440 / 480; loss 1.3759\n",
      "Evaluation loss 1.3772058294718812, accuracy 0.2631875, acc CATHODE: 0.229, acc CURTAINS: 0.345, acc FETA: 0.369, acc SALAD: 0.109\n",
      "   - - - - -   \n",
      "Epoch 386 / 400\n",
      "step    0 / 480; loss 1.3752\n",
      "step   40 / 480; loss 1.3704\n",
      "step   80 / 480; loss 1.3753\n",
      "step  120 / 480; loss 1.3758\n",
      "step  160 / 480; loss 1.3691\n",
      "step  200 / 480; loss 1.3687\n",
      "step  240 / 480; loss 1.3691\n",
      "step  280 / 480; loss 1.3683\n",
      "step  320 / 480; loss 1.3703\n",
      "step  360 / 480; loss 1.3818\n",
      "step  400 / 480; loss 1.3695\n",
      "step  440 / 480; loss 1.3673\n",
      "Evaluation loss 1.3774822568357157, accuracy 0.26211875, acc CATHODE: 0.212, acc CURTAINS: 0.317, acc FETA: 0.420, acc SALAD: 0.099\n",
      "   - - - - -   \n",
      "Epoch 387 / 400\n",
      "step    0 / 480; loss 1.3785\n",
      "step   40 / 480; loss 1.3642\n",
      "step   80 / 480; loss 1.3786\n",
      "step  120 / 480; loss 1.3709\n",
      "step  160 / 480; loss 1.3785\n",
      "step  200 / 480; loss 1.3725\n",
      "step  240 / 480; loss 1.3710\n",
      "step  280 / 480; loss 1.3736\n",
      "step  320 / 480; loss 1.3862\n",
      "step  360 / 480; loss 1.3700\n",
      "step  400 / 480; loss 1.3638\n",
      "step  440 / 480; loss 1.3776\n",
      "Evaluation loss 1.3773974336622818, accuracy 0.262725, acc CATHODE: 0.240, acc CURTAINS: 0.262, acc FETA: 0.402, acc SALAD: 0.147\n",
      "   - - - - -   \n",
      "Epoch 388 / 400\n",
      "step    0 / 480; loss 1.3693\n",
      "step   40 / 480; loss 1.3763\n",
      "step   80 / 480; loss 1.3723\n",
      "step  120 / 480; loss 1.3749\n",
      "step  160 / 480; loss 1.3773\n",
      "step  200 / 480; loss 1.3684\n",
      "step  240 / 480; loss 1.3652\n",
      "step  280 / 480; loss 1.3831\n",
      "step  320 / 480; loss 1.3770\n",
      "step  360 / 480; loss 1.3698\n",
      "step  400 / 480; loss 1.3742\n",
      "step  440 / 480; loss 1.3711\n",
      "Evaluation loss 1.3774441363863459, accuracy 0.26263125, acc CATHODE: 0.267, acc CURTAINS: 0.389, acc FETA: 0.298, acc SALAD: 0.096\n",
      "   - - - - -   \n",
      "Epoch 389 / 400\n",
      "step    0 / 480; loss 1.3812\n",
      "step   40 / 480; loss 1.3740\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3732\n",
      "step  160 / 480; loss 1.3693\n",
      "step  200 / 480; loss 1.3748\n",
      "step  240 / 480; loss 1.3794\n",
      "step  280 / 480; loss 1.3727\n",
      "step  320 / 480; loss 1.3767\n",
      "step  360 / 480; loss 1.3665\n",
      "step  400 / 480; loss 1.3672\n",
      "step  440 / 480; loss 1.3673\n",
      "Evaluation loss 1.3773372914011455, accuracy 0.261534375, acc CATHODE: 0.245, acc CURTAINS: 0.311, acc FETA: 0.414, acc SALAD: 0.077\n",
      "   - - - - -   \n",
      "Epoch 390 / 400\n",
      "step    0 / 480; loss 1.3663\n",
      "step   40 / 480; loss 1.3771\n",
      "step   80 / 480; loss 1.3743\n",
      "step  120 / 480; loss 1.3834\n",
      "step  160 / 480; loss 1.3685\n",
      "step  200 / 480; loss 1.3758\n",
      "step  240 / 480; loss 1.3719\n",
      "step  280 / 480; loss 1.3683\n",
      "step  320 / 480; loss 1.3808\n",
      "step  360 / 480; loss 1.3724\n",
      "step  400 / 480; loss 1.3694\n",
      "step  440 / 480; loss 1.3775\n",
      "Evaluation loss 1.3773187613649793, accuracy 0.2624875, acc CATHODE: 0.278, acc CURTAINS: 0.287, acc FETA: 0.375, acc SALAD: 0.109\n",
      "   - - - - -   \n",
      "Epoch 391 / 400\n",
      "step    0 / 480; loss 1.3737\n",
      "step   40 / 480; loss 1.3665\n",
      "step   80 / 480; loss 1.3833\n",
      "step  120 / 480; loss 1.3803\n",
      "step  160 / 480; loss 1.3662\n",
      "step  200 / 480; loss 1.3758\n",
      "step  240 / 480; loss 1.3792\n",
      "step  280 / 480; loss 1.3778\n",
      "step  320 / 480; loss 1.3648\n",
      "step  360 / 480; loss 1.3786\n",
      "step  400 / 480; loss 1.3760\n",
      "step  440 / 480; loss 1.3723\n",
      "Evaluation loss 1.3773128025420194, accuracy 0.26289375, acc CATHODE: 0.261, acc CURTAINS: 0.292, acc FETA: 0.417, acc SALAD: 0.081\n",
      "   - - - - -   \n",
      "Epoch 392 / 400\n",
      "step    0 / 480; loss 1.3751\n",
      "step   40 / 480; loss 1.3876\n",
      "step   80 / 480; loss 1.3761\n",
      "step  120 / 480; loss 1.3777\n",
      "step  160 / 480; loss 1.3807\n",
      "step  200 / 480; loss 1.3686\n",
      "step  240 / 480; loss 1.3644\n",
      "step  280 / 480; loss 1.3845\n",
      "step  320 / 480; loss 1.3734\n",
      "step  360 / 480; loss 1.3840\n",
      "step  400 / 480; loss 1.3751\n",
      "step  440 / 480; loss 1.3818\n",
      "Evaluation loss 1.3774005890166574, accuracy 0.261959375, acc CATHODE: 0.181, acc CURTAINS: 0.447, acc FETA: 0.328, acc SALAD: 0.091\n",
      "   - - - - -   \n",
      "Epoch 393 / 400\n",
      "step    0 / 480; loss 1.3768\n",
      "step   40 / 480; loss 1.3820\n",
      "step   80 / 480; loss 1.3744\n",
      "step  120 / 480; loss 1.3758\n",
      "step  160 / 480; loss 1.3743\n",
      "step  200 / 480; loss 1.3729\n",
      "step  240 / 480; loss 1.3664\n",
      "step  280 / 480; loss 1.3801\n",
      "step  320 / 480; loss 1.3706\n",
      "step  360 / 480; loss 1.3673\n",
      "step  400 / 480; loss 1.3712\n",
      "step  440 / 480; loss 1.3709\n",
      "Evaluation loss 1.3773139319776766, accuracy 0.26251875, acc CATHODE: 0.218, acc CURTAINS: 0.359, acc FETA: 0.341, acc SALAD: 0.132\n",
      "   - - - - -   \n",
      "Epoch 394 / 400\n",
      "step    0 / 480; loss 1.3689\n",
      "step   40 / 480; loss 1.3766\n",
      "step   80 / 480; loss 1.3657\n",
      "step  120 / 480; loss 1.3640\n",
      "step  160 / 480; loss 1.3705\n",
      "step  200 / 480; loss 1.3716\n",
      "step  240 / 480; loss 1.3785\n",
      "step  280 / 480; loss 1.3738\n",
      "step  320 / 480; loss 1.3779\n",
      "step  360 / 480; loss 1.3798\n",
      "step  400 / 480; loss 1.3756\n",
      "step  440 / 480; loss 1.3776\n",
      "Evaluation loss 1.3773626203264573, accuracy 0.26271875, acc CATHODE: 0.301, acc CURTAINS: 0.318, acc FETA: 0.354, acc SALAD: 0.078\n",
      "   - - - - -   \n",
      "Epoch 395 / 400\n",
      "step    0 / 480; loss 1.3806\n",
      "step   40 / 480; loss 1.3778\n",
      "step   80 / 480; loss 1.3791\n",
      "step  120 / 480; loss 1.3706\n",
      "step  160 / 480; loss 1.3811\n",
      "step  200 / 480; loss 1.3732\n",
      "step  240 / 480; loss 1.3723\n",
      "step  280 / 480; loss 1.3625\n",
      "step  320 / 480; loss 1.3731\n",
      "step  360 / 480; loss 1.3711\n",
      "step  400 / 480; loss 1.3859\n",
      "step  440 / 480; loss 1.3806\n",
      "Evaluation loss 1.3774054928999668, accuracy 0.263190625, acc CATHODE: 0.288, acc CURTAINS: 0.374, acc FETA: 0.288, acc SALAD: 0.102\n",
      "   - - - - -   \n",
      "Epoch 396 / 400\n",
      "step    0 / 480; loss 1.3738\n",
      "step   40 / 480; loss 1.3763\n",
      "step   80 / 480; loss 1.3682\n",
      "step  120 / 480; loss 1.3783\n",
      "step  160 / 480; loss 1.3600\n",
      "step  200 / 480; loss 1.3715\n",
      "step  240 / 480; loss 1.3675\n",
      "step  280 / 480; loss 1.3689\n",
      "step  320 / 480; loss 1.3789\n",
      "step  360 / 480; loss 1.3741\n",
      "step  400 / 480; loss 1.3731\n",
      "step  440 / 480; loss 1.3762\n",
      "Evaluation loss 1.3772669805377893, accuracy 0.26284375, acc CATHODE: 0.344, acc CURTAINS: 0.292, acc FETA: 0.344, acc SALAD: 0.071\n",
      "   - - - - -   \n",
      "Epoch 397 / 400\n",
      "step    0 / 480; loss 1.3735\n",
      "step   40 / 480; loss 1.3873\n",
      "step   80 / 480; loss 1.3745\n",
      "step  120 / 480; loss 1.3745\n",
      "step  160 / 480; loss 1.3662\n",
      "step  200 / 480; loss 1.3734\n",
      "step  240 / 480; loss 1.3720\n",
      "step  280 / 480; loss 1.3625\n",
      "step  320 / 480; loss 1.3826\n",
      "step  360 / 480; loss 1.3682\n",
      "step  400 / 480; loss 1.3742\n",
      "step  440 / 480; loss 1.3721\n",
      "Evaluation loss 1.3774021870735478, accuracy 0.26215625, acc CATHODE: 0.268, acc CURTAINS: 0.249, acc FETA: 0.392, acc SALAD: 0.139\n",
      "   - - - - -   \n",
      "Epoch 398 / 400\n",
      "step    0 / 480; loss 1.3681\n",
      "step   40 / 480; loss 1.3906\n",
      "step   80 / 480; loss 1.3769\n",
      "step  120 / 480; loss 1.3734\n",
      "step  160 / 480; loss 1.3764\n",
      "step  200 / 480; loss 1.3748\n",
      "step  240 / 480; loss 1.3827\n",
      "step  280 / 480; loss 1.3639\n",
      "step  320 / 480; loss 1.3646\n",
      "step  360 / 480; loss 1.3755\n",
      "step  400 / 480; loss 1.3855\n",
      "step  440 / 480; loss 1.3744\n",
      "Evaluation loss 1.37737333153044, accuracy 0.26311875, acc CATHODE: 0.179, acc CURTAINS: 0.342, acc FETA: 0.417, acc SALAD: 0.114\n",
      "   - - - - -   \n",
      "Epoch 399 / 400\n",
      "step    0 / 480; loss 1.3795\n",
      "step   40 / 480; loss 1.3778\n",
      "step   80 / 480; loss 1.3790\n",
      "step  120 / 480; loss 1.3777\n",
      "step  160 / 480; loss 1.3642\n",
      "step  200 / 480; loss 1.3698\n",
      "step  240 / 480; loss 1.3745\n",
      "step  280 / 480; loss 1.3789\n",
      "step  320 / 480; loss 1.3644\n",
      "step  360 / 480; loss 1.3642\n",
      "step  400 / 480; loss 1.3749\n",
      "step  440 / 480; loss 1.3770\n",
      "Evaluation loss 1.3772782201977334, accuracy 0.26233125, acc CATHODE: 0.238, acc CURTAINS: 0.350, acc FETA: 0.369, acc SALAD: 0.093\n",
      "   - - - - -   \n",
      "Epoch 400 / 400\n",
      "step    0 / 480; loss 1.3678\n",
      "step   40 / 480; loss 1.3755\n",
      "step   80 / 480; loss 1.3755\n",
      "step  120 / 480; loss 1.3793\n",
      "step  160 / 480; loss 1.3813\n",
      "step  200 / 480; loss 1.3789\n",
      "step  240 / 480; loss 1.3736\n",
      "step  280 / 480; loss 1.3713\n",
      "step  320 / 480; loss 1.3701\n",
      "step  360 / 480; loss 1.3749\n",
      "step  400 / 480; loss 1.3787\n",
      "step  440 / 480; loss 1.3717\n",
      "Evaluation loss 1.3774779440064275, accuracy 0.262621875, acc CATHODE: 0.133, acc CURTAINS: 0.392, acc FETA: 0.414, acc SALAD: 0.111\n",
      "   - - - - -   \n",
      "Model multiclass_weights_run10_balanced_modified.pt loaded\n",
      "Evaluation loss 1.3766724147159581, accuracy 0.26406875, acc CATHODE: 0.184, acc CURTAINS: 0.449, acc FETA: 0.319, acc SALAD: 0.105\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 400\n",
    "name_appendix = 'run01_balanced_5_modified'\n",
    "\n",
    "best_val_loss = 1e6\n",
    "\n",
    "accuracy = []\n",
    "_, acc = evaluate(dense_net, test_dataloader, criterion)\n",
    "accuracy.append(acc.mean())\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    print(f\"Epoch {epoch+1} / {num_epoch}\")\n",
    "    train_loss = train(dense_net, train_dataloader, optimizer, criterion)\n",
    "    eval_loss, acc = evaluate(dense_net, test_dataloader, criterion)\n",
    "    train_losses.append(train_loss.mean())\n",
    "    eval_losses.append(eval_loss)\n",
    "    accuracy.append(acc.mean())\n",
    "    if eval_loss.mean() < best_val_loss:\n",
    "        save_weights(dense_net, appendix=name_appendix)\n",
    "        best_val_loss = eval_loss.mean()\n",
    "    print(\"   - - - - -   \")\n",
    "\n",
    "load_weights(dense_net, device, appendix=name_appendix)\n",
    "_ = evaluate(dense_net, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ff3f71f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(num_epoch) + 1\n",
    "plt.plot(epochs, train_losses, label='train')\n",
    "plt.plot(epochs, eval_losses, label='test')\n",
    "plt.legend()\n",
    "plt.ylim(1.373, 1.38)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4a59c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run nr 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NeuralNet:\n\tUnexpected key(s) in state_dict: \"fc4.weight\", \"fc4.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 6]) from checkpoint, the shape in current model is torch.Size([64, 6]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([4, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_217332/2853869387.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Run nr {run_nr}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mname_appendix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'run{run_nr:02d}_balanced_modified'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_appendix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpreds_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpreds_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_217332/4027621524.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(model, device, appendix)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'multiclass_weights.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NeuralNet:\n\tUnexpected key(s) in state_dict: \"fc4.weight\", \"fc4.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 6]) from checkpoint, the shape in current model is torch.Size([64, 6]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([128, 128]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([4, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([4])."
     ]
    }
   ],
   "source": [
    "# run multiple runs at once\n",
    "\n",
    "output_str = \"log posterior of {} is {}. Argmax is at {}\"\n",
    "output_str_2 = \"log posterior of {} is {} +/- {}. Argmax is at {}\"\n",
    "\n",
    "preds_cathode_list = []\n",
    "preds_curtains_list = []\n",
    "preds_feta_list = []\n",
    "preds_salad_list = []\n",
    "weights_salad_list = []\n",
    "preds_truth_list = []\n",
    "\n",
    "log_posterior_dict = {'CATHODE': [], 'CURTAINS': [], 'FETA': [], 'SALAD': [], 'TRUTH': []}\n",
    "\n",
    "for run_nr in [1,2,3,4,5, 6, 7, 8, 9, 10]:\n",
    "    print(f\"Run nr {run_nr}\")\n",
    "    name_appendix = f'run{run_nr:02d}_balanced_5_modified'\n",
    "    load_weights(dense_net, device, appendix=name_appendix)\n",
    "    preds_models, weights_models = get_prediction(dense_net, test_dataloader)\n",
    "    preds_truth, _ = get_prediction(dense_net, truth_dataloader)\n",
    "    preds_cathode_list.append(preds_models[preds_models[:, -1] == 0.][:, :4])\n",
    "    preds_curtains_list.append(preds_models[preds_models[:, -1] == 1.][:, :4])\n",
    "    preds_feta_list.append(preds_models[preds_models[:, -1] == 2.][:, :4])\n",
    "    preds_salad_list.append(preds_models[preds_models[:, -1] == 3.][:, :4])\n",
    "    weights_salad_list.append(weights_models[preds_models[:, -1] == 3.].reshape(-1,1))\n",
    "    preds_truth_list.append(preds_truth[:, :4])\n",
    "    \n",
    "    print(output_str.format(\"CATHODE samples\", \n",
    "                            post_local:=log_posterior(preds_cathode_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_cathode_list[-1]))))\n",
    "    log_posterior_dict['CATHODE'].append(post_local/len(preds_cathode_list[-1]))\n",
    "    print(output_str.format(\"CURTAINS samples\", \n",
    "                            post_local:=log_posterior(preds_curtains_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_curtains_list[-1]))))\n",
    "    log_posterior_dict['CURTAINS'].append(post_local/len(preds_curtains_list[-1]))\n",
    "    print(output_str.format(\"FETA samples\", \n",
    "                            post_local:=log_posterior(preds_feta_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_feta_list[-1]))))\n",
    "    log_posterior_dict['FETA'].append(post_local/len(preds_feta_list[-1]))\n",
    "    print(output_str.format(\"SALAD samples\", \n",
    "                            post_local:=log_posterior(preds_salad_list[-1], weights=weights_salad_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_salad_list[-1]))))\n",
    "    #log_posterior_dict['SALAD'].append(post_local/(weights_salad_list[-1].sum()))\n",
    "    log_posterior_dict['SALAD'].append(post_local/len(preds_salad_list[-1]))\n",
    "    print(output_str.format(\"True samples\", \n",
    "                            post_local:=log_posterior(preds_truth_list[-1]), \n",
    "                            np.argmax(log_posterior(preds_truth_list[-1]))))\n",
    "    log_posterior_dict['TRUTH'].append(post_local/len(preds_truth_list[-1]))\n",
    "    \n",
    "preds_cathode_list = np.array(preds_cathode_list)\n",
    "preds_curtains_list = np.array(preds_curtains_list)\n",
    "preds_feta_list = np.array(preds_feta_list)\n",
    "preds_salad_list = np.array(preds_salad_list)\n",
    "weights_salad_list = np.array(weights_salad_list)\n",
    "preds_truth_list = np.array(preds_truth_list)\n",
    "\n",
    "for key in log_posterior_dict:\n",
    "    log_posterior_dict[key] = np.array(log_posterior_dict[key])\n",
    "\n",
    "print(\"averaged scores: \")\n",
    "print(output_str.format(\"CATHODE samples\", \n",
    "                        log_posterior(preds_cathode_list.mean(0)),\n",
    "                        np.argmax(log_posterior(preds_cathode_list.mean(0)))))\n",
    "print(output_str.format(\"CURTAINS samples\", \n",
    "                        log_posterior(preds_curtains_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_curtains_list.mean(0)))))\n",
    "print(output_str.format(\"FETA samples\", \n",
    "                        log_posterior(preds_feta_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_feta_list.mean(0)))))\n",
    "print(output_str.format(\"SALAD samples\", \n",
    "                        log_posterior(preds_salad_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_salad_list.mean(0)))))\n",
    "print(output_str.format(\"True samples\", \n",
    "                        log_posterior(preds_truth_list.mean(0)), \n",
    "                        np.argmax(log_posterior(preds_truth_list.mean(0)))))\n",
    "print(\"Mean and std of individual runs: \")\n",
    "\n",
    "to_plot_central = []\n",
    "to_plot_err = []\n",
    "\n",
    "for method in ['CATHODE', 'CURTAINS', 'FETA', 'SALAD', 'TRUTH']:\n",
    "    print(f\"Based on {len(log_posterior_dict[method])} runs.\")\n",
    "    print(output_str_2.format(f\"{method} samples\", \n",
    "                              cen:=log_posterior_dict[method].mean(0),\n",
    "                              err:=log_posterior_dict[method].std(0),\n",
    "                              np.argmax(log_posterior_dict[method].mean(0))))\n",
    "    to_plot_central.append(cen)\n",
    "    to_plot_err.append(err)\n",
    "to_plot_central = np.array(to_plot_central).flatten()\n",
    "to_plot_err = np.array(to_plot_err).flatten()\n",
    "\n",
    "with open('log_posterior_mod.npy', 'wb') as f:\n",
    "    np.save(f, to_plot_central)\n",
    "    np.save(f, to_plot_err)\n",
    "with open('log_posterior_mod.npy', 'rb') as f:\n",
    "    to_plot_central = np.load(f)\n",
    "    to_plot_err = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97037d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot average of the log posterior of the runs above\n",
    "ymin, ymax = np.min(to_plot_central-to_plot_err), np.max(to_plot_central+to_plot_err)\n",
    "\n",
    "plt.errorbar(np.arange(20), to_plot_central, to_plot_err, fmt='_')\n",
    "plt.vlines([3.5, 7.5, 11.5, 15.5], ymin, ymax, ls='dashed', color='k')\n",
    "plt.ylim((ymin, ymax))\n",
    "plt.xlim((-0.5, 19.5))\n",
    "plt.ylabel('log posterior = $\\sum_{x} \\log{p_{model}(x)}$')\n",
    "plt.gca().set_xticks(np.arange(20), *[5 * ['$p_{CATHODE}$', '$p_{CURTAINS}$', '$p_{FETA}$', '$p_{SALAD}$']], rotation=70)\n",
    "plt.text(1.5, -1.345, '$x \\in$ CATHODE', ha='center')\n",
    "plt.text(5.5, -1.34, '$x \\in$ CURTAINS', ha='center')\n",
    "plt.text(9.5, -1.345, '$x \\in$ FETA', ha='center')\n",
    "plt.text(13.5, -1.34, '$x \\in$ SALAD', ha='center')\n",
    "plt.text(17.5, -1.345, '$x \\in$ Truth', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723bcd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328801f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79d61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5198ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c557cfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65b3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4726b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
